{
    "docs": [
        {
            "location": "/", 
            "text": "Hope this site help you with your SAS programming.\nContact me if you want any content to be added to this brief (hopefully not so brief in the future) manual. \nI will be glad to include it.\n\n\nEnjoy! :)\n\n\n\n\nIf this is your starting point with SAS programming, maybe these readings could be useful:\n\n\n\n\nGetting Started with SAS Programming\n\n\nWorking with SAS Programs\n\n\nInteresting configuration tips and tricks\n\n\nPrint out the available SAS packages according to your license and the expiration dates: \n\n\n\n\n1\n2\nPROC SETINIT;\n \n\nRUN;\n\n\n\n\n\n\n\n\n\nComments:\n\n\n\n\n1\n2\n/* comment */\n\n\n* comment statement;\n\n\n\n\n\n\n\n\n\n\n\nWhen you name a process flow \nAutoexec\n, SAS Enterprise Guide prompts you to run the process flow when you open the project. This makes it easy to recreate your data when you return to the project.\n\n\n\n\n\n\nHow to \ncompare SAS programs\n in SAS Enterprise Guide\n\n\n\n\n\n\nSend an \nemail\n with some coding\n\n\n\n\n\n\nSAS seminars by the UCLA Statistical Consulting Group\n\n\n\n\n\n\nShortcuts\n\n\n\n\n\n\n\n\nShortcut\n\n\nFunction\n\n\n\n\n\n\n\n\n\n\nF3\n\n\nRun selection or run all if there's nothing selected\n\n\n\n\n\n\nCtrl + I\n\n\nBeautify code (proper indentation)\n\n\n\n\n\n\nCtrl + Shift + U\n\n\nConvert to uppercase\n\n\n\n\n\n\nCtrl + Shift + L\n\n\nConvert to lowercase\n\n\n\n\n\n\nCtrl + /\n\n\nWrap selection (or current line) in a comment\n\n\n\n\n\n\nCtrl + Shift + /\n\n\nUnwrap selection (or current line) from a comment\n\n\n\n\n\n\nCtrl + G\n\n\nGo to line (prompts for a line number)\n\n\n\n\n\n\nCtrl + [, Ctrl + ]\n\n\nMove caret to matching parenthesis/brace\n\n\n\n\n\n\nAlt + [, Alt + ]\n\n\nMove caret to matching DO/END keyword\n\n\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\n5 keyboard shortcuts in SAS that will change your life", 
            "title": "Gettin' Started!"
        }, 
        {
            "location": "/#shortcuts", 
            "text": "Shortcut  Function      F3  Run selection or run all if there's nothing selected    Ctrl + I  Beautify code (proper indentation)    Ctrl + Shift + U  Convert to uppercase    Ctrl + Shift + L  Convert to lowercase    Ctrl + /  Wrap selection (or current line) in a comment    Ctrl + Shift + /  Unwrap selection (or current line) from a comment    Ctrl + G  Go to line (prompts for a line number)    Ctrl + [, Ctrl + ]  Move caret to matching parenthesis/brace    Alt + [, Alt + ]  Move caret to matching DO/END keyword      See also   5 keyboard shortcuts in SAS that will change your life", 
            "title": "Shortcuts"
        }, 
        {
            "location": "/essentials/accessing/", 
            "text": "Chapter summary in SAS\n\n\nAccessing SAS libraries\n\n\nlibref\n is a \nlibrary reference name\n (a shortcut to the physical location). There are three rules for valid librefs:\n\n\n\n\nA length of one to eight characters\n\n\nBegin with a letter or underscore\n\n\nThe remaining characters are letters, numbers, or underscores\n\n\n\n\nValid \nvariable names\n begin with a letter or underscore, and continue with letters, numbers, or underscores. The \nVALIDVARNAME\n system option specifies the rules for valid SAS variable names that can be created and processed during a SAS session: \n\n\n1\nOPTIONS VALIDVARNAME=V7 (default) | UPCASE | ANY;\n\n\n\n\n\n\n\n\nlibref.data-set-name\n: data set reference two-level name\n\n\ndata-set-name\n: when the data set belongs to a temporary library, you can optionally use a one-level name (SAS assumes that it is contained in the \nwork\n library, which is the default)\n\n\nThe \nLIBNAME\n statement associates the \nlibref\n with the physical location of the library/data for the current SAS session\n\n\n\n\n1\n2\n3\n4\n5\n6\nLIBNAME\n \nlibref\n-\nname\n \nSAS\n-\nlibrary\n-\nfolder\n-\npath\n \noptions\n;\n\n\n\n/* Example */\n\n\n\n%let\n \npath\n=/\nfolders\n/\nmyfolders\n/\necprg193\n;\n \n\nlibname\n \norion\n \npath\n;\n\n\n\n\n\n\n\n\n\nErase the association between SAS and a custom library:\n\n\n1\nLIBNAME libref-name CLEAR;\n\n\n\n\n\n\nDelete all the contents of a library:\n\n\n1\n2\n3\nPROC DATASETS LIB=library-name MEMTYPE=LIB KILL;\nRUN;\nQUIT;\n\n\n\n\n\n\nDelete some data sets of a library:\n\n\n1\n2\n3\n4\nPROC\n \nDATASETS\n \nLIB\n=\nwork\n \nNOWARN\n \nNOLIST\n \nNODETAILS\n;\n \n  \nDELETE\n \ndata-set1\n \n:\nsuffix\n \nprefix\n:\n \n;\n\n\nRUN\n;\n \n\nQUIT\n;\n\n\n\n\n\n\n\nRemove data sets:\n\n\n1\n2\nPROC DELETE DATA=data1 data2 data3;\nRUN;\n\n\n\n\n\n\nCopy database \ntest\n into \nwork\n:\n\n\n1\n2\nproc copy in=test out=work;\nrun;\n\n\n\n\n\n\nRename data sets:\n\n\n1\n2\n3\nPROC DATASETS LIBRARY=library-name;\n   CHANGE data-set-name1=data-set-new-name1 data-set-name2=data-set-new-name2;\nRUN;\n\n\n\n\n\n\nTo check the \ncontents of a library\n programatically:\n\n\n1\n2\nPROC CONTENTS DATA=libref._ALL_;\nRUN;\n\n\n\n\n\n\nTo hide the descriptors of all data sets in the library (it could generate a very long report) you can add the option \nnods\n (only compatible with the keybord \n_all_\n):\n\n\n1\n2\nPROC CONTENTS DATA=libref._ALL_ NODS;\nRUN;\n\n\n\n\n\n\nTo access a data set you can use a \nPROC PRINT\n step:\n\n\n1\n2\nPROC PRINT DATA=SAS-data-set;\nRUN;\n\n\n\n\n\n\nExamining SAS Data Sets\n\n\nParts of a library (SAS notation):\n\n\n\n\nTable = \ndata set\n\n\nColumn = \nvariable\n\n\nRow = \nobservation\n\n\n\n\nPROC CONTENTS\n\n\nThe \ndescriptor portion\n contains information about the attributes of the data set (metadata), including the variable names. It is show in three tables:\n\n\n\n\nTable 1:\n general information about the data set (name, creation date/time, etc.)\n\n\nTable 2:\n operating environment information, file location, etc.\n\n\nTable 3:\n alphabetic list of variables in the data set and their attributes\n\n\n\n\nPROC PRINT\n\n\nThe \ndata portion\n contains the data values, stored in variables (numeric/character)\n\n\n\n\nNumeric values:\n right-aligned digits 0-9, minus sign, single decimal point, scientific notation (E)\n\n\nCharacter values:\n left-aligned; letters, numbers, special characters and blanks\n\n\nMissing values:\n \nblank\n for character variables and \nperiod\n for numeric ones. To change this default behaviour use  \nMISSING='new-character'\n \n\n\nValues length:\n for character variables 1 byte = 1 character, numeric variables have 8 bytes of storage by default (16-17 significant digits)\n\n\nOther attributes:\n \nformat\n, \ninformat\n, \nlabel", 
            "title": "Accessing Data"
        }, 
        {
            "location": "/essentials/accessing/#accessing-sas-libraries", 
            "text": "libref  is a  library reference name  (a shortcut to the physical location). There are three rules for valid librefs:   A length of one to eight characters  Begin with a letter or underscore  The remaining characters are letters, numbers, or underscores   Valid  variable names  begin with a letter or underscore, and continue with letters, numbers, or underscores. The  VALIDVARNAME  system option specifies the rules for valid SAS variable names that can be created and processed during a SAS session:   1 OPTIONS VALIDVARNAME=V7 (default) | UPCASE | ANY;    libref.data-set-name : data set reference two-level name  data-set-name : when the data set belongs to a temporary library, you can optionally use a one-level name (SAS assumes that it is contained in the  work  library, which is the default)  The  LIBNAME  statement associates the  libref  with the physical location of the library/data for the current SAS session   1\n2\n3\n4\n5\n6 LIBNAME   libref - name   SAS - library - folder - path   options ;  /* Example */  %let   path =/ folders / myfolders / ecprg193 ;   libname   orion   path ;     Erase the association between SAS and a custom library:  1 LIBNAME libref-name CLEAR;   Delete all the contents of a library:  1\n2\n3 PROC DATASETS LIB=library-name MEMTYPE=LIB KILL;\nRUN;\nQUIT;   Delete some data sets of a library:  1\n2\n3\n4 PROC   DATASETS   LIB = work   NOWARN   NOLIST   NODETAILS ;  \n   DELETE   data-set1   : suffix   prefix :   ;  RUN ;   QUIT ;    Remove data sets:  1\n2 PROC DELETE DATA=data1 data2 data3;\nRUN;   Copy database  test  into  work :  1\n2 proc copy in=test out=work;\nrun;   Rename data sets:  1\n2\n3 PROC DATASETS LIBRARY=library-name;\n   CHANGE data-set-name1=data-set-new-name1 data-set-name2=data-set-new-name2;\nRUN;   To check the  contents of a library  programatically:  1\n2 PROC CONTENTS DATA=libref._ALL_;\nRUN;   To hide the descriptors of all data sets in the library (it could generate a very long report) you can add the option  nods  (only compatible with the keybord  _all_ ):  1\n2 PROC CONTENTS DATA=libref._ALL_ NODS;\nRUN;   To access a data set you can use a  PROC PRINT  step:  1\n2 PROC PRINT DATA=SAS-data-set;\nRUN;", 
            "title": "Accessing SAS libraries"
        }, 
        {
            "location": "/essentials/accessing/#examining-sas-data-sets", 
            "text": "Parts of a library (SAS notation):   Table =  data set  Column =  variable  Row =  observation", 
            "title": "Examining SAS Data Sets"
        }, 
        {
            "location": "/essentials/accessing/#proc-contents", 
            "text": "The  descriptor portion  contains information about the attributes of the data set (metadata), including the variable names. It is show in three tables:   Table 1:  general information about the data set (name, creation date/time, etc.)  Table 2:  operating environment information, file location, etc.  Table 3:  alphabetic list of variables in the data set and their attributes", 
            "title": "PROC CONTENTS"
        }, 
        {
            "location": "/essentials/accessing/#proc-print", 
            "text": "The  data portion  contains the data values, stored in variables (numeric/character)   Numeric values:  right-aligned digits 0-9, minus sign, single decimal point, scientific notation (E)  Character values:  left-aligned; letters, numbers, special characters and blanks  Missing values:   blank  for character variables and  period  for numeric ones. To change this default behaviour use   MISSING='new-character'    Values length:  for character variables 1 byte = 1 character, numeric variables have 8 bytes of storage by default (16-17 significant digits)  Other attributes:   format ,  informat ,  label", 
            "title": "PROC PRINT"
        }, 
        {
            "location": "/essentials/reporting/", 
            "text": "Chapter summary in SAS\n\n\nSubsetting Report Data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nPROC PRINT DATA=SAS-data-set(OBS=3) NOOBS;  /* OBS=3 prints only 3 elements | NOOBS hides the \nObs\n */\n    VAR variable1 variable2 variable3;      /* prints out only this variables in the report */\n    SUM variable1 variable2;                /* adds an extra line at the end with the total */\n    WHERE variable3\n1000; variable3\n1000;   /* operators: \n \n \n= \n= = ^= in + - / * ** \n | ~ ^ ? */\n    WHERE variable4 in (\nChild\n,\nElder\n);   /* only the last WHERE condition is applied */\n    WHERE variable1=20 AND variable4 CONTAINS \ncase-sensitive-substring\n;  /* CONTAINS = ? */\n    IDWHERE ANYALPHA(variable) NE 0         /* only values containing at least a letter */\n    ID variable1                            /* replaces the \nObs\n column by a selected variable values */\n    BY variable3variable3                   /* separate in different tables for different variable values (sort first) */\nRUN;\n\n\n\n\n\n\nSpecial \nWHERE\n operators:\n\n\n\n\nBETWEEN x AND y\n: an inclusive range\n\n\nWHERE SAME AND\n: augment a previous where expression (both applied)\n\n\nIS NULL\n: a missing value\n\n\nIS MISSING\n: a missing value\n\n\nLIKE\n: matches a pattern (% = any number of characters, _ = one character). E.g.: \n'T_m%'\n\n\nThe \nSOUNDS-LIKE (=\\*)\n operator selects observations that contain a spelling variation of a specified word or words. This operator uses the \nSoundex\n algorithm to compare the variable value and the operand.\n\n\nANYVALUE\n is an interesting function that searches a character string for an alphabetic character, and returns the first position at which the character is found\n\n\n\n\n\n\nNote\n\n\nTo compare with a SAS date value you need to express is as a SAS date constant: \n'DDMM\n\\YY\nYY'D\n.\n\n\n\n\nSorting and Grouping Report Data\n\n\n1\n2\n3\n4\n5\nPROC SORT DATA=SAS-data-set\n    OUT=new-SAS-data-set NODUPKEY;                                           /* optional */\n    DUPOUT=work.duplicates;                                                  /* optional */\n    BY ASCENDING variable1-to-be-sorted DESCENDING variable2-to-be-sorted;   /* optional (ASCENDING is the default order)*/\nRUN;\n\n\n\n\n\n\n\n\nThe \nNODUPKEY\n option deletes observations with duplicate \nBY\n values\n\n\nDUPOUT\n writes duplicate observations to a separate output data set\n\n\n\n\nEnhancing Reports\n\n\nTitles and footnotes\n\n\n1\n2\n3\n4\n5\n6\nTITLEline \ntext\n;       \nFOOTNOTEline \ntext\n;\n\nTITLE1 \ntext1\n;\nTITLE1 \ntext1_change\n;     /* Change title text and also cancels all footnotes with higher numbers */\nTITLE;                     /* Cancel (erase) all titles */\n\n\n\n\n\n\n\n\nThe \nlines\n specifies the line (1-10) on which the title/footnote will appear (line = 1 is the default value)\n\n\nThe title/footnote will remain until you \nchange\n it, \ncancel\n it or you \nend your SAS session\n\n\n\n\nTitles and footnotes inside AND outside a graph\n\n\nCOMPLETE\n\n\nLabels\n\n\nAssigning \ntemporary labels\n to display in the report instead of the variable names:\n\n\n1\n2\n3\n4\n5\nPROC PRINT DATA=SAS-data-set LABEL;            \n    LABEL variable1 = \nnew variable1 name\n \n          variable2 = \nnew variable2 name\n;\n    LABEL variable3 = \nnew variable3 name\n;\nRUN;\n\n\n\n\n\n\n\n\nYou need to add the \nLABEL\n option at the \nPROC PRINT\n definition to display the labels \n\n\nThe \nLABEL\n lengths can go up to 256 characters long\n\n\nYou can specify several labels in one \nLABEL\n statement or use a separate \nLABEL\n statement for each variable\n\n\n\n\nThe \nSPLIT\n option\n\n\n1\n2\n3\nPROC PRINT DATA=SAS-data-set SPLIT=\n*\n;             \n    LABEL variable1 = \nvariable label line 1*variable label line 1\n;   \nRUN;\n\n\n\n\n\n\n\n\nWhen you use \nSPLIT\n you no longer need to add the \nLABEL\n option to get the labels printed out\n\n\nThe \nSPLIT\n option introduces a line break at the label text whenever it finds the specified character (\n*\n)", 
            "title": "Producing Detailed Reports"
        }, 
        {
            "location": "/essentials/reporting/#subsetting-report-data", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 PROC PRINT DATA=SAS-data-set(OBS=3) NOOBS;  /* OBS=3 prints only 3 elements | NOOBS hides the  Obs  */\n    VAR variable1 variable2 variable3;      /* prints out only this variables in the report */\n    SUM variable1 variable2;                /* adds an extra line at the end with the total */\n    WHERE variable3 1000; variable3 1000;   /* operators:      =  = = ^= in + - / * **   | ~ ^ ? */\n    WHERE variable4 in ( Child , Elder );   /* only the last WHERE condition is applied */\n    WHERE variable1=20 AND variable4 CONTAINS  case-sensitive-substring ;  /* CONTAINS = ? */\n    IDWHERE ANYALPHA(variable) NE 0         /* only values containing at least a letter */\n    ID variable1                            /* replaces the  Obs  column by a selected variable values */\n    BY variable3variable3                   /* separate in different tables for different variable values (sort first) */\nRUN;   Special  WHERE  operators:   BETWEEN x AND y : an inclusive range  WHERE SAME AND : augment a previous where expression (both applied)  IS NULL : a missing value  IS MISSING : a missing value  LIKE : matches a pattern (% = any number of characters, _ = one character). E.g.:  'T_m%'  The  SOUNDS-LIKE (=\\*)  operator selects observations that contain a spelling variation of a specified word or words. This operator uses the  Soundex  algorithm to compare the variable value and the operand.  ANYVALUE  is an interesting function that searches a character string for an alphabetic character, and returns the first position at which the character is found    Note  To compare with a SAS date value you need to express is as a SAS date constant:  'DDMM \\YY YY'D .", 
            "title": "Subsetting Report Data"
        }, 
        {
            "location": "/essentials/reporting/#sorting-and-grouping-report-data", 
            "text": "1\n2\n3\n4\n5 PROC SORT DATA=SAS-data-set\n    OUT=new-SAS-data-set NODUPKEY;                                           /* optional */\n    DUPOUT=work.duplicates;                                                  /* optional */\n    BY ASCENDING variable1-to-be-sorted DESCENDING variable2-to-be-sorted;   /* optional (ASCENDING is the default order)*/\nRUN;    The  NODUPKEY  option deletes observations with duplicate  BY  values  DUPOUT  writes duplicate observations to a separate output data set", 
            "title": "Sorting and Grouping Report Data"
        }, 
        {
            "location": "/essentials/reporting/#enhancing-reports", 
            "text": "", 
            "title": "Enhancing Reports"
        }, 
        {
            "location": "/essentials/reporting/#titles-and-footnotes", 
            "text": "1\n2\n3\n4\n5\n6 TITLEline  text ;       \nFOOTNOTEline  text ;\n\nTITLE1  text1 ;\nTITLE1  text1_change ;     /* Change title text and also cancels all footnotes with higher numbers */\nTITLE;                     /* Cancel (erase) all titles */    The  lines  specifies the line (1-10) on which the title/footnote will appear (line = 1 is the default value)  The title/footnote will remain until you  change  it,  cancel  it or you  end your SAS session", 
            "title": "Titles and footnotes"
        }, 
        {
            "location": "/essentials/reporting/#titles-and-footnotes-inside-and-outside-a-graph", 
            "text": "COMPLETE", 
            "title": "Titles and footnotes inside AND outside a graph"
        }, 
        {
            "location": "/essentials/reporting/#labels", 
            "text": "Assigning  temporary labels  to display in the report instead of the variable names:  1\n2\n3\n4\n5 PROC PRINT DATA=SAS-data-set LABEL;            \n    LABEL variable1 =  new variable1 name  \n          variable2 =  new variable2 name ;\n    LABEL variable3 =  new variable3 name ;\nRUN;    You need to add the  LABEL  option at the  PROC PRINT  definition to display the labels   The  LABEL  lengths can go up to 256 characters long  You can specify several labels in one  LABEL  statement or use a separate  LABEL  statement for each variable", 
            "title": "Labels"
        }, 
        {
            "location": "/essentials/reporting/#the-split-option", 
            "text": "1\n2\n3 PROC PRINT DATA=SAS-data-set SPLIT= * ;             \n    LABEL variable1 =  variable label line 1*variable label line 1 ;   \nRUN;    When you use  SPLIT  you no longer need to add the  LABEL  option to get the labels printed out  The  SPLIT  option introduces a line break at the label text whenever it finds the specified character ( * )", 
            "title": "The SPLIT option"
        }, 
        {
            "location": "/essentials/formatting/", 
            "text": "Chapter summary in SAS\n\n\nUsing SAS Formats\n\n\n1\n2\n3\n4\nPROC PRINT DATA=SAS-data-base;\n    FORMAT variable1 variable2 format;\n    FORMAT variable3 format3 variable4 format4;\nRUN;\n\n\n\n\n\n\nFormat definition\n\n\n$\nformat\nw\n.\nd\n\n\n\n\n$\n = character format\n\n\nformat\n = format name\n\n\nw\n = total width (includes special characters, commas, decimal point and decimal places)\n\n\n.\n = required syntax (dot)\n\n\nd\n = decimal places (numeric format)\n\n\n\n\nSAS formats\n\n\nDictionary of formats\n\n\n\n\n$w.\n = writes standard character data\n\n\n$QUOTE.\n = writes a string in quotation marks \n\n\nw.d\n = writes standard numeric data\n\n\nCOMMAw.d\n = writes numeric values with a comma that separates every three digits and a period that separates the decimal fraction\n\n\nDOLLARw.d\n = writes numeric values with a leading dollar sign, a comma that separates every three digits and a period that separates the decimal fraction\n\n\nCOMMAXw.d\n = writes numeric values with a period that separates every three digits and a coma that separates the decimal fraction\n\n\nEUROXw.d\n = writes numeric values with a leading euro symbol, a period that separates every three digits and a comma that separates the decimal fraction\n\n\nDOSEF.\n = you can see the actual variable level values in the output rather than some indexes\n\n\n$UPCASE.\n = writes a string in uppercase\n\n\n\n\nIf you want to uppercase \nonly the first letter\n of words there is not a format but a function that you could use to transform your value:\n\n\n1\nvar_propercase = PROPCASE(var_uppercase);\n\n\n\n\n\n\nSAS date values\n\n\nMMDDYY\nw\n.\n | \nDDMMYY\nw\n.\n | \nMONYY\nw\n.\n | \nDATE\nw\n.\n | \nWEEKDATE.\n\n\n\n\nw = 6: only date numbers\n\n\nw = 8: date numbers with \n/\n separators (just the last 2 digits of year)\n\n\nw = 10: date numbers with \n/\n separators (full 4-digit year)\n\n\n\n\n\n\nNote\n\n\nDates before 01/01/1960 (0 value) will appear as negative numbers.\n\n\n\n\n\n\nWarning\n\n\nIf you ever have to deal with hours (4-character strings with the military hour) you better create an auxiliary character variable with a \n:\n in between hours and minutes or translate it into seconds (numeric) before applying an \nHOURw.d\n (time interval in hours and its fractions) or \nHHMMw.d\n (time in HH:MM appearance) format. \n\n\n\n\nCreating and Applying User-Defined Formats\n\n\nPROC FORMAT\n\n\n1\n2\n3\n4\nPROC FORMAT;\n    VALUE \n$\nformat-name value-or-range1=\nformatted-value1\n\n                         value-or-range2=\nformatted-value2\n;\nRUN;\n\n\n\n\n\n\n1\n2\n3\nPROC PRINT DATA=SAS-data-set;\n    FORMAT variable1 \n$\nformat-name.;\nRUN;\n\n\n\n\n\n\n\n\nA format name can have a maximum of \n32 characters\n\n\nThe name of a format that applies to \ncharacter values\n must begin with a \ndollar sign\n followed by a letter or underscore\n\n\nThe name of a format that applies to \nnumeric values\n must begin with a letter or underscore\n\n\nA format name cannot end in a number\n\n\nAll remaining characters can be letters, underscores or numbers\n\n\nA user defined format name cannot be the name of a SAS format\n\n\n\n\nEach \nvalue-range set\n has three parts:\n\n\n\n\nvalue-or-range\n: specifies one or more values to be formatted (it can be a value, a range or a list of values)\n\n\n=\n: equal sign\n\n\nformatted-value\n: the formatted value you want to display instead of the stored value/s (it is allways a character string no matter wheter the format applies to character values or numeric values)\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPROC FORMAT LIBRARY = my-format-library;   /* To save the custom formats */\n    VALUE string \nA\n-\nH\n=\nFirst\n\n                 \nI\n,\nJ\n,\nK\n=\nMiddle\n\n                  OTHER = \nEnd\n;           /* Non-specified values */\n    VALUE tiers low-\n50000=\nTier1\n         /* 50000 not included */\n                50000-\n100000=\nTier2\n      /* 100000 not included */\n                100000-high=\nTier3\n\n                .=\nMissing value\n;\nRUN;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf you omit the \nLIBRARY\n option, then formats and informats are stored in the \nwork.formats\n catalog.\n\n\nIf you do not include the keyword \nOTHER\n, then SAS applies the format only to values that match the value-range sets that you specify and the rest of values are displayed as they are stored in the data set.\n\n\nYou can only use the \n symbol to define a non-inclusive range.\n\n\n\n\n\n\n1\nOPTIONS FMTSEARCH = (libref1 libref2... librefn)\n\n\n\n\n\n\n\n\nThe \nFMTSEARCH\n system option controls the order in which format catalogs are searched until the desired member is found.\n\n\nThe \nWORK.FORMATS\n catalog is always searched first, unless it appears in the \nFMTSEARCH\n list.\n\n\n\n\nCreating a Format from a SAS Dataset\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nDATA formatdataset;\n    SET originaldataset;\n    RETAIN fmtname \n$custom_format_name\n TYPE \nC\n;\n    RENAME index_variable=start label_variable=label;\nRUN;\n\nPROC FORMAT CNTLIN=formatdataset;\nRUN;\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n'See More Information'\n    \nCreating a Format from Raw Data or a SAS Dataset\n\n\nPROC FORMAT\n's \nPICTURE\n statement\n\n\nLOW-HIGH\n ensures that all values are included in the range. The \nMULT=\n statement option specifies that each value is multiplied by 1.61. The \nPREFIX=\n statement adds a US dollar sign to any number that you format. The picture contains six digit selectors, five for the salary and one for the dollar sign prefix.\n\n\n1\n2\n3\n4\n5\n6\nPROC FORMAT;\n    PICTURE pct (round)   low-high =\n0009.9%)\n  (mult=10 prefix=\n(\n);\n    PICTURE pctl (round)  low-high =\n0000.00%)\n (mult=100 prefix=\n(\n);\n    PICTURE numero        low-high =\n0000000)\n  (prefix=\n(N=\n);\n    PICTURE uscurrency    low-high=\n000,000\n    (mult=1.61 prefix=\n$\n);\nRUN;\n\n\n\n\n\n\nExamples\n\n\nHow to order categorical variables\n\n\nYou first create a format that you will apply to an auxiliary variable: \n\n\n1\n2\n3\n4\n5\n6\nvalue SmFmt 1 = \nNon-smoker\n\n            2 = \nLight (1-5)\n\n            3 = \nModerate (6-15)\n\n            4 = \nHeavy (16-25)\n\n            5 = \nVery Heavy (\n 25)\n;\nrun;\n\n\n\n\n\n\nThen you create a data set view rather than a data set in order to save storage space (which might be important for large data sets) on which you define your auxiliary variable with the predefined format:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\ndata Heart / view=Heart;\n    format Smoking_Cat SmFmt.;\n    set sashelp.heart;\n    counter = _n_;\n    keep counter Status Sex AgeAtStart Height Weight Diastolic Systolic Smoking_Cat;\n\n    select (Smoking_Status);\n        when (\nNon-smoker\n)        Smoking_Cat=1;\n        when (\nLight (1-5)\n)       Smoking_Cat=2;\n        when (\nModerate (6-15)\n)   Smoking_Cat=3;\n        when (\nHeavy (16-25)\n)     Smoking_Cat=4;\n        when (\nVery Heavy (\n 25)\n) Smoking_Cat=5;\n        when (\n \n)                 Smoking_Cat=.;\n    end;\nrun;\n\n\n\n\n\n\nIf you then use a \nPROC REPORT\n to display your results, the order of appearance will be the numeric order of your auxiliary variable. By using this technique, you can specify any order for the categories of a contingency table.", 
            "title": "Formatting Data Values"
        }, 
        {
            "location": "/essentials/formatting/#using-sas-formats", 
            "text": "1\n2\n3\n4 PROC PRINT DATA=SAS-data-base;\n    FORMAT variable1 variable2 format;\n    FORMAT variable3 format3 variable4 format4;\nRUN;", 
            "title": "Using SAS Formats"
        }, 
        {
            "location": "/essentials/formatting/#format-definition", 
            "text": "$ format w . d   $  = character format  format  = format name  w  = total width (includes special characters, commas, decimal point and decimal places)  .  = required syntax (dot)  d  = decimal places (numeric format)", 
            "title": "Format definition"
        }, 
        {
            "location": "/essentials/formatting/#sas-formats", 
            "text": "Dictionary of formats   $w.  = writes standard character data  $QUOTE.  = writes a string in quotation marks   w.d  = writes standard numeric data  COMMAw.d  = writes numeric values with a comma that separates every three digits and a period that separates the decimal fraction  DOLLARw.d  = writes numeric values with a leading dollar sign, a comma that separates every three digits and a period that separates the decimal fraction  COMMAXw.d  = writes numeric values with a period that separates every three digits and a coma that separates the decimal fraction  EUROXw.d  = writes numeric values with a leading euro symbol, a period that separates every three digits and a comma that separates the decimal fraction  DOSEF.  = you can see the actual variable level values in the output rather than some indexes  $UPCASE.  = writes a string in uppercase   If you want to uppercase  only the first letter  of words there is not a format but a function that you could use to transform your value:  1 var_propercase = PROPCASE(var_uppercase);", 
            "title": "SAS formats"
        }, 
        {
            "location": "/essentials/formatting/#sas-date-values", 
            "text": "MMDDYY w .  |  DDMMYY w .  |  MONYY w .  |  DATE w .  |  WEEKDATE.   w = 6: only date numbers  w = 8: date numbers with  /  separators (just the last 2 digits of year)  w = 10: date numbers with  /  separators (full 4-digit year)    Note  Dates before 01/01/1960 (0 value) will appear as negative numbers.    Warning  If you ever have to deal with hours (4-character strings with the military hour) you better create an auxiliary character variable with a  :  in between hours and minutes or translate it into seconds (numeric) before applying an  HOURw.d  (time interval in hours and its fractions) or  HHMMw.d  (time in HH:MM appearance) format.", 
            "title": "SAS date values"
        }, 
        {
            "location": "/essentials/formatting/#creating-and-applying-user-defined-formats", 
            "text": "", 
            "title": "Creating and Applying User-Defined Formats"
        }, 
        {
            "location": "/essentials/formatting/#proc-format", 
            "text": "1\n2\n3\n4 PROC FORMAT;\n    VALUE  $ format-name value-or-range1= formatted-value1 \n                         value-or-range2= formatted-value2 ;\nRUN;   1\n2\n3 PROC PRINT DATA=SAS-data-set;\n    FORMAT variable1  $ format-name.;\nRUN;    A format name can have a maximum of  32 characters  The name of a format that applies to  character values  must begin with a  dollar sign  followed by a letter or underscore  The name of a format that applies to  numeric values  must begin with a letter or underscore  A format name cannot end in a number  All remaining characters can be letters, underscores or numbers  A user defined format name cannot be the name of a SAS format   Each  value-range set  has three parts:   value-or-range : specifies one or more values to be formatted (it can be a value, a range or a list of values)  = : equal sign  formatted-value : the formatted value you want to display instead of the stored value/s (it is allways a character string no matter wheter the format applies to character values or numeric values)   1\n2\n3\n4\n5\n6\n7\n8\n9 PROC FORMAT LIBRARY = my-format-library;   /* To save the custom formats */\n    VALUE string  A - H = First \n                  I , J , K = Middle \n                  OTHER =  End ;           /* Non-specified values */\n    VALUE tiers low- 50000= Tier1          /* 50000 not included */\n                50000- 100000= Tier2       /* 100000 not included */\n                100000-high= Tier3 \n                .= Missing value ;\nRUN;    Note   If you omit the  LIBRARY  option, then formats and informats are stored in the  work.formats  catalog.  If you do not include the keyword  OTHER , then SAS applies the format only to values that match the value-range sets that you specify and the rest of values are displayed as they are stored in the data set.  You can only use the   symbol to define a non-inclusive range.    1 OPTIONS FMTSEARCH = (libref1 libref2... librefn)    The  FMTSEARCH  system option controls the order in which format catalogs are searched until the desired member is found.  The  WORK.FORMATS  catalog is always searched first, unless it appears in the  FMTSEARCH  list.", 
            "title": "PROC FORMAT"
        }, 
        {
            "location": "/essentials/formatting/#creating-a-format-from-a-sas-dataset", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 DATA formatdataset;\n    SET originaldataset;\n    RETAIN fmtname  $custom_format_name  TYPE  C ;\n    RENAME index_variable=start label_variable=label;\nRUN;\n\nPROC FORMAT CNTLIN=formatdataset;\nRUN;    Summary   'See More Information'\n     Creating a Format from Raw Data or a SAS Dataset", 
            "title": "Creating a Format from a SAS Dataset"
        }, 
        {
            "location": "/essentials/formatting/#proc-formats-picture-statement", 
            "text": "LOW-HIGH  ensures that all values are included in the range. The  MULT=  statement option specifies that each value is multiplied by 1.61. The  PREFIX=  statement adds a US dollar sign to any number that you format. The picture contains six digit selectors, five for the salary and one for the dollar sign prefix.  1\n2\n3\n4\n5\n6 PROC FORMAT;\n    PICTURE pct (round)   low-high = 0009.9%)   (mult=10 prefix= ( );\n    PICTURE pctl (round)  low-high = 0000.00%)  (mult=100 prefix= ( );\n    PICTURE numero        low-high = 0000000)   (prefix= (N= );\n    PICTURE uscurrency    low-high= 000,000     (mult=1.61 prefix= $ );\nRUN;", 
            "title": "PROC FORMAT's PICTURE statement"
        }, 
        {
            "location": "/essentials/formatting/#examples", 
            "text": "", 
            "title": "Examples"
        }, 
        {
            "location": "/essentials/formatting/#how-to-order-categorical-variables", 
            "text": "You first create a format that you will apply to an auxiliary variable:   1\n2\n3\n4\n5\n6 value SmFmt 1 =  Non-smoker \n            2 =  Light (1-5) \n            3 =  Moderate (6-15) \n            4 =  Heavy (16-25) \n            5 =  Very Heavy (  25) ;\nrun;   Then you create a data set view rather than a data set in order to save storage space (which might be important for large data sets) on which you define your auxiliary variable with the predefined format:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 data Heart / view=Heart;\n    format Smoking_Cat SmFmt.;\n    set sashelp.heart;\n    counter = _n_;\n    keep counter Status Sex AgeAtStart Height Weight Diastolic Systolic Smoking_Cat;\n\n    select (Smoking_Status);\n        when ( Non-smoker )        Smoking_Cat=1;\n        when ( Light (1-5) )       Smoking_Cat=2;\n        when ( Moderate (6-15) )   Smoking_Cat=3;\n        when ( Heavy (16-25) )     Smoking_Cat=4;\n        when ( Very Heavy (  25) ) Smoking_Cat=5;\n        when (   )                 Smoking_Cat=.;\n    end;\nrun;   If you then use a  PROC REPORT  to display your results, the order of appearance will be the numeric order of your auxiliary variable. By using this technique, you can specify any order for the categories of a contingency table.", 
            "title": "How to order categorical variables"
        }, 
        {
            "location": "/essentials/reading-sas/", 
            "text": "Chapter summary in SAS\n\n\nSubsetting using the \nWHERE\n statement\n\n\nTo create a new data set that is a subset of a previous data set:\n\n\n1\n2\n3\n4\n5\n6\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    WHERE where-expression;\n    variable_name   WHERE where-expression;\n    variable_name = expression;     /* new variable creation */\nRUN;\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf a missing value is involved in an arithmetic calculation the result will be a missing value too\n\n\nNew variables being created in the \nDATA\n step and not contained in the original data set cannot be used in a \nWHERE\n statement\n\n\n\n\n\n\nOther \nWHERE\n options\n\n\n\n\nHere's how to set a filter for \nWHERE\n a variable \nIS MISSING\n\n\n\n\nCustomizing a SAS Data Set\n\n\nHow to select a subset of the variables/observations of the original data set:\n\n\n1\n2\n3\n4\n5\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    DROP variable-list;        /* variables to exclude */\n    KEEP variable-list;        /* variables to include */\nRUN;\n\n\n\n\n\n\nHow SAS processes the \nDATA\n step\n\n\nCompilation phase\n\n\n\n\nSAS scan each DATA step statement for syntax errors and converts the program into machine code if everything's alright. \n\n\nSAS also creates the program data vector (\nPDV\n) in memory to hold the current observation.\n\n\n_N_\n: iteration number of the DATA step\n\n\n_ERROR_\n: its value is 0 is there are no errors (1 if there are some)\n\n\nSAS creates the descriptor portion of the new data set (takes the original one, adds the new variables and flags the variables to be dropped). \n\n\n\n\nExecution phase\n\n\n\n\nSAS initializes the PDV to missing\n\n\nSAS reads and processes the observations from the input data set \n\n\nSAS creates observations in the data portion of the output data set (an implicit output/implicit return loop over all the observations that continues until EOF)\n\n\n\n\nSubsetting using the \nIF\n statement\n\n\n1\n2\n3\n4\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression;\nRUN;\n\n\n\n\n\n\n\n\nWhen the expression is false, SAS excludes the observation from the output data set and continues processing\n\n\nWhile original values can be managed with a \nWHERE\n statement as well as an \nIF\n statement, for \nnew variable\n conditionals only \nIF\n can be used\n\n\nYou should subset as early as possible in your program for more efficient processing (a \nWHERE\n before an \nIF\n can make the processing more efficient).\n\n\nIn a \nPROC\n step \nIF\n statements are \nNOT allowed\n\n\nIF THEN\n analogue to \nCONTAINS\n:\n\n\n\n\n1\nIF FIND(variable_name,\npattern\n) THEN (...);\n\n\n\n\n\n\nSubsetting \nIF-THEN/DELETE\n statement\n\n\n1\n2\n3\n4\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression1 or expression2 THEN DELETE;\nRUN;\n\n\n\n\n\n\n\n\nThe \nIF-THEN/DELETE\n statement eliminates the observations where the \nconditions are not met\n (on the contrary of what the \nIF\n does)\n\n\nThe \nDELETE\n statement stops processing the current observation. It is often used in a \nTHEN\n clause of an \nIF-THEN\n statement or as part of a conditionally executed \nDO\n group.\n\n\n\n\n\n\nTip\n\n\nYou can remove all the observations with at least one missing value using this condition inside a \nDATA\n step:\n\nif cmiss(of _all_) then delete;\n\n\n\n\nCreate different data sets from one\n\n\n1\n2\n3\n4\n5\n6\nDATA data1 data2 data3;\n    SET original_data;\n    IF (condition1) THEN OUTPUT prueba1;\n    IF (condition2) THEN OUTPUT prueba2;\n    IF (condition3) THEN OUTPUT prueba3;\nRUN;\n\n\n\n\n\n\nAvailable operations\n\n\n\n\nAddition of several variables: \nTotal=sum(var1, var2, var3)\n\n\nCount of nonmissing values: \nNonmissing=n(var1, var2, var3)\n\n\n\n\nUsing \nPROC SQL\n to \nGROUP BY\n variables\n\n\nPROC SQL\n is a wonderful tool for \nsummarizing or aggregating\n data. When you use a \nGROUP BY\n clause, you also use an aggregate function in the \nSELECT\n clause or in a \nHAVING\n clause to instruct \nPROC SQL\n in how to summarize the data for each group:\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC SQL;\n    select T, range(survival) as RangeSurvival, sqrt(sum(sdf_stderr**2)) as Squares, range(survival)/sqrt(sum(sdf_stderr**2)) as z,\n           probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2)))) as pz, 2 * (1-probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2))))) as pvalue\n    from BTM_param \n    where T \n 0\n    group by T;\nQUIT;\n\n\n\n\n\n\nAdding Permanent Attributes\n\n\nPermanent variable labels\n\n\n1\n2\n3\n4\n5\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    LABEL variable1=\nlabel1\n\n          variable2=\nlabel2\n;\nRUN;\n\n\n\n\n\n\n1\n2\nPROC PRINT DATA=output-SAS-data-set label;\nRUN;\n\n\n\n\n\n\n\n\nIf you use the \nLABEL\n statement in the \nPROC\n step the labels are \ntemporary\n while if you use it in the \nDATA\n step, SAS \npermanently\n associates the labels to the variables\n\n\nLabels and formats that you specify in \nPROC\n steps override the permanent labels in the current step. However, the permanent labels are not changed.\n\n\n\n\nPermanent variable formats\n\n\n1\n2\n3\n4\n5\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    FORMAT variable1 format1\n           variable2 format2;\nRUN;", 
            "title": "Reading SAS Data Sets"
        }, 
        {
            "location": "/essentials/reading-sas/#subsetting-using-the-where-statement", 
            "text": "To create a new data set that is a subset of a previous data set:  1\n2\n3\n4\n5\n6 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    WHERE where-expression;\n    variable_name   WHERE where-expression;\n    variable_name = expression;     /* new variable creation */\nRUN;    Note   If a missing value is involved in an arithmetic calculation the result will be a missing value too  New variables being created in the  DATA  step and not contained in the original data set cannot be used in a  WHERE  statement", 
            "title": "Subsetting using the WHERE statement"
        }, 
        {
            "location": "/essentials/reading-sas/#other-where-options", 
            "text": "Here's how to set a filter for  WHERE  a variable  IS MISSING", 
            "title": "Other WHERE options"
        }, 
        {
            "location": "/essentials/reading-sas/#customizing-a-sas-data-set", 
            "text": "How to select a subset of the variables/observations of the original data set:  1\n2\n3\n4\n5 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    DROP variable-list;        /* variables to exclude */\n    KEEP variable-list;        /* variables to include */\nRUN;", 
            "title": "Customizing a SAS Data Set"
        }, 
        {
            "location": "/essentials/reading-sas/#how-sas-processes-the-data-step", 
            "text": "Compilation phase   SAS scan each DATA step statement for syntax errors and converts the program into machine code if everything's alright.   SAS also creates the program data vector ( PDV ) in memory to hold the current observation.  _N_ : iteration number of the DATA step  _ERROR_ : its value is 0 is there are no errors (1 if there are some)  SAS creates the descriptor portion of the new data set (takes the original one, adds the new variables and flags the variables to be dropped).    Execution phase   SAS initializes the PDV to missing  SAS reads and processes the observations from the input data set   SAS creates observations in the data portion of the output data set (an implicit output/implicit return loop over all the observations that continues until EOF)", 
            "title": "How SAS processes the DATA step"
        }, 
        {
            "location": "/essentials/reading-sas/#subsetting-using-the-if-statement", 
            "text": "1\n2\n3\n4 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression;\nRUN;    When the expression is false, SAS excludes the observation from the output data set and continues processing  While original values can be managed with a  WHERE  statement as well as an  IF  statement, for  new variable  conditionals only  IF  can be used  You should subset as early as possible in your program for more efficient processing (a  WHERE  before an  IF  can make the processing more efficient).  In a  PROC  step  IF  statements are  NOT allowed  IF THEN  analogue to  CONTAINS :   1 IF FIND(variable_name, pattern ) THEN (...);", 
            "title": "Subsetting using the IF statement"
        }, 
        {
            "location": "/essentials/reading-sas/#subsetting-if-thendelete-statement", 
            "text": "1\n2\n3\n4 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression1 or expression2 THEN DELETE;\nRUN;    The  IF-THEN/DELETE  statement eliminates the observations where the  conditions are not met  (on the contrary of what the  IF  does)  The  DELETE  statement stops processing the current observation. It is often used in a  THEN  clause of an  IF-THEN  statement or as part of a conditionally executed  DO  group.    Tip  You can remove all the observations with at least one missing value using this condition inside a  DATA  step: if cmiss(of _all_) then delete;", 
            "title": "Subsetting IF-THEN/DELETE statement"
        }, 
        {
            "location": "/essentials/reading-sas/#create-different-data-sets-from-one", 
            "text": "1\n2\n3\n4\n5\n6 DATA data1 data2 data3;\n    SET original_data;\n    IF (condition1) THEN OUTPUT prueba1;\n    IF (condition2) THEN OUTPUT prueba2;\n    IF (condition3) THEN OUTPUT prueba3;\nRUN;", 
            "title": "Create different data sets from one"
        }, 
        {
            "location": "/essentials/reading-sas/#available-operations", 
            "text": "Addition of several variables:  Total=sum(var1, var2, var3)  Count of nonmissing values:  Nonmissing=n(var1, var2, var3)", 
            "title": "Available operations"
        }, 
        {
            "location": "/essentials/reading-sas/#using-proc-sql-to-group-by-variables", 
            "text": "PROC SQL  is a wonderful tool for  summarizing or aggregating  data. When you use a  GROUP BY  clause, you also use an aggregate function in the  SELECT  clause or in a  HAVING  clause to instruct  PROC SQL  in how to summarize the data for each group:  1\n2\n3\n4\n5\n6\n7 PROC SQL;\n    select T, range(survival) as RangeSurvival, sqrt(sum(sdf_stderr**2)) as Squares, range(survival)/sqrt(sum(sdf_stderr**2)) as z,\n           probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2)))) as pz, 2 * (1-probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2))))) as pvalue\n    from BTM_param \n    where T   0\n    group by T;\nQUIT;", 
            "title": "Using PROC SQL to GROUP BY variables"
        }, 
        {
            "location": "/essentials/reading-sas/#adding-permanent-attributes", 
            "text": "", 
            "title": "Adding Permanent Attributes"
        }, 
        {
            "location": "/essentials/reading-sas/#permanent-variable-labels", 
            "text": "1\n2\n3\n4\n5 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    LABEL variable1= label1 \n          variable2= label2 ;\nRUN;   1\n2 PROC PRINT DATA=output-SAS-data-set label;\nRUN;    If you use the  LABEL  statement in the  PROC  step the labels are  temporary  while if you use it in the  DATA  step, SAS  permanently  associates the labels to the variables  Labels and formats that you specify in  PROC  steps override the permanent labels in the current step. However, the permanent labels are not changed.", 
            "title": "Permanent variable labels"
        }, 
        {
            "location": "/essentials/reading-sas/#permanent-variable-formats", 
            "text": "1\n2\n3\n4\n5 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    FORMAT variable1 format1\n           variable2 format2;\nRUN;", 
            "title": "Permanent variable formats"
        }, 
        {
            "location": "/essentials/reading-spreadsheets/", 
            "text": "Chapter summary in SAS\n\n\nReading Spreadsheet Data\n\n\nSAS/ACCESS LIBNAME statement (read/write/update data):\n\n\n1\nLIBNAME libref \nengine\n \nPATH=\nworkbook-name\n \noptions\n;\n\n\n\n\n\n\nE.g.:\n\n\nDefault engine:\n \nLIBNAME orionx excel \"\npath/sales.xls\"\n\n\nPC Files server engine:\n \nLIBNAME orionx pcfiles PATH=\"\npath/sales.xls\"\n\n\n\n\n\\engine\n: excel (if both SAS and Office are 32/64 bits), pcfiles (if the value is different)\n\n\nThe icon of the library will be different (a globe) indicating that the data is outside SAS\n\n\nThe members whose name ends with a \n$\n are the \nspreadsheets\n while the others are named \nranges\n. In case it has the \n$\n, you need to refer to that Excel worksheet in a special way to account for that special character (SAS name literal): \nlibref.'worksheetname$'n\n\n\nYou can use the \nVALIDVARNAME = v7\n option in SAS Enterprise Guide to cause it to behave the same as in the SAS window environment\n\n\nIs important to disassociate the library: the workbook cannot be opened in Excel meanwhile (SAS puts a lock on the Excel file when the libref is assigned): \nLIBNAME libref CLEAR;\n\n\n\n\n\n\nImport the xls data:\n\n\n1\n2\n3\n4\n5\nPROC IMPORT DATAFILE=\n/folders/myfolders/reading_test.xlsx\n\n            OUT=work.myexcel\n            DBMS=xlsx \n            REPLACE;\nRUN;\n\n\n\n\n\n\nReading Database Data\n\n\n1\nLIBNAME libref engine \nSAS/ACCESS options\n;\n\n\n\n\n\n\n\n\nengine\n: oracle or BD2\n\n\nSAS/ACCESS options\n: USER, PASSWORD/PW, PATH (specifies the Oracle driver, node and database), SCHEMA (enables you to read database objects such as tables and views)", 
            "title": "Reading Spreadsheet and Database Data"
        }, 
        {
            "location": "/essentials/reading-spreadsheets/#reading-spreadsheet-data", 
            "text": "SAS/ACCESS LIBNAME statement (read/write/update data):  1 LIBNAME libref  engine   PATH= workbook-name   options ;   E.g.:  Default engine:   LIBNAME orionx excel \" path/sales.xls\"  PC Files server engine:   LIBNAME orionx pcfiles PATH=\" path/sales.xls\"   \\engine : excel (if both SAS and Office are 32/64 bits), pcfiles (if the value is different)  The icon of the library will be different (a globe) indicating that the data is outside SAS  The members whose name ends with a  $  are the  spreadsheets  while the others are named  ranges . In case it has the  $ , you need to refer to that Excel worksheet in a special way to account for that special character (SAS name literal):  libref.'worksheetname$'n  You can use the  VALIDVARNAME = v7  option in SAS Enterprise Guide to cause it to behave the same as in the SAS window environment  Is important to disassociate the library: the workbook cannot be opened in Excel meanwhile (SAS puts a lock on the Excel file when the libref is assigned):  LIBNAME libref CLEAR;    Import the xls data:  1\n2\n3\n4\n5 PROC IMPORT DATAFILE= /folders/myfolders/reading_test.xlsx \n            OUT=work.myexcel\n            DBMS=xlsx \n            REPLACE;\nRUN;", 
            "title": "Reading Spreadsheet Data"
        }, 
        {
            "location": "/essentials/reading-spreadsheets/#reading-database-data", 
            "text": "1 LIBNAME libref engine  SAS/ACCESS options ;    engine : oracle or BD2  SAS/ACCESS options : USER, PASSWORD/PW, PATH (specifies the Oracle driver, node and database), SCHEMA (enables you to read database objects such as tables and views)", 
            "title": "Reading Database Data"
        }, 
        {
            "location": "/essentials/reading-raw/", 
            "text": "Chapter summary in SAS\n\n\nIntroduction to Reading Raw Data Files\n\n\n\n\nRaw data files\n are not software specific\n\n\nA \ndelimited raw data file\n is an external text file in which the values are separated by spaces or other special characters.\n\n\nA \nlist input\n will be used to work with delimited raw data files that contain standard and/or nonstandard data\n\n\nStandard data\n is data that SAS can read without any special instructions\n\n\nNonstandard data\n includes values like dates or numeric values that include special characters like dollar signs (extra instructions needed)\n\n\nYou cannot use a \nWHERE\n statement when the input data is a raw data file instead of a SAS data set\n\n\n\n\nReading Standard Delimited Data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\nDATA output-SAS-data-set-name;\n    LENGTH variable(s) \n$\n length;\n    INFILE \nraw-data-file-name\n DLM=\ndelimiter\n;  \n    INPUT variable1 \n$\n variable2 \n$\n ... variableN \n$\n;       \nRUN;\n\n/* Example */\n\nDATA work.sales1;\n    LENGTH First_Name Last_Name $ 12 Gender $ 1;\n    INFILE \npath/sales.csv\n DLM=\n,\n;  \n    INPUT Employee_ID Gender $ Salary $ Job_Title $ Country $; \nRUN;\n\n\n\n\n\n\n\n\nWith \nlist input\n, the default length for all variables is 8 bytes\n\n\nSAS uses an \ninput buffer\n only if the input data is a raw data file\n\n\nThe variable names will appear in the report as stated in the \nLENGTH\n statement (watch out the uppercase/lowercase)\n\n\nThe \nLENGTH\n statement must precede the \nINPUT\n statement in order to correctly set the length of the variable\n\n\nThe variables not specified in the \nLENGTH\n statement will appear at the end of the table. If you want to keep the original order you should include all variables even if you want them to have the defaul length (8)\n\n\n\n\nReading Nonstandard Delimited Data\n\n\nYou can use a \nmodified list input\n to read all of the fields from a raw data file (including nonstandard variables)\n\n\n\n\nInformats are similar to formats except that \nformats\n provide instruction on how to \nwrite\n a value while \ninformats\n provide instruction on how to \nread\n a value\n\n\nThe \ncolon format modifier (:)\n causes SAS to read up to the delimiter\n\n\n\n\n1\n2\n3\n4\n5\n6\nINPUT variable \n$\n variable \n:informat\n;\n\n/* Example */\n\n:date.\n:mmddyy.\n\n\n\n\n\n\n\n\nCOMMA./DOLLAR.\n: reads nonstandard numeric data and removes embedded commas, blanks, dollar sign, percent signs and dashes\n\n\nCOMMAX./DOLLARX.\n: reads nonstandard numeric data and removes embedded non-numeric characters; reverses the roles of the decima point and the comma\n\n\nEUROX.\n: reads nonstandard numeric data and removes embedded non-numeric characters in European currency\n\n\n$CHAR.\n: reads character values and preserves leading blanks\n\n\n$UPCASE.\n: reads character values and converts them to uppercase\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nDATA (...);\n    INFILE DATALINES DLM=\n,\n;   /* only if datalines are delimited */\n    INPUT (...);\n    DATALINES;\n    \ninstream data\n\n    ;\n    INPUT (...);\n    DATALINES;\n\ninstream data\n\n;\n\n\n\n\n\n\n\n\nThe null statement (\n;\n) indicates the end of the input data\n\n\nYou precede the instream data with the \nDATALINES\n statement and follow it with a null statement\n\n\nThe instream data should be the last part of the \nDATA step\n except for a null statement\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n/* Example */\n\n\nDATA\n \nwork\n.\nmanagers\n;\n\n\n   \ninfile\n \ndatalines\n \ndlm\n=\n/\n;\n\n\n   \ninput\n \nID\n \nFirst\n \n:\n$\n12\n.\n \nLast\n \n:\n$\n12\n.\n \nGender\n \n$\n \nSalary\n \n:\ncomma\n.\n \n\n\n            \nTitle\n \n:\n$\n25\n.\n \nHireDate\n \n:\ndate\n.\n;\n\n\n   \ndatalines\n;\n\n\n120102\n/\nTom\n/\nZhou\n/\nM\n/\n108\n,\n255\n/\nSales\n \nManager\n/\n01\nJun1993\n\n\n120103\n/\nWilson\n/\nDawes\n/\nM\n/\n87\n,\n975\n/\nSales\n \nManager\n/\n01\nJan1978\n\n\n120261\n/\nHarry\n/\nHighpoint\n/\nM\n/\n243\n,\n190\n/\nChief\n \nSales\n \nOfficer\n/\n01\nAug1991\n\n\n121143\n/\nLouis\n/\nFavaron\n/\nM\n/\n95\n,\n090\n/\nSenior\n \nSales\n \nManager\n/\n01\nJul2001\n\n\n121144\n/\nRenee\n/\nCapachietti\n/\nF\n/\n83\n,\n505\n/\nSales\n \nManager\n/\n01\nNov1995\n\n\n121145\n/\nDennis\n/\nLansberry\n/\nM\n/\n84\n,\n260\n/\nSales\n \nManager\n/\n01\nApr1980\n\n\n;\n\n\ntitle\n \nOrion Star Management Team\n;\n\nproc\n \nprint\n \ndata\n=\nwork\n.\nmanagers\n \nnoobs\n;\n\n\n   \nformat\n \nHireDate\n \nmmddyy10\n.\n;\n\nrun\n;\n\ntitle\n;\n\n\n\n\n\n\n\nValidating Data\n\n\nWhen SAS encounters a data error, it prints messages and a ruler in the log and assigns a missing value to the affected variable. Then SAS continues processing.\n\n\nMissing Values between Delimiters (Consecutive Delimiters)\n\n\n1\nINFILE \nraw-data-file-name\n \nDLM=\n DSD;\n\n\n\n\n\n\nThe \nDSD\n option sets the default delimiter to a comma, treats consecutive delimiters as missing values and enables SAS to read values with embedded delimiters if the value is surrounded by quotation marks\n\n\nMissing Values at the End of a Line\n\n\n1\nINFILE \nraw-data-file-name\n MISSOVER;\n\n\n\n\n\n\nWith the \nMISSOVER\n option, if SAS reaches the end of a record without finding values for all fields, variables without values are set to missing.", 
            "title": "Reading Raw Data Files"
        }, 
        {
            "location": "/essentials/reading-raw/#introduction-to-reading-raw-data-files", 
            "text": "Raw data files  are not software specific  A  delimited raw data file  is an external text file in which the values are separated by spaces or other special characters.  A  list input  will be used to work with delimited raw data files that contain standard and/or nonstandard data  Standard data  is data that SAS can read without any special instructions  Nonstandard data  includes values like dates or numeric values that include special characters like dollar signs (extra instructions needed)  You cannot use a  WHERE  statement when the input data is a raw data file instead of a SAS data set", 
            "title": "Introduction to Reading Raw Data Files"
        }, 
        {
            "location": "/essentials/reading-raw/#reading-standard-delimited-data", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 DATA output-SAS-data-set-name;\n    LENGTH variable(s)  $  length;\n    INFILE  raw-data-file-name  DLM= delimiter ;  \n    INPUT variable1  $  variable2  $  ... variableN  $ ;       \nRUN;\n\n/* Example */\n\nDATA work.sales1;\n    LENGTH First_Name Last_Name $ 12 Gender $ 1;\n    INFILE  path/sales.csv  DLM= , ;  \n    INPUT Employee_ID Gender $ Salary $ Job_Title $ Country $; \nRUN;    With  list input , the default length for all variables is 8 bytes  SAS uses an  input buffer  only if the input data is a raw data file  The variable names will appear in the report as stated in the  LENGTH  statement (watch out the uppercase/lowercase)  The  LENGTH  statement must precede the  INPUT  statement in order to correctly set the length of the variable  The variables not specified in the  LENGTH  statement will appear at the end of the table. If you want to keep the original order you should include all variables even if you want them to have the defaul length (8)", 
            "title": "Reading Standard Delimited Data"
        }, 
        {
            "location": "/essentials/reading-raw/#reading-nonstandard-delimited-data", 
            "text": "You can use a  modified list input  to read all of the fields from a raw data file (including nonstandard variables)   Informats are similar to formats except that  formats  provide instruction on how to  write  a value while  informats  provide instruction on how to  read  a value  The  colon format modifier (:)  causes SAS to read up to the delimiter   1\n2\n3\n4\n5\n6 INPUT variable  $  variable  :informat ;\n\n/* Example */\n\n:date.\n:mmddyy.    COMMA./DOLLAR. : reads nonstandard numeric data and removes embedded commas, blanks, dollar sign, percent signs and dashes  COMMAX./DOLLARX. : reads nonstandard numeric data and removes embedded non-numeric characters; reverses the roles of the decima point and the comma  EUROX. : reads nonstandard numeric data and removes embedded non-numeric characters in European currency  $CHAR. : reads character values and preserves leading blanks  $UPCASE. : reads character values and converts them to uppercase     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 DATA (...);\n    INFILE DATALINES DLM= , ;   /* only if datalines are delimited */\n    INPUT (...);\n    DATALINES;\n     instream data \n    ;\n    INPUT (...);\n    DATALINES; instream data \n;    The null statement ( ; ) indicates the end of the input data  You precede the instream data with the  DATALINES  statement and follow it with a null statement  The instream data should be the last part of the  DATA step  except for a null statement    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20 /* Example */ \n\nDATA   work . managers ;      infile   datalines   dlm = / ;      input   ID   First   : $ 12 .   Last   : $ 12 .   Gender   $   Salary   : comma .                 Title   : $ 25 .   HireDate   : date . ;      datalines ;  120102 / Tom / Zhou / M / 108 , 255 / Sales   Manager / 01 Jun1993  120103 / Wilson / Dawes / M / 87 , 975 / Sales   Manager / 01 Jan1978  120261 / Harry / Highpoint / M / 243 , 190 / Chief   Sales   Officer / 01 Aug1991  121143 / Louis / Favaron / M / 95 , 090 / Senior   Sales   Manager / 01 Jul2001  121144 / Renee / Capachietti / F / 83 , 505 / Sales   Manager / 01 Nov1995  121145 / Dennis / Lansberry / M / 84 , 260 / Sales   Manager / 01 Apr1980  ; \n\ntitle   Orion Star Management Team ; \nproc   print   data = work . managers   noobs ;      format   HireDate   mmddyy10 . ; \nrun ; \ntitle ;", 
            "title": "Reading Nonstandard Delimited Data"
        }, 
        {
            "location": "/essentials/reading-raw/#validating-data", 
            "text": "When SAS encounters a data error, it prints messages and a ruler in the log and assigns a missing value to the affected variable. Then SAS continues processing.", 
            "title": "Validating Data"
        }, 
        {
            "location": "/essentials/reading-raw/#missing-values-between-delimiters-consecutive-delimiters", 
            "text": "1 INFILE  raw-data-file-name   DLM=  DSD;   The  DSD  option sets the default delimiter to a comma, treats consecutive delimiters as missing values and enables SAS to read values with embedded delimiters if the value is surrounded by quotation marks", 
            "title": "Missing Values between Delimiters (Consecutive Delimiters)"
        }, 
        {
            "location": "/essentials/reading-raw/#missing-values-at-the-end-of-a-line", 
            "text": "1 INFILE  raw-data-file-name  MISSOVER;   With the  MISSOVER  option, if SAS reaches the end of a record without finding values for all fields, variables without values are set to missing.", 
            "title": "Missing Values at the End of a Line"
        }, 
        {
            "location": "/essentials/manipulation/", 
            "text": "Chapter summary in SAS\n\n\nUsing SAS Functions\n\n\nSUM\n Summation Function\n\n\n1\nSUM(argument1, argument2, ...)\n\n\n\n\n\n\n\n\nThe arguments must be numeric values\n\n\nThe \nSUM\n function ignores missing values, so if an argument has a missing value, the result of the \nSUM\n function is the sum of the nonmissing values\n\n\nIf you add two values by \n+\n, if one of them is missing, the result will be a missing value, which makes the \nSUM\n function a better choice\n\n\n\n\n\n\nDATE\n Function\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nYEAR(SAS-date)     \nQTR(SAS-date)\nMONTH(SAS-date)\nDAY(SAS-date)\nWEEKDAY(SAS-date)\nTODAY()                /* Obtain the current date and convert to SAS-date (no argument) */\nDATE()                 /* Obtain the current date and convert to SAS-date (no argument) */\nMDY(month, day, year)\n\n\n\n\n\n\n\n\nThe arguments must be numeric values (except from \nTODAY()\n and \nDATE()\n functions)\n\n\nYou can subtract dates: \nAgein2012=(Bday2012-Birth_Date)/365.25;\n\n\n\n\n\n\nCATX\n Concatenation Function\n\n\n1\nCATX(\n \n, First_Name, Last_Name)\n\n\n\n\n\n\nThe \nCATX\n function removes leading and trailing blanks, inserts delimiters, and returns a concatenated character string. In the code, you first specify a character string that is used as a delimiter between concatenated items.\n\n\n\n\nINTCK\n Time Interval Function\n\n\n1\nINTCK(\nyear\n, Hire_Date, \n01JAN2012\nd)\n\n\n\n\n\n\nThe \nINTCK\n function returns the number of interval boundaries of a given kind that lie between the two dates, times, or datetime values. In the code, you first specify the interval value.\n\n\n\n\nWhat happens when you define a new variable from another that you are gonna \nDROP\n in this DATA statement?\n\n\nThe \nDROP\n statement is a compile-time-only statement. SAS sets a drop flag for the dropped variables, but the variables are in the PDV and, therefore, are available for processing.\n\n\n\n\nConditional Processing\n\n\nIF-THEN-ELSE\n Conditional Structures\n\n\n1\n2\n3\nIF expression THEN statement;\nELSE IF expression THEN statement;\nELSE statement;\n\n\n\n\n\n\nIn the conditional expressions involving strings watch out for possible mixed case values where the condition may not be met:  \ncountry = UPCASE(country);\n to avoid problems\n\n\n\n\nExecuting Multiple Statements\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nIF expression THEN\n    DO;\n        executable statements;\n    END;\nELSE IF expression THEN\n    DO;\n        executable statements;\n    END;\n\n\n\n\n\n\n\n\nIn the \nDATA\n step, the first reference to a variable determines its length. The first reference to a new variable can be in a \nLENGTH\n statement, an \nassignment\n statement, or \nanother\n statement such as an \nINPUT\n statement. After a variable is created in the PDV, the length of the variable's first value doesn't matter. \n\n\nTo avoid truncation in a variable defined inside a conditional structure you can:\n\n\n\n\nDefine the longer string as the first condition\n\n\nAdd some blanks at the end of shorter strings to fit the longer one\n\n\nDefine the length explicitly before any other reference to the variable\n\n\n\n\n\n\nSELECT\n Group\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nSELECT(Gender);\n      WHEN(\nF\n) DO;\n         Gift1=\nScarf\n;\n         Gift2=\nPedometer\n;\n      END;\n      WHEN(\nM\n) DO;\n         Gift1=\nGloves\n;\n         Gift2=\nMoney Clip\n;\n      END;\n      OTHERWISE DO;\n         Gift1=\nCoffee\n;\n         Gift2=\nCalendar\n;\n      END;\nEND;\n\n\n\n\n\n\n\n\nThe \nSELECT\n statement executes one of several statements or groups of statements\n\n\nThe \nSELECT\n statement begins a \nSELECT\n group. They contain \nWHEN\n statements that identify SAS statements that are executed when a particular condition is true\n\n\nUse at least one \nWHEN\n statement in a \nSELECT\n group\n\n\nAn optional \nOTHERWISE\n statement specifies a statement to be executed if no \nWHEN\n condition is met\n\n\nAn \nEND\n statement ends a \nSELECT\n group\n\n\n\n\nAvoiding Duplicates\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n* Period has more than one register per patient and the calculation of time periods between each one and the reference;\nPROC SORT DATA=period;\n    BY pt periodmax periodvisit;\nRUN;\n\n* Keep only highest (last value after the sorting);\nDATA maxperiod;\n    SET period;\n    BY pt;\n    IF last.pt;\nRUN;", 
            "title": "Manipulating Data"
        }, 
        {
            "location": "/essentials/manipulation/#using-sas-functions", 
            "text": "", 
            "title": "Using SAS Functions"
        }, 
        {
            "location": "/essentials/manipulation/#sum-summation-function", 
            "text": "1 SUM(argument1, argument2, ...)    The arguments must be numeric values  The  SUM  function ignores missing values, so if an argument has a missing value, the result of the  SUM  function is the sum of the nonmissing values  If you add two values by  + , if one of them is missing, the result will be a missing value, which makes the  SUM  function a better choice", 
            "title": "SUM Summation Function"
        }, 
        {
            "location": "/essentials/manipulation/#date-function", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 YEAR(SAS-date)     \nQTR(SAS-date)\nMONTH(SAS-date)\nDAY(SAS-date)\nWEEKDAY(SAS-date)\nTODAY()                /* Obtain the current date and convert to SAS-date (no argument) */\nDATE()                 /* Obtain the current date and convert to SAS-date (no argument) */\nMDY(month, day, year)    The arguments must be numeric values (except from  TODAY()  and  DATE()  functions)  You can subtract dates:  Agein2012=(Bday2012-Birth_Date)/365.25;", 
            "title": "DATE Function"
        }, 
        {
            "location": "/essentials/manipulation/#catx-concatenation-function", 
            "text": "1 CATX(   , First_Name, Last_Name)   The  CATX  function removes leading and trailing blanks, inserts delimiters, and returns a concatenated character string. In the code, you first specify a character string that is used as a delimiter between concatenated items.", 
            "title": "CATX Concatenation Function"
        }, 
        {
            "location": "/essentials/manipulation/#intck-time-interval-function", 
            "text": "1 INTCK( year , Hire_Date,  01JAN2012 d)   The  INTCK  function returns the number of interval boundaries of a given kind that lie between the two dates, times, or datetime values. In the code, you first specify the interval value.   What happens when you define a new variable from another that you are gonna  DROP  in this DATA statement?  The  DROP  statement is a compile-time-only statement. SAS sets a drop flag for the dropped variables, but the variables are in the PDV and, therefore, are available for processing.", 
            "title": "INTCK Time Interval Function"
        }, 
        {
            "location": "/essentials/manipulation/#conditional-processing", 
            "text": "", 
            "title": "Conditional Processing"
        }, 
        {
            "location": "/essentials/manipulation/#if-then-else-conditional-structures", 
            "text": "1\n2\n3 IF expression THEN statement;\nELSE IF expression THEN statement;\nELSE statement;   In the conditional expressions involving strings watch out for possible mixed case values where the condition may not be met:   country = UPCASE(country);  to avoid problems", 
            "title": "IF-THEN-ELSE Conditional Structures"
        }, 
        {
            "location": "/essentials/manipulation/#executing-multiple-statements", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8 IF expression THEN\n    DO;\n        executable statements;\n    END;\nELSE IF expression THEN\n    DO;\n        executable statements;\n    END;    In the  DATA  step, the first reference to a variable determines its length. The first reference to a new variable can be in a  LENGTH  statement, an  assignment  statement, or  another  statement such as an  INPUT  statement. After a variable is created in the PDV, the length of the variable's first value doesn't matter.   To avoid truncation in a variable defined inside a conditional structure you can:   Define the longer string as the first condition  Add some blanks at the end of shorter strings to fit the longer one  Define the length explicitly before any other reference to the variable", 
            "title": "Executing Multiple Statements"
        }, 
        {
            "location": "/essentials/manipulation/#select-group", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 SELECT(Gender);\n      WHEN( F ) DO;\n         Gift1= Scarf ;\n         Gift2= Pedometer ;\n      END;\n      WHEN( M ) DO;\n         Gift1= Gloves ;\n         Gift2= Money Clip ;\n      END;\n      OTHERWISE DO;\n         Gift1= Coffee ;\n         Gift2= Calendar ;\n      END;\nEND;    The  SELECT  statement executes one of several statements or groups of statements  The  SELECT  statement begins a  SELECT  group. They contain  WHEN  statements that identify SAS statements that are executed when a particular condition is true  Use at least one  WHEN  statement in a  SELECT  group  An optional  OTHERWISE  statement specifies a statement to be executed if no  WHEN  condition is met  An  END  statement ends a  SELECT  group", 
            "title": "SELECT Group"
        }, 
        {
            "location": "/essentials/manipulation/#avoiding-duplicates", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 * Period has more than one register per patient and the calculation of time periods between each one and the reference;\nPROC SORT DATA=period;\n    BY pt periodmax periodvisit;\nRUN;\n\n* Keep only highest (last value after the sorting);\nDATA maxperiod;\n    SET period;\n    BY pt;\n    IF last.pt;\nRUN;", 
            "title": "Avoiding Duplicates"
        }, 
        {
            "location": "/essentials/combination/", 
            "text": "Chapter summary in SAS\n\n\nConcatenating Data Sets (Vertical Combination)\n\n\n1\n2\n3\nDATA SAS-data-set;\n    SET SAS-data-set1 SAS-data-set2 ...;\nRUN;\n\n\n\n\n\n\nCombine two different variables that are actually the same one\n\n\n1\n2\n3\nDATA SAS-data-set;\n    SET SAS-data-set1 (RENAME=(old-name1 = new-name1 old-name2 = new-name2)) SAS-data-set2 ...;\nRUN;\n\n\n\n\n\n\n\n\nThe name change affects the PDV and the output data set, but has no effect on the input data set\n\n\nThe \nvariable attributes\n are assigned from the \nfirst data set\n in the \nSET\n statement\n\n\nYou will get an \nerror\n in the \nDATA\n step if a variable is defined with \ndifferent data types\n in the files that you are trying to concatenate\n\n\n\n\nMerging SAS Data Sets (Horizontal Combination)\n\n\n\n\nIn a \none-to-one\n relationship, a single observation in one data set is related to one, and only one, observation in another data set based on the values of one or more common variables\n\n\nIn a \none-to-many\n relationship, a single observation in one data set is related to one or more observations in another data set\n\n\nIn a \nmany-to-one\n relationship, multiple observations in one data set are related to one observation in another data set\n\n\nIn a \nmany-to-many\n relationship, multiple observations in one data set are related to multiple observations in another data set\n\n\nSometimes the data sets have \nnon-matches\n: at least one observation in one of the data sets is unrelated to any observation in another data set based on the values of one or more common variables\n\n\n\n\nMerging SAS Data Sets One-to-One\n\n\n\n\nThe \nmatch-merging\n is a process based on the values of common variables\n\n\nData sets are merged in the order that they appear in the MERGE statement\n\n\nYou may need to \nSORT\n the files by the \nBY-variable(s)\n before merging the files\n\n\n\n\n1\n2\n3\n4\n5\nDATA SAS-data-set;\n    MERGE SAS-data-set1 (RENAME=(old-name1 = new-name1 ...)) SAS-data-set2 ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\nMerging SAS Data Sets One-to-Many\n\n\n1\n2\n3\n4\n5\nDATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\nIn a \none-to-many merge\n, does it matter which data set is listed first in the \nMERGE\n statement?\n\n\nWhen you reverse the order of the data sets in the \nMERGE\n statement, the results are the same, but the order of the variables is different. SAS performs a \nmany-to-one merge\n.\n\n\n\n\nMERGENOBY\n (\n= NOWARN (default) | WARN | ERROR\n) controls whether a message is issued when \nMERGE\n processing occurs without an associated \nBY\n statement\n\n\n\n\nPerforming a merge without a BY statement merges the observations based on their positions\n\n\nThis is almost never done intentionally and can lead to unexpected results\n\n\n\n\nMerging SAS Data Sets that Have Non-Matches\n\n\n1\n2\n3\n4\n5\nDATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\n\n\nAfter the merging, the output data set contains \nboth matches and non-matches\n\n\nYou want the new data set to contain only the observations that match across the input data sets, and not those ones that are missing in one of the data sets that you are merging\n\n\n\n\n1\n2\n3\n4\n5\n6\nDATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\n\n\nWhen you spefify the \nIN\n option after an input data set in the \nMERGE\n statement, SAS creates a \ntemporary numeric variable\n that indicates whether the data set contributed data to the current observation (0 = it did not contribute to the current observation, 1 = it did contribute)\n\n\nThese variables are only available \nduring execution\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\nDATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY \nDESCENDING\n BY-variable(s);\n    IF variable1 = 1 and variable2 = 1;     /* write only matches */\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\n\n\nMatches\n\n\n\n\n1\n2\nIF variable1 = 1 and variable2 = 1 \nIF variable1 and variable2\n\n\n\n\n\n\n\n\nNon-matches from either data set\n\n\n\n\n1\n2\nIF variable1 = 0 or not variable2 = 0\nIF not variable1 or not variable2\n\n\n\n\n\n\nE.g.:\n\n\n1\n2\n3\n4\n5\n6\n7\nDATA SAS-new-data-set1 SAS-new-data-set2;\n    MERGE SAS-data-set1 (in=var1) SAS-data-set2 (in=var2);\n    BY BY-variable(s);\n    IF var2 THEN OUTPUT SAS-new-data-set1;\n    ELSE IF var1 and not var2 THEN OUTPUT SAS-new-data-set2;\n    KEEP variable1 variable2 variable5 variable8;\nRUN;\n\n\n\n\n\n\nMerging SAS Data Sets Many-to-Many\n\n\nWith the macros \nmakewide.sas\n and \nmakelong.sas\n you can \n\n\n\n\nMake one of your data sets wide\n\n\nPerform a one-to-many merge with the other data set\n\n\nMake your resultant data set long to obtain the required result", 
            "title": "Combining SAS Data Sets"
        }, 
        {
            "location": "/essentials/combination/#concatenating-data-sets-vertical-combination", 
            "text": "1\n2\n3 DATA SAS-data-set;\n    SET SAS-data-set1 SAS-data-set2 ...;\nRUN;   Combine two different variables that are actually the same one  1\n2\n3 DATA SAS-data-set;\n    SET SAS-data-set1 (RENAME=(old-name1 = new-name1 old-name2 = new-name2)) SAS-data-set2 ...;\nRUN;    The name change affects the PDV and the output data set, but has no effect on the input data set  The  variable attributes  are assigned from the  first data set  in the  SET  statement  You will get an  error  in the  DATA  step if a variable is defined with  different data types  in the files that you are trying to concatenate", 
            "title": "Concatenating Data Sets (Vertical Combination)"
        }, 
        {
            "location": "/essentials/combination/#merging-sas-data-sets-horizontal-combination", 
            "text": "In a  one-to-one  relationship, a single observation in one data set is related to one, and only one, observation in another data set based on the values of one or more common variables  In a  one-to-many  relationship, a single observation in one data set is related to one or more observations in another data set  In a  many-to-one  relationship, multiple observations in one data set are related to one observation in another data set  In a  many-to-many  relationship, multiple observations in one data set are related to multiple observations in another data set  Sometimes the data sets have  non-matches : at least one observation in one of the data sets is unrelated to any observation in another data set based on the values of one or more common variables", 
            "title": "Merging SAS Data Sets (Horizontal Combination)"
        }, 
        {
            "location": "/essentials/combination/#merging-sas-data-sets-one-to-one", 
            "text": "The  match-merging  is a process based on the values of common variables  Data sets are merged in the order that they appear in the MERGE statement  You may need to  SORT  the files by the  BY-variable(s)  before merging the files   1\n2\n3\n4\n5 DATA SAS-data-set;\n    MERGE SAS-data-set1 (RENAME=(old-name1 = new-name1 ...)) SAS-data-set2 ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;", 
            "title": "Merging SAS Data Sets One-to-One"
        }, 
        {
            "location": "/essentials/combination/#merging-sas-data-sets-one-to-many", 
            "text": "1\n2\n3\n4\n5 DATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;   In a  one-to-many merge , does it matter which data set is listed first in the  MERGE  statement?  When you reverse the order of the data sets in the  MERGE  statement, the results are the same, but the order of the variables is different. SAS performs a  many-to-one merge .   MERGENOBY  ( = NOWARN (default) | WARN | ERROR ) controls whether a message is issued when  MERGE  processing occurs without an associated  BY  statement   Performing a merge without a BY statement merges the observations based on their positions  This is almost never done intentionally and can lead to unexpected results", 
            "title": "Merging SAS Data Sets One-to-Many"
        }, 
        {
            "location": "/essentials/combination/#merging-sas-data-sets-that-have-non-matches", 
            "text": "1\n2\n3\n4\n5 DATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;    After the merging, the output data set contains  both matches and non-matches  You want the new data set to contain only the observations that match across the input data sets, and not those ones that are missing in one of the data sets that you are merging   1\n2\n3\n4\n5\n6 DATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;    When you spefify the  IN  option after an input data set in the  MERGE  statement, SAS creates a  temporary numeric variable  that indicates whether the data set contributed data to the current observation (0 = it did not contribute to the current observation, 1 = it did contribute)  These variables are only available  during execution   1\n2\n3\n4\n5\n6\n7 DATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY  DESCENDING  BY-variable(s);\n    IF variable1 = 1 and variable2 = 1;     /* write only matches */\n     additional SAS statements \nRUN;    Matches   1\n2 IF variable1 = 1 and variable2 = 1 \nIF variable1 and variable2    Non-matches from either data set   1\n2 IF variable1 = 0 or not variable2 = 0\nIF not variable1 or not variable2   E.g.:  1\n2\n3\n4\n5\n6\n7 DATA SAS-new-data-set1 SAS-new-data-set2;\n    MERGE SAS-data-set1 (in=var1) SAS-data-set2 (in=var2);\n    BY BY-variable(s);\n    IF var2 THEN OUTPUT SAS-new-data-set1;\n    ELSE IF var1 and not var2 THEN OUTPUT SAS-new-data-set2;\n    KEEP variable1 variable2 variable5 variable8;\nRUN;", 
            "title": "Merging SAS Data Sets that Have Non-Matches"
        }, 
        {
            "location": "/essentials/combination/#merging-sas-data-sets-many-to-many", 
            "text": "With the macros  makewide.sas  and  makelong.sas  you can    Make one of your data sets wide  Perform a one-to-many merge with the other data set  Make your resultant data set long to obtain the required result", 
            "title": "Merging SAS Data Sets Many-to-Many"
        }, 
        {
            "location": "/essentials/summary/", 
            "text": "Chapter summary in SAS\n\n\nPROC FREQ\n to Create Summary Reports\n\n\n\n\nWhen you're summarizing data, there's no need to show a frequency distribution for variables that have a large number of distinct values\n\n\nFrequency distributions work best with variables whose values meet two criteria: variable with \ncategorical values\n and values are \nbest summarized by counts instead of averages\n\n\nVariables that have continuous numerical values, such as dollar amounts and dates, will need to be \ngrouped into categories\n by \napplying formats\n inside the \nPROC FREQ\n step (substitute an specific range of those values by a tag)\n\n\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set \noption\n(s)\n;\n    TABLE(S) variable(s) \n/ option(s)\n;\n    \nadditional\n \nstatements\n\nRUN;\n\n\n\n\n\n\n\n\nPROC FREQ\n produces frequency tables that report the distribution of any or all variable values in a SAS data set\n\n\nIn the \nTABLE(S)\n statement you specify the frequency tables to produce \n\n\nEach unique variable's value displayed in the 1\nst\n column of the output is called a \nlevel of the variable\n\n\n\n\nOne-way Frequency Tables\n\n\nTo create \none-way\n frequency tables you specify one or more variable names separated by space.\n\n\n\n\nWarning\n\n\nIf you omit the \nTABLE\n statement, SAS produces a one-way tables for every variable in the data set, which could be very messy if you have a lot of variables.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nPROC SORT DATA=SAS-data-set\n    OUT=SAS-data-set-sorted;\n    BY variable_sorted;\nRUN;\n\nPROC FREQ DATA=SAS-data-set-sorted ORDER=FREQ \noption(s)\n;\n    TABLES variable / NOCUM NOPERCENT OUT=custom-output-name;\n    BY variable_sorted;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\n\n\nNOCUM\n option supresses the display of  the cummulative frequency and cummulative percent values \n\n\nNOPERCENT\n option supresses the display of all percentages\n\n\nORDER=FREQ\n option orders the output in descending frequency order\n\n\nOUT=\n option saves the output data set with a custom name\n\n\nBY\n option produces a frequency table for each value of \nvariable_sorted\n (the data set must be sorted by the variable named in the statement)\n\n\n\n\nCrosstabulation Tables\n\n\n\n\nSometimes it is useful to view a single table with statistics for each distintic combination of values of the selected variables\n\n\nThe simplest crosstabulation table is a \ntwo-way table\n\n\n\n\n1\n2\n3\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable_rows * variable_columns / NOFREQ NOPERCENT NOROW NOCOL;\nRUN;\n\n\n\n\n\n\nInformation contained in crosstabulation tables (legend):\n\n\n\n\nFrequency\n: indicates the number of observations with the unique combination of values represented in that cell\n\n\nPercent\n: indicates the cell's percentage of the total frequency\n\n\nRow Pct\n: cell's percentage of the total frequency for its row\n\n\nCol Pct\n: cell's percentage of the total frequency for its column \n\n\n\nLIST\n option format: the first two columns specify each possible combination of the two variables; it displays the same statistics as the default \none-way frequency\n table\n\n\nCROSSLIST\n option format: it displays the same statistics as the default \ncrosstabulation\n table\n\n\n\n\nFormatting Variables in \nPROC FREQ\n\n\nThe \nFORMAT=\n option allows you to format the frequency value (to any SAS numeric format or a user-defined numeric format while its length is not more than 24) and to change the width of the column (e.g. to allow variable labels to fit in one line). \n\n\n1\n2\n3\n4\n5\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable1 * variable2 /\n    FORMAT = \nw\n.;\n    FORMAT variable1 $format-name.;    \nRUN;\n\n\n\n\n\n\nThe \nFORMAT=\n option applies only to crosstabulation tables displayed in the default format. It doesn't apply to crosstabulation tables produced with the \nLIST\n/\nCROSSLIST\n option.\n\n\nUsing the \nMEANS\n and \nUNIVARIATE\n Procedures\n\n\nPROC MEANS\n produces summary reports with descriptive statistics and you can create statistics for groups of observations\n\n\n\n\nIt automatically displays output in a report and you can also save the output in a SAS data set\n\n\nIt reports the \nnumber of nonmissing values\n of the analysis variable (N), and the \nmean\n, the \nstandard deviation\n and \nminimum\n/\nmaximum values\n of every numeric variable in the data set\n\n\nThe variables in the \nCLASS\n statement are called \nclassification variables\n or \nclass variables\n (they typically have few discrete values)\n\n\nEach combination of class variable values is called a \nclass level\n\n\nThe data set \ndoesn't need to be sorted\n or indexed by the class variables\n\n\nN Obs\n reports the number of observations with each unique combination of class variables, whether or not there are missing values (if these \nN Obs\n are identical to \nN\n, there are no missing values in you data set)\n\n\n\n\n1\n2\n3\n4\nPROC MEANS DATA=SAS-data-set \nstatistic(s)\n;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;\n\n\n\n\n\n\nTo write the report in a new data set (including total addition):\n\n\n1\n2\n3\n4\n5\nPROC MEANS DATA=SAS-data-set NOPRINT NWAY;\n    OUTPUT OUT=SAS-new-data-set SUM=addition-new-variable;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;\n\n\n\n\n\n\nFormat options: \n\n\n\n\nMAXDEC=number\n (default format = \nBESTw.\n) \n\n\nNONOBS\n\n\nFW=number\n: specifies that the field width for all columns is \nnumber\n\n\nPRINTALLTYPES\n: displays statistics for all requested combination of class variables\n\n\n\n\n\n\n\n\nHow to Use these Procedures for Data Validation\n\n\nPROC FREQ\n\n\nYou can use a \nPROC FREQ\n step with the \nTABLES\n statement to detect invalud numeric and character data by looking at distinct values. The \nFREQ\n procedure \nlists all discrete values\n for a variable and \nreports its missing values\n.\n\n\n1\n2\n3\nPROC FREQ DATA=SAS-data-set \nORDER=FREQ\n;\n    TABLES variable;\nRUN;\n\n\n\n\n\n\n\n\nYou can check for non-expected variable's values\n\n\nYou can check for missing values\n\n\nYou can find duplicated values\n\n\n\n\n\n\nThe table showing the \nNumber of Variable Levels\n can indicate whether a variable contains duplicate/missing/non-expected values:\n\n\n1\n2\n3\nPROC FREQ DATA=SAS-data-set NLEVELS;\n    TABLES variable / NOPRINT;\nRUN;\n\n\n\n\n\n\n\n\nYou can use a \nWHERE\n statement to print out only the invalid values to be checked:\n\n\n1\n2\n3\n4\n5\n6\nPROC PRINT DATA=SAS-data-set;\n    WHERE gender NOT IN (\nF\n,\nM\n) OR\n          job_title IS NULL OR\n          salary NOT BETWEEN 24000 AND 500000 OR\n          employee IS MISSING;\nRUN;\n\n\n\n\n\n\n\n\nYou can output the tables to a new data set instead of displaying it:\n\n\n1\n2\n3\nPROC FREQ DATA=SAS-data-set NOPRINT;\n   TABLE variable / OUT=SAS-new-data-set;\nRUN;\n\n\n\n\n\n\nPROC MEANS\n\n\n\n\nThe \nMIN\n/\nMAX\n values can be useful to check if the data is within a range\n\n\nNMISS\n option displays the number of observations with missing values\n\n\n\n\nPROC UNIVARIATE\n\n\nPROC UNIVARIATE\n is a procedure that is useful for detecting data outliers that also produces summary reports of \ndescriptive statistics\n.\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\n    ID variable_to_relate;\n    HISTOGRAM variables \n/options\n;\n    PROBPLOT variables \n/options\n;\n    INSET keywords \n/options\n;\nRUN;\n\n\n\n\n\n\n\n\nIf you omit the \nVAR\n statement, all numeric variables in the data set are analyzed\n\n\nThe \nExtreme Observations\n table contains useful information to locate outliers: it displays the 5 lowest/highest values by default along with the corresponding observation number. The \nID\n statement specifies that SAS will use this variable as a label in the table of extreme observations and as an identifier for any extreme.\n\n\nTo specify the number of listed observations you can use \nNEXTROBS=\n\n\nHISTOGRAM\n/\nPROBPLOT\n options: \nnormal(mu=est sigma=est)\n creates a normal curve overlay to the histogram using the estimates of the population mean and standard deviation\n\n\nINSET\n writes a legend for the graph. \n/ position=ne\n moves the \nINSET\n to the north-east corner of the graph.\n\n\n\n\nTo include in the report only one of the automatically produced tables:\n\n\n\n\nCheck the specific table name in the \nLOG information\n using \nODS TRACE ON\n:\n\n\n\n\n1\n2\n3\n4\n5\nODS TRACE ON;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;\nODS TRACE OFF;\n\n\n\n\n\n\n\n\nSelect the wanted table with \nODS SELECT\n:\n\n\n\n\n1\n2\n3\n4\nODS SELECT ExtremeObs;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;\n\n\n\n\n\n\nSummary of Validation Procedures\n\n\n\n\nUsing the SAS Output Delivery System (\nODS\n)\n\n\n1\n2\n3\nODS destination FILE=\nfilename\n \noptions\n;\n    \nSAS code to generate the report\n\nODS destination CLOSE;\n\n\n\n\n\n\n\n\nYou can have multiple destinations open and execute multiple procedures\n\n\nAll generated output will be sent to every open destination\n\n\nYou might not be able to view the file, or the most updated file, outside of SAS until you close the destination\n\n\n\n\nE.g.:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nODS\n \npdf\n \nFILE\n=\nC:/output/test.pdf\n;\n\n\n(...)\n\n\nODS\n \npdf\n \nCLOSE\n;\n\n\n\nODS\n \ncsvall\n \nFILE\n=\nC:/output/test.cvs\n;\n\n\nODS\n \nrtf\n \nFILE\n=\nC:/output/test.rtf\n;\n\n\n(...)\n\n\nODS\n \ncsvall\n \nCLOSE\n;\n\n\nODS\n \nrtf\n \nCLOSE\n;\n\n\n\n\n\n\n\nAllowed File Formats and Their Corresponding Destinations\n\n\n\n\nYou can also export a database to a different format:\n\n\n\n\nExport to \n*.csv\n:\n\n\n\n\n1\n2\n3\n4\n5\nPROC EXPORT DATA=sashelp.class\n    OUTFILE=\nc:\\temp\\sashelp class.csv\n\n    DBMS=CSV\n    REPLACE;\nRUN;\n\n\n\n\n\n\n\n\nExport to \n*.dat\n:\n\n\n\n\n1\n2\n3\n4\n5\ndata\n \n_null_\n;\n\n    \nset\n \nlibrary\n.\nSAS-data-set\n;\n\n    \nfile\n \nC:\\your-custom-path\\your-file-name.dat\n;\n\n    \nput\n \nvariable1\n \nvariable2\n \nvariable3\n;\n\n\nrun\n;", 
            "title": "Creating Summary Reports"
        }, 
        {
            "location": "/essentials/summary/#proc-freq-to-create-summary-reports", 
            "text": "When you're summarizing data, there's no need to show a frequency distribution for variables that have a large number of distinct values  Frequency distributions work best with variables whose values meet two criteria: variable with  categorical values  and values are  best summarized by counts instead of averages  Variables that have continuous numerical values, such as dollar amounts and dates, will need to be  grouped into categories  by  applying formats  inside the  PROC FREQ  step (substitute an specific range of those values by a tag)   1\n2\n3\n4 PROC FREQ DATA=SAS-data-set  option (s) ;\n    TABLE(S) variable(s)  / option(s) ;\n     additional   statements \nRUN;    PROC FREQ  produces frequency tables that report the distribution of any or all variable values in a SAS data set  In the  TABLE(S)  statement you specify the frequency tables to produce   Each unique variable's value displayed in the 1 st  column of the output is called a  level of the variable", 
            "title": "PROC FREQ to Create Summary Reports"
        }, 
        {
            "location": "/essentials/summary/#one-way-frequency-tables", 
            "text": "To create  one-way  frequency tables you specify one or more variable names separated by space.   Warning  If you omit the  TABLE  statement, SAS produces a one-way tables for every variable in the data set, which could be very messy if you have a lot of variables.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 PROC SORT DATA=SAS-data-set\n    OUT=SAS-data-set-sorted;\n    BY variable_sorted;\nRUN;\n\nPROC FREQ DATA=SAS-data-set-sorted ORDER=FREQ  option(s) ;\n    TABLES variable / NOCUM NOPERCENT OUT=custom-output-name;\n    BY variable_sorted;\n     additional statements \nRUN;    NOCUM  option supresses the display of  the cummulative frequency and cummulative percent values   NOPERCENT  option supresses the display of all percentages  ORDER=FREQ  option orders the output in descending frequency order  OUT=  option saves the output data set with a custom name  BY  option produces a frequency table for each value of  variable_sorted  (the data set must be sorted by the variable named in the statement)", 
            "title": "One-way Frequency Tables"
        }, 
        {
            "location": "/essentials/summary/#crosstabulation-tables", 
            "text": "Sometimes it is useful to view a single table with statistics for each distintic combination of values of the selected variables  The simplest crosstabulation table is a  two-way table   1\n2\n3 PROC FREQ DATA=SAS-data-set;\n    TABLES variable_rows * variable_columns / NOFREQ NOPERCENT NOROW NOCOL;\nRUN;   Information contained in crosstabulation tables (legend):   Frequency : indicates the number of observations with the unique combination of values represented in that cell  Percent : indicates the cell's percentage of the total frequency  Row Pct : cell's percentage of the total frequency for its row  Col Pct : cell's percentage of the total frequency for its column   LIST  option format: the first two columns specify each possible combination of the two variables; it displays the same statistics as the default  one-way frequency  table  CROSSLIST  option format: it displays the same statistics as the default  crosstabulation  table", 
            "title": "Crosstabulation Tables"
        }, 
        {
            "location": "/essentials/summary/#formatting-variables-in-proc-freq", 
            "text": "The  FORMAT=  option allows you to format the frequency value (to any SAS numeric format or a user-defined numeric format while its length is not more than 24) and to change the width of the column (e.g. to allow variable labels to fit in one line).   1\n2\n3\n4\n5 PROC FREQ DATA=SAS-data-set;\n    TABLES variable1 * variable2 /\n    FORMAT =  w .;\n    FORMAT variable1 $format-name.;    \nRUN;   The  FORMAT=  option applies only to crosstabulation tables displayed in the default format. It doesn't apply to crosstabulation tables produced with the  LIST / CROSSLIST  option.", 
            "title": "Formatting Variables in PROC FREQ"
        }, 
        {
            "location": "/essentials/summary/#using-the-means-and-univariate-procedures", 
            "text": "PROC MEANS  produces summary reports with descriptive statistics and you can create statistics for groups of observations   It automatically displays output in a report and you can also save the output in a SAS data set  It reports the  number of nonmissing values  of the analysis variable (N), and the  mean , the  standard deviation  and  minimum / maximum values  of every numeric variable in the data set  The variables in the  CLASS  statement are called  classification variables  or  class variables  (they typically have few discrete values)  Each combination of class variable values is called a  class level  The data set  doesn't need to be sorted  or indexed by the class variables  N Obs  reports the number of observations with each unique combination of class variables, whether or not there are missing values (if these  N Obs  are identical to  N , there are no missing values in you data set)   1\n2\n3\n4 PROC MEANS DATA=SAS-data-set  statistic(s) ;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;   To write the report in a new data set (including total addition):  1\n2\n3\n4\n5 PROC MEANS DATA=SAS-data-set NOPRINT NWAY;\n    OUTPUT OUT=SAS-new-data-set SUM=addition-new-variable;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;   Format options:    MAXDEC=number  (default format =  BESTw. )   NONOBS  FW=number : specifies that the field width for all columns is  number  PRINTALLTYPES : displays statistics for all requested combination of class variables", 
            "title": "Using the MEANS and UNIVARIATE Procedures"
        }, 
        {
            "location": "/essentials/summary/#how-to-use-these-procedures-for-data-validation", 
            "text": "", 
            "title": "How to Use these Procedures for Data Validation"
        }, 
        {
            "location": "/essentials/summary/#proc-freq", 
            "text": "You can use a  PROC FREQ  step with the  TABLES  statement to detect invalud numeric and character data by looking at distinct values. The  FREQ  procedure  lists all discrete values  for a variable and  reports its missing values .  1\n2\n3 PROC FREQ DATA=SAS-data-set  ORDER=FREQ ;\n    TABLES variable;\nRUN;    You can check for non-expected variable's values  You can check for missing values  You can find duplicated values    The table showing the  Number of Variable Levels  can indicate whether a variable contains duplicate/missing/non-expected values:  1\n2\n3 PROC FREQ DATA=SAS-data-set NLEVELS;\n    TABLES variable / NOPRINT;\nRUN;    You can use a  WHERE  statement to print out only the invalid values to be checked:  1\n2\n3\n4\n5\n6 PROC PRINT DATA=SAS-data-set;\n    WHERE gender NOT IN ( F , M ) OR\n          job_title IS NULL OR\n          salary NOT BETWEEN 24000 AND 500000 OR\n          employee IS MISSING;\nRUN;    You can output the tables to a new data set instead of displaying it:  1\n2\n3 PROC FREQ DATA=SAS-data-set NOPRINT;\n   TABLE variable / OUT=SAS-new-data-set;\nRUN;", 
            "title": "PROC FREQ"
        }, 
        {
            "location": "/essentials/summary/#proc-means", 
            "text": "The  MIN / MAX  values can be useful to check if the data is within a range  NMISS  option displays the number of observations with missing values", 
            "title": "PROC MEANS"
        }, 
        {
            "location": "/essentials/summary/#proc-univariate", 
            "text": "PROC UNIVARIATE  is a procedure that is useful for detecting data outliers that also produces summary reports of  descriptive statistics .  1\n2\n3\n4\n5\n6\n7 PROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\n    ID variable_to_relate;\n    HISTOGRAM variables  /options ;\n    PROBPLOT variables  /options ;\n    INSET keywords  /options ;\nRUN;    If you omit the  VAR  statement, all numeric variables in the data set are analyzed  The  Extreme Observations  table contains useful information to locate outliers: it displays the 5 lowest/highest values by default along with the corresponding observation number. The  ID  statement specifies that SAS will use this variable as a label in the table of extreme observations and as an identifier for any extreme.  To specify the number of listed observations you can use  NEXTROBS=  HISTOGRAM / PROBPLOT  options:  normal(mu=est sigma=est)  creates a normal curve overlay to the histogram using the estimates of the population mean and standard deviation  INSET  writes a legend for the graph.  / position=ne  moves the  INSET  to the north-east corner of the graph.   To include in the report only one of the automatically produced tables:   Check the specific table name in the  LOG information  using  ODS TRACE ON :   1\n2\n3\n4\n5 ODS TRACE ON;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;\nODS TRACE OFF;    Select the wanted table with  ODS SELECT :   1\n2\n3\n4 ODS SELECT ExtremeObs;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;", 
            "title": "PROC UNIVARIATE"
        }, 
        {
            "location": "/essentials/summary/#summary-of-validation-procedures", 
            "text": "", 
            "title": "Summary of Validation Procedures"
        }, 
        {
            "location": "/essentials/summary/#using-the-sas-output-delivery-system-ods", 
            "text": "1\n2\n3 ODS destination FILE= filename   options ;\n     SAS code to generate the report \nODS destination CLOSE;    You can have multiple destinations open and execute multiple procedures  All generated output will be sent to every open destination  You might not be able to view the file, or the most updated file, outside of SAS until you close the destination   E.g.:  1\n2\n3\n4\n5\n6\n7\n8\n9 ODS   pdf   FILE = C:/output/test.pdf ;  (...)  ODS   pdf   CLOSE ;  ODS   csvall   FILE = C:/output/test.cvs ;  ODS   rtf   FILE = C:/output/test.rtf ;  (...)  ODS   csvall   CLOSE ;  ODS   rtf   CLOSE ;", 
            "title": "Using the SAS Output Delivery System (ODS)"
        }, 
        {
            "location": "/essentials/summary/#allowed-file-formats-and-their-corresponding-destinations", 
            "text": "You can also export a database to a different format:   Export to  *.csv :   1\n2\n3\n4\n5 PROC EXPORT DATA=sashelp.class\n    OUTFILE= c:\\temp\\sashelp class.csv \n    DBMS=CSV\n    REPLACE;\nRUN;    Export to  *.dat :   1\n2\n3\n4\n5 data   _null_ ; \n     set   library . SAS-data-set ; \n     file   C:\\your-custom-path\\your-file-name.dat ; \n     put   variable1   variable2   variable3 ;  run ;", 
            "title": "Allowed File Formats and Their Corresponding Destinations"
        }, 
        {
            "location": "/statistics/introduction/", 
            "text": "Chapter summary in SAS\n\n\nBasic Statistical Concepts\n\n\nDescriptive statistics (exploratory data analysis, EDA)\n\n\n\n\nExplore your data\n\n\n\n\nInferential statistics (explanatory modelling)\n\n\n\n\nHow is X related to Y?\n\n\nSample sizes are typically small and include few variables\n\n\nThe focus is on the parameters of the model\n\n\nTo assess the model, you use p-values and confidence intervals\n\n\n\n\nPredictive modelling\n\n\n\n\nIf you know X, can you predict Y?\n\n\nSample sizes are large and include many predictive (input) variables\n\n\nThe focus is on the predictions of observations rather than the parameters of the model\n\n\nTo assess a predictive model, you validate predictions using holdout sample data\n\n\n\n\n\n\n\n\nParameters\n: numerical values (typically unknown, you can't measure the entire population) that summarize characteristics of a population (greek letters)\n\n\nStatistics\n: summarizes characteristics of a sample (standard alphabet)\n\n\nYou use \nstatistics\n to estimate \nparameters\n\n\n\n\n\n\n\n\nIndependent variable\n: it can take different values, it affects or determines a \ndependent variable\n. It can be called predictor, explanatory, control or input variable.\n\n\nDependent variable\n: it can take different values in response to an \nindependent variable\n. Also known as response, outcome or target variable.\n\n\n\n\n\n\nScale of measurement\n: variable's classification\n\n\n\n\nQuantitative/numerical variables\n: counts or measurements, you can perform arithmetical operations with it\n\n\nDiscrete data\n: variables that can have only a countable number of values within a measurement range\n\n\nContinuous data\n: variables that are measured on a scale that has infinite number of values and has no breaks or jumps\n\n\nInterval scale data\n: it can be rank-ordered like ordinal data but also has a sensible spacing of observations such that differenes between measurements are meaningful but it lacks a true zero (ratios are meaningless)\n\n\nRatio scale data\n: it is rank-ordered with meaningful spacing and also includes a true zero point and can therefore accurately indicate the ratio difference between two spaces on the measurement scale\n\n\n\n\n\n\n\n\n\n\nCategorical/attribute variables\n: variables that denote groupings or labels\n\n\nNominal data (qualitative/classification variable)\n: exhibits no ordering within its observed levels, groups or categories\n\n\nOrdinal data\n: the observed labels can be ordered in some meaningful way that implies that the differences between the groups or categories are due to magnitude\n\n\n\n\n\n\n\n\n\n\n\n\nUnivariate analysis\n provides techniques for analyzing and describing a sigle variable. It reveals patterns in the data by looking at the \nrange\n of values, measures of \ndispersion\n, the \ncentral tendecy\n of the values and \nfrequency distribution\n.\n\n\nBivariate analysis\n describes and explains the relationships between two variables and how they change or covary together. It include techniques such as \ncorrelation analysis\n and \nchi-square tests of independance\n.\n\n\nMultivariate/Multivariable analysis\n examines two or more variables at the same time in order to understand the relationships among them. \n\n\nTechniques such as \nmutiple linear regression\n and n-way \nANOVA\n are typically called \nmultivariable\n analysis (only one response variable). \n\n\nTechniques such as \nfactora analysis\n and \nclustering\n are typically called \nmutivariate\n analysis (they consider more than one response variable).\n\n\n\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\nMeasures of central tendencies\n: mean (affected by outliers), median (less sensitive to outliers), mode\n\n\n\n\n\n\n\n\nPercentile\n\n\nQuartile\n\n\n\n\n\n\n\n\n\n\n\n\n25th\n\n\n1st / lower / Q1\n\n\n\n\n\n\n\n\n50th\n\n\n2nd / middle / Q2\n\n\nMedian\n\n\n\n\n\n\n75th\n\n\n3rd / upper / Q3\n\n\n\n\n\n\n\n\n\n\nThe \ninterquartile range (IQR)\n is the difference between Q1 and Q3, it is a \nrobust estimate of the variability\n because changes in the upper/lower 25% of the data do not affect it. If there are \noutliers\n in the data, then the IQR is a more reliable measure of the spread than the overall range.\n\n\nThe \ncoefficient of variation (CV)\n is a measure of the standard deviation expressed as a percentage of the mean ($c_v = \\sigma / 100 \\mu$).\n\n\n\n\nNormal distribution\n\n\n\n\n\n\n\n\nIntervals\n\n\nPercentage contained\n\n\n\n\n\n\n\n\n\n\n$\\mu \\pm \\sigma$\n\n\n68%\n\n\n\n\n\n\n$\\mu \\pm 2 \\sigma$\n\n\n95%\n\n\n\n\n\n\n$\\mu \\pm 3 \\sigma$\n\n\n99%\n\n\n\n\n\n\n\n\nHow to check the normality of a sample?\n\n\n\n\nCompare the \nmean\n and the \nmedian\n: if they are nearly equal, that is an indicator of symmetry (requirement for normality)\n\n\nCheck that \nskewness\n and \nkurtosis\n are close to 0:\n\n\nIf both are greater than 1 or less than -1: data is not normal\n\n\nIf either is greater than 2 or less than -2: data is not normal\n\n\n\n\n\n\n\n\nStatistical summaries\n \n\n\nSkewness\n and \nkurtosis\n measure certain aspects of the shape of a distribution (they are \n0\n and \n3\n for a normal distribution, although SAS has standardized both to 0)\n\n\n\n\nSkewness\n measures the tendency of your data to be more spread out on one side of the mean than on the other (asymmetry of the distribution). \n\n\nYou can think of the direction of skewness as the direction the data is trailing off to. \n\n\nA \nright-skewed\n distribution tells us that the mean is \ngreater than the median\n.\n\n\n\n\n\n\nKurtosis\n measures the tendency of your data to be concentrated toward the center or toward the tails of the distribution (peakedness of the data, tail thickness). \n\n\nA \nnegative kurtosis (platykurtic distribution)\n means that the data has lighter tails than in a normal distribution. \n\n\nA \npositive kurtosis (leptokurtic/heavy-tailed/outlier-prone distribution)\n means that the data has heavier tails and is more concentrated around the mean than a normal distribution.\n\n\nRectangular, bimodal and multimodal distributions tend to have low values of kurtosis.\n\n\nAsymmetric distributions\n also tend to have nonzero kurtosis. In these cases, understanding kurtosis is considerably more complex and can be difficult to assess visually.\n\n\n\n\n\n\n\n\n\n\nPROC SURVEYSELECT\n\n\nHow to generate random (representative) samples (population subsets):\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC SURVEYSELECT DATA=SAS-data-set \n                  OUT=name-of-output-data-set\n                  METHOD=method-of-random-sampling\n                  SEED=seed-value \n                  SAMPSIZE=number-of-observations-desired;\n     \nSTRATA stratification-variable(s);\n\nRUN;\n\n\n\n\n\n\n\n\nMETHOD\n specifies the random sampling method to be used. For simple random sampling without replacement, use \nMETHOD=SRS\n. For simple random sampling with replacement, use \nMETHOD=URS\n. For other selection methods and details on sampling algorithms, see the SAS online documentation for \nPROC SURVEYSELECT\n.\n\n\nSEED\n specifies the initial seed for random number generation. If no \nSEED\n option is specified, SAS uses the system time as its seed value. This creates a different random sample every time the procedure is run.\n\n\nSAMPSIZE\n indicates the number of observations to be included in the sample. To select a certain fraction of the original data set rather than a given number of observations, use the \nSAMPRATE\n option.\n\n\n\n\nPicturing Your Data\n\n\nPROC UNIVARIATE\n\n\nPlots that can be produced with this procedure:\n\n\n\n\nHistograms\n\n\nNormal probability plots\n: expected percentiles from standard normal vs actual data values\n\n\n\n\n\n\nPROC SGSCATTER\n\n\nPlots that can be produced with this procedure:\n\n\n\n\nScatter plots\n: you can create a \nsingle-cell\n (simple Y by X) scatter plot, a \nmulti-cell\n scatter plot with multiple independent scatter plots in a grid and a \nscatter plot matrix\n, which produces a matrix of scatter plots comparing multiple variables.\n\n\n\n\nPROC SGPLOT\n\n\nPlots that can be produced with this procedure:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nPROC SGPLOT DATA=SAS-data-set \noptions\n;\n        DOT category-variable \n/options\n;\n        HBAR category-variable \n/options\n;\n        VBAR category-variable \n/options\n;\n        HBOX response-variable \n/options\n;\n        VBOX response-variable \n/options\n;\n        HISTOGRAM response-variable \n/options\n;\n        SCATTER X=variable Y=variable \n/options\n;\n        NEEDLE X=variable Y=numeric-variable \n/options\n;\n        REG X=numeric-variable Y=numeric-variable \n/options\n;\nRUN;\n\n\n\n\n\n\nAnywhere in the procedure you can add \nreference lines\n:\n\n\n1\n2\n3\n4\nREFLINE variable | value-1 \n...\n \nvalue-n\n \n/option(s)\n\n\n/* Example: */\nREFLINE 1200 / axis=y lineattrs=(color=blue);\n\n\n\n\n\n\n\n\nNote\n\n\nThe order on which you define the parts of the plot will the determined the order on which it is displayed (if you want to send a \nREFLINE\n to the back, define it first)\n\n\n\n\n\n\nScatter plots (\nSCATTER\n)\n\n\nLine graphs\n\n\nHistograms (\nHISTOGRAM\n)\n with overlaid distribution curves\n\n\nRegression lines (\nREG\n)\n with confidence and prediction bands\n\n\nDot plots (\nDOT\n)\n\n\nBox plots (\nHBOX\n/\nVBOX\n)\n: it makes it easy to see how spread out your data is and if there are any outliers. The box represents the middle 50% of your data (IQR). The lower/middle/upper \nline of the box\n represent Q1/Q2/Q3. The \ndiamond\n denotes the mean (easy to check how close the mean is to the median). The \nwhiskers\n extend as far as the data extends to a maximum length of 1.5 times the IQR above Q3. Any data points farther than this distance are considered possible outliers and are represented in this plot as \ncircles\n.\n\n\nBar charts (\nHBAR\n/\nVBAR\n)\n\n\nNeedle plot (\nNEEDLE\n)\n: creates a plot with needles connecting each point to the baseline\n\n\nYou can also \noverlay plots together\n to produce many different types of graphs\n\n\n\n\nPROC SGPANEL\n\n\nPlots that can be produced with this procedure:\n\n\n\n\nPanels of plots\n for different levels of a factor or several different time periods depending on the classification variable\n\n\nSide-by-side histograms\n which provide a visual comparison for your data\n\n\n\n\nPROC SGRENDER\n\n\n\n\nPlots from graphs templates you have modified or written yourself\n\n\n\n\n\n\nTo specify options for graphs you submit the \nODS GRAPHICS\n statement:\n\n\n1\nODS GRAPHICS ON \noptions\n;\n\n\n\n\n\n\n\n\nTo select/exclude specific test results, graphs or tables from you output, you can use \nODS SELECT\n and \nODS EXCLUDE\n statements.\n\n\nYou can use ODS templates to modify the layout and details of each graph\n\n\nYou can use ODS styles to control the general appearance and consistency of yous graphs and tables (by default \nHTMLBLUE\n).\n\n\n\n\nAnother way to control your output is to use the \nPLOTS\n option which is usually available in the procedure statement:\n\n\n1\nPROC UNIVARIATE DATA=SAS-data-set PLOTS=options;\n\n\n\n\n\n\nThis option enables you to specify which graphs SAS should create, either in addtion or instead of the default plots.\n\n\nConfidence Intervals for the Mean\n\n\n\n\nA \npoint estimator\n is a sample statistic used to estimate a population parameter\n\n\nAn estimator takes on different values from sample to sample, so it's important to know its variance\n\n\nA statistic that measures the variability of your estimator is the \nstandard error\n\n\nIt differs from the standard deviation: the \nstandard deviation\n deals with the variability of your data while \nstandard error\n deals with the variability of you sample statistic\n\n\n\n\nE.g.:\n $standard \\ error \\ of \\ the \\ mean = standard \\ deviation/ \\sqrt{sample \\ size}$\n\n\nThe \ndistribution of sample means\n is always less variable than the data.\n\n\n\n\nBecause we know that point estimators vary from sample to sample, it would be nice to have an estimator of the mean that directly accounts for this natural variability\n\n\nThe \ninterval estimator\n gives us a range of values that is likely to contain the population mean\n\n\nIt is calculated from the \nstandard error\n and a value that is determined by the \ndegree of certainty\n we require (\nsignificance level\n)\n\n\nConfidence intervals\n are a type of interval estimator used to estimate the population mean\n\n\nYou can make the confidence interval narrower by increasing the sample size and by decreasing the confidence level\n\n\n\n\n$CI = sample \\ mean \\pm quantile \\cdot standard \\ error$\n\n\n\n\nThe \nCLM\n option of \nPROC MEANS\n calculates the confidence limits for the mean, you can add \nALPHA=\n to change the default 0.05 value for a 95% confidence level\n\n\nThe \ncentral limit theorem\n states that the distribution of sample means is approximately normal regardless of the population distribution's shape, if the sample size is large enough (~30 observations)\n\n\n\n\nHypothesis Testing\n\n\n\n\nThe \nnull hypothesis\n ($H_0$) is what you assume to be true when you start your analysis\n\n\nThe \nalternative hypothesis\n ($H_a$ or $H_1$) is your initial research hypothesis, that is, your proposed explanation\n\n\n\n\nDecision-making process:\n\n\n\n\nDefine null and alternative hypothesis\n\n\nSpecify significance level (type I error rate)\n\n\nCollect data\n\n\nReject or fail to reject the null hypothesis\n\n\n\n\n\n\n\n\nThe type I and II errors are \ninversely related\n: as one type increases the other decreases \n\n\nThe \npower\n is the probability of a \ncorrect rejection\n = 1 - \n\n\nIt is the ability of the statistical test to detect a true difference\n\n\n\n\nIt is the ability to successfully reject a false null hypothesis\n\n\n\n\n\n\nA \np-value\n measures the probability of observing a value as extreme as the one observed\n\n\n\n\nThe p-value is used to determine \nstatistical significance\n\n\nIt helps you assess whether you should reject the null hypothesis\n\n\n\n\n\n\n\n\nThe \np-value\n is affected by:\n\n\n\n\nThe \neffect size\n: the difference between the observed statistic and the hypothesized value\n\n\nThe \nsample size\n: the larger the sample size, the more sure you are about the sample statistics, the lower the p-value is\n\n\n\n\n\n\n\n\nA reference distribution enables you to quantify the probability (p-value) of observing a particular outcome (the calculated sample statistic) or a more extreme outcome, if the nul hypothesis is true\n\n\n\n\nTwo common reference distributions for statistical hypothesis testing are the \nt distribution\n and the \nF distribution\n\n\nThese distributions are characterized by the \ndegrees of freedom\n associated with your data\n\n\nThe \nt distribution\n arises when you're making inferences about a population mean and the population standard deviation is unknown and has to be estimated from the data\n\n\nIt is \napproximately normal\n as the \nsample size grows larger\n\n\nThe t distribution is a \nsymmetric distribution\n like the normal distribution except that the t distribution has \nthicker tails\n\n\nThe \nt statistic\n is positive/negative when the sample is more/less than the hypothesized mean\n\n\nIf the data doesn't come from a normal distribution, then the t statistic approximately follows a t distribution as long as the sample size is large (\ncentral limit theorem\n)\n\n\n\n\n\n\n\n\nCalculation with \nPROC UNIVARIATE\n:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nODS SELECT TESTSFORLOCATION;\nPROC UNIVARIATE DATA=SAS-data-set MU0=number alpha=number;\n  VAR variable(s);\n  ID variable_to_relate;\n  HISTOGRAM variables \n/options\n;\n  PROBPLOT variables \n/options\n;\n  INSET keywords \n/options\n;\nRUN;\n\n\n\n\n\n\n\n\nTESTSFORLOCATION\n displays only the p-values calculation\n\n\nBy default \nMU0 = 0", 
            "title": "Introduction"
        }, 
        {
            "location": "/statistics/introduction/#basic-statistical-concepts", 
            "text": "Descriptive statistics (exploratory data analysis, EDA)   Explore your data   Inferential statistics (explanatory modelling)   How is X related to Y?  Sample sizes are typically small and include few variables  The focus is on the parameters of the model  To assess the model, you use p-values and confidence intervals   Predictive modelling   If you know X, can you predict Y?  Sample sizes are large and include many predictive (input) variables  The focus is on the predictions of observations rather than the parameters of the model  To assess a predictive model, you validate predictions using holdout sample data     Parameters : numerical values (typically unknown, you can't measure the entire population) that summarize characteristics of a population (greek letters)  Statistics : summarizes characteristics of a sample (standard alphabet)  You use  statistics  to estimate  parameters     Independent variable : it can take different values, it affects or determines a  dependent variable . It can be called predictor, explanatory, control or input variable.  Dependent variable : it can take different values in response to an  independent variable . Also known as response, outcome or target variable.    Scale of measurement : variable's classification   Quantitative/numerical variables : counts or measurements, you can perform arithmetical operations with it  Discrete data : variables that can have only a countable number of values within a measurement range  Continuous data : variables that are measured on a scale that has infinite number of values and has no breaks or jumps  Interval scale data : it can be rank-ordered like ordinal data but also has a sensible spacing of observations such that differenes between measurements are meaningful but it lacks a true zero (ratios are meaningless)  Ratio scale data : it is rank-ordered with meaningful spacing and also includes a true zero point and can therefore accurately indicate the ratio difference between two spaces on the measurement scale      Categorical/attribute variables : variables that denote groupings or labels  Nominal data (qualitative/classification variable) : exhibits no ordering within its observed levels, groups or categories  Ordinal data : the observed labels can be ordered in some meaningful way that implies that the differences between the groups or categories are due to magnitude       Univariate analysis  provides techniques for analyzing and describing a sigle variable. It reveals patterns in the data by looking at the  range  of values, measures of  dispersion , the  central tendecy  of the values and  frequency distribution .  Bivariate analysis  describes and explains the relationships between two variables and how they change or covary together. It include techniques such as  correlation analysis  and  chi-square tests of independance .  Multivariate/Multivariable analysis  examines two or more variables at the same time in order to understand the relationships among them.   Techniques such as  mutiple linear regression  and n-way  ANOVA  are typically called  multivariable  analysis (only one response variable).   Techniques such as  factora analysis  and  clustering  are typically called  mutivariate  analysis (they consider more than one response variable).      Descriptive Statistics  Measures of central tendencies : mean (affected by outliers), median (less sensitive to outliers), mode     Percentile  Quartile       25th  1st / lower / Q1     50th  2nd / middle / Q2  Median    75th  3rd / upper / Q3      The  interquartile range (IQR)  is the difference between Q1 and Q3, it is a  robust estimate of the variability  because changes in the upper/lower 25% of the data do not affect it. If there are  outliers  in the data, then the IQR is a more reliable measure of the spread than the overall range.  The  coefficient of variation (CV)  is a measure of the standard deviation expressed as a percentage of the mean ($c_v = \\sigma / 100 \\mu$).   Normal distribution     Intervals  Percentage contained      $\\mu \\pm \\sigma$  68%    $\\mu \\pm 2 \\sigma$  95%    $\\mu \\pm 3 \\sigma$  99%     How to check the normality of a sample?   Compare the  mean  and the  median : if they are nearly equal, that is an indicator of symmetry (requirement for normality)  Check that  skewness  and  kurtosis  are close to 0:  If both are greater than 1 or less than -1: data is not normal  If either is greater than 2 or less than -2: data is not normal     Statistical summaries    Skewness  and  kurtosis  measure certain aspects of the shape of a distribution (they are  0  and  3  for a normal distribution, although SAS has standardized both to 0)   Skewness  measures the tendency of your data to be more spread out on one side of the mean than on the other (asymmetry of the distribution).   You can think of the direction of skewness as the direction the data is trailing off to.   A  right-skewed  distribution tells us that the mean is  greater than the median .    Kurtosis  measures the tendency of your data to be concentrated toward the center or toward the tails of the distribution (peakedness of the data, tail thickness).   A  negative kurtosis (platykurtic distribution)  means that the data has lighter tails than in a normal distribution.   A  positive kurtosis (leptokurtic/heavy-tailed/outlier-prone distribution)  means that the data has heavier tails and is more concentrated around the mean than a normal distribution.  Rectangular, bimodal and multimodal distributions tend to have low values of kurtosis.  Asymmetric distributions  also tend to have nonzero kurtosis. In these cases, understanding kurtosis is considerably more complex and can be difficult to assess visually.", 
            "title": "Basic Statistical Concepts"
        }, 
        {
            "location": "/statistics/introduction/#proc-surveyselect", 
            "text": "How to generate random (representative) samples (population subsets):  1\n2\n3\n4\n5\n6\n7 PROC SURVEYSELECT DATA=SAS-data-set \n                  OUT=name-of-output-data-set\n                  METHOD=method-of-random-sampling\n                  SEED=seed-value \n                  SAMPSIZE=number-of-observations-desired;\n      STRATA stratification-variable(s); \nRUN;    METHOD  specifies the random sampling method to be used. For simple random sampling without replacement, use  METHOD=SRS . For simple random sampling with replacement, use  METHOD=URS . For other selection methods and details on sampling algorithms, see the SAS online documentation for  PROC SURVEYSELECT .  SEED  specifies the initial seed for random number generation. If no  SEED  option is specified, SAS uses the system time as its seed value. This creates a different random sample every time the procedure is run.  SAMPSIZE  indicates the number of observations to be included in the sample. To select a certain fraction of the original data set rather than a given number of observations, use the  SAMPRATE  option.", 
            "title": "PROC SURVEYSELECT"
        }, 
        {
            "location": "/statistics/introduction/#picturing-your-data", 
            "text": "", 
            "title": "Picturing Your Data"
        }, 
        {
            "location": "/statistics/introduction/#proc-univariate", 
            "text": "Plots that can be produced with this procedure:   Histograms  Normal probability plots : expected percentiles from standard normal vs actual data values", 
            "title": "PROC UNIVARIATE"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgscatter", 
            "text": "Plots that can be produced with this procedure:   Scatter plots : you can create a  single-cell  (simple Y by X) scatter plot, a  multi-cell  scatter plot with multiple independent scatter plots in a grid and a  scatter plot matrix , which produces a matrix of scatter plots comparing multiple variables.", 
            "title": "PROC SGSCATTER"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgplot", 
            "text": "Plots that can be produced with this procedure:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 PROC SGPLOT DATA=SAS-data-set  options ;\n        DOT category-variable  /options ;\n        HBAR category-variable  /options ;\n        VBAR category-variable  /options ;\n        HBOX response-variable  /options ;\n        VBOX response-variable  /options ;\n        HISTOGRAM response-variable  /options ;\n        SCATTER X=variable Y=variable  /options ;\n        NEEDLE X=variable Y=numeric-variable  /options ;\n        REG X=numeric-variable Y=numeric-variable  /options ;\nRUN;   Anywhere in the procedure you can add  reference lines :  1\n2\n3\n4 REFLINE variable | value-1  ...   value-n   /option(s) \n\n/* Example: */\nREFLINE 1200 / axis=y lineattrs=(color=blue);    Note  The order on which you define the parts of the plot will the determined the order on which it is displayed (if you want to send a  REFLINE  to the back, define it first)    Scatter plots ( SCATTER )  Line graphs  Histograms ( HISTOGRAM )  with overlaid distribution curves  Regression lines ( REG )  with confidence and prediction bands  Dot plots ( DOT )  Box plots ( HBOX / VBOX ) : it makes it easy to see how spread out your data is and if there are any outliers. The box represents the middle 50% of your data (IQR). The lower/middle/upper  line of the box  represent Q1/Q2/Q3. The  diamond  denotes the mean (easy to check how close the mean is to the median). The  whiskers  extend as far as the data extends to a maximum length of 1.5 times the IQR above Q3. Any data points farther than this distance are considered possible outliers and are represented in this plot as  circles .  Bar charts ( HBAR / VBAR )  Needle plot ( NEEDLE ) : creates a plot with needles connecting each point to the baseline  You can also  overlay plots together  to produce many different types of graphs", 
            "title": "PROC SGPLOT"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgpanel", 
            "text": "Plots that can be produced with this procedure:   Panels of plots  for different levels of a factor or several different time periods depending on the classification variable  Side-by-side histograms  which provide a visual comparison for your data", 
            "title": "PROC SGPANEL"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgrender", 
            "text": "Plots from graphs templates you have modified or written yourself    To specify options for graphs you submit the  ODS GRAPHICS  statement:  1 ODS GRAPHICS ON  options ;    To select/exclude specific test results, graphs or tables from you output, you can use  ODS SELECT  and  ODS EXCLUDE  statements.  You can use ODS templates to modify the layout and details of each graph  You can use ODS styles to control the general appearance and consistency of yous graphs and tables (by default  HTMLBLUE ).   Another way to control your output is to use the  PLOTS  option which is usually available in the procedure statement:  1 PROC UNIVARIATE DATA=SAS-data-set PLOTS=options;   This option enables you to specify which graphs SAS should create, either in addtion or instead of the default plots.", 
            "title": "PROC SGRENDER"
        }, 
        {
            "location": "/statistics/introduction/#confidence-intervals-for-the-mean", 
            "text": "A  point estimator  is a sample statistic used to estimate a population parameter  An estimator takes on different values from sample to sample, so it's important to know its variance  A statistic that measures the variability of your estimator is the  standard error  It differs from the standard deviation: the  standard deviation  deals with the variability of your data while  standard error  deals with the variability of you sample statistic   E.g.:  $standard \\ error \\ of \\ the \\ mean = standard \\ deviation/ \\sqrt{sample \\ size}$  The  distribution of sample means  is always less variable than the data.   Because we know that point estimators vary from sample to sample, it would be nice to have an estimator of the mean that directly accounts for this natural variability  The  interval estimator  gives us a range of values that is likely to contain the population mean  It is calculated from the  standard error  and a value that is determined by the  degree of certainty  we require ( significance level )  Confidence intervals  are a type of interval estimator used to estimate the population mean  You can make the confidence interval narrower by increasing the sample size and by decreasing the confidence level   $CI = sample \\ mean \\pm quantile \\cdot standard \\ error$   The  CLM  option of  PROC MEANS  calculates the confidence limits for the mean, you can add  ALPHA=  to change the default 0.05 value for a 95% confidence level  The  central limit theorem  states that the distribution of sample means is approximately normal regardless of the population distribution's shape, if the sample size is large enough (~30 observations)", 
            "title": "Confidence Intervals for the Mean"
        }, 
        {
            "location": "/statistics/introduction/#hypothesis-testing", 
            "text": "The  null hypothesis  ($H_0$) is what you assume to be true when you start your analysis  The  alternative hypothesis  ($H_a$ or $H_1$) is your initial research hypothesis, that is, your proposed explanation   Decision-making process:   Define null and alternative hypothesis  Specify significance level (type I error rate)  Collect data  Reject or fail to reject the null hypothesis     The type I and II errors are  inversely related : as one type increases the other decreases   The  power  is the probability of a  correct rejection  = 1 -   It is the ability of the statistical test to detect a true difference   It is the ability to successfully reject a false null hypothesis    A  p-value  measures the probability of observing a value as extreme as the one observed   The p-value is used to determine  statistical significance  It helps you assess whether you should reject the null hypothesis     The  p-value  is affected by:   The  effect size : the difference between the observed statistic and the hypothesized value  The  sample size : the larger the sample size, the more sure you are about the sample statistics, the lower the p-value is     A reference distribution enables you to quantify the probability (p-value) of observing a particular outcome (the calculated sample statistic) or a more extreme outcome, if the nul hypothesis is true   Two common reference distributions for statistical hypothesis testing are the  t distribution  and the  F distribution  These distributions are characterized by the  degrees of freedom  associated with your data  The  t distribution  arises when you're making inferences about a population mean and the population standard deviation is unknown and has to be estimated from the data  It is  approximately normal  as the  sample size grows larger  The t distribution is a  symmetric distribution  like the normal distribution except that the t distribution has  thicker tails  The  t statistic  is positive/negative when the sample is more/less than the hypothesized mean  If the data doesn't come from a normal distribution, then the t statistic approximately follows a t distribution as long as the sample size is large ( central limit theorem )     Calculation with  PROC UNIVARIATE :  1\n2\n3\n4\n5\n6\n7\n8 ODS SELECT TESTSFORLOCATION;\nPROC UNIVARIATE DATA=SAS-data-set MU0=number alpha=number;\n  VAR variable(s);\n  ID variable_to_relate;\n  HISTOGRAM variables  /options ;\n  PROBPLOT variables  /options ;\n  INSET keywords  /options ;\nRUN;    TESTSFORLOCATION  displays only the p-values calculation  By default  MU0 = 0", 
            "title": "Hypothesis Testing"
        }, 
        {
            "location": "/statistics/anova/", 
            "text": "Chapter summary in SAS\n\n\n\n\nGraphical Analysis of Associations\n\n\n\n\nBefore analyzing your data, you need to have a general idea of any associations between \npredictor variables\n and \nresponse variables\n\n\nAn \nassociation\n exists between two variables when the expected value of one variable differs at different levels of the other variable\n\n\nOne method for doing this is to conduct a \ngraphical analysis\n of your data\n\n\nAssociations between \ncategorical\n predictor variable and a \ncontinuous\n response variable can be explored with \nSGPLOT\n to product \nbox plots\n (box-and-whisker plots) (\nX\n predictor variable vs \nY\n response variable)\n\n\nIf the \nregression line\n conecting the means of Y at each value of X is not horizontal \nthere might be an association\n between them\n\n\nIf the \nregression line\n is horizontal \nthere is no association\n: knowing the value of X doesn't tell you anything about the value of Y\n\n\n\n\nPROC SGPLOT\n\n\n1\n2\n3\n4\nPROC SGPLOT DATA=SAS-data-set;\n    VBOX response-variable / CATEGORY=predictor-variable\n    CONNECT=MEAN DATALABEL=outlier-ID-variable;\nRUN;\n\n\n\n\n\n\nTwo-Sample t-Tests\n\n\n\n\nYou can use a \none-sample t-test\n to determine if the mean of a population is equal to a particular value or not\n\n\nWhen you collect a random sample of independent observations from two different populations, you can perform a \ntwo-sample t-test\n\n\n\n\nWhen you compare the means of two populations using a \ntwo-sample t-test\n you make three assumptions:\n\n\n\n\nThe data contains independent observations\n\n\nThe distributions of the two populations are normal (check histograms and normal probability/Q-Q plots)\n\n\nThe variances in these normal distributions are equal (\nF-test\n is the formal way to verify this assumption)\n\n\n\n\n$F$ statistic: $F=max(s_1^2,s_2^2)/min(s_1^2,s_2^2) \\ge 1$\n\n\n$H_0$: \n$_1^2$ $=$  \n$_2^2\\rightarrow F \\approx 1$\n\n\n$H_a$: \n$_1^2$ $\\ne$  \n$_2^2\\rightarrow F\\gt 1$\n\n\nThe \nPr\nF\n value in the \nEquality of Variances\n table represents the \np-value\n of the F-test for equal variances\n\n\n\n\nTwo-sided Tests\n\n\nPROC TTEST\n\n\n\n\nPROC TTEST\n performs a two-sided two-sample t-test by default (confidence limits and ODS graphics included)\n\n\nIt \nautomatically test the assumption of equal variances\n and provides an exact two-sample t-test (\npooled\n) when the assumptions are met and an approximate t-test (\nscatterthwaite\n) when it is not met \n\n\nThe pooled and scatterthwaite t-tests are equal when the variances are equal\n\n\n\n\n1\n2\n3\n4\n5\nPROC TTEST DATA=SAS-data-set \noptions\n\n    plots(shownull)=interval;         \\* shownull = vertical reference line at the mean value of H0 *\\\n    CLASS variable;                   \\* Classification variable *\\\n    VAR variable(s);                  \\* Continuous response variables *\\\nRUN;\n\n\n\n\n\n\n\n\nOne-sided Tests\n\n\n\n\nIt \ncan increase the power\n of a statistical test, meaning that if you are right about the direction of the true difference, you will more likely detect a significant difference with a one-sided test than with a tow-sided test\n\n\nThe difference between the mean values for the null hypothesis will be defined by the alphabetical order of the classification variables (e.g.: female - male)\n\n\n\n\n1\n2\n3\n4\n5\nPROC TTEST DATA=SAS-data-set \n    plots(only shownull)=interval H0=0 SIDES=u;     \\* only = suppress the default plots; u/l = upper/lower-tailed t-test  *\\\n    CLASS variable;                                 \\* Classification variable *\\\n    VAR variable(s);                                \\* Continuous response variables *\\\nRUN;\n\n\n\n\n\n\nOne-Way ANOVA\n\n\nWhen you want to determine whether there are significant differences between the \nmeans of two or more populations\n, you can use analysis of variance (ANOVA).\n\n\n\n\nYou have a continuous dependent (\nresponse\n) variable and a categorical independent (\npredictor\n) variable\n\n\nYou can have \nmany levels of the predictor variable\n, but you can have \nonly one predictor variable\n\n\nThe \nsquared value of the t statistic\n for a two-sample t-test is equal to the \nF statistic\n of a one-way ANOVA with two populations\n\n\nWith ANOVA the $H_0$ is that all of the population means are equal and $H_a$ is that not all the population means are equal (at least one mean is different)\n\n\n\n\nTo perform an ANOVA test you make three assumptions:\n\n\n\n\nYou have a \ngood, random, representative sample\n\n\nThe \nerror terms are normally distributed\n\n\nThe \nresiduals\n (each observation minus its group mean) are estimates of the error term in the model so you verify this assumption by examining diagnostic plots of the residuals (if they are approximately normal, the error terms will be too)\n\n\nIf your sample sizes are reasonably large and approximately equal across groups, then only severe departures from normality are considered a problem\n\n\nResiduals always sum to 0, regardless of the number of observations.\n\n\n\n\n\n\nThe \nerror terms have equal variances\n across the predictor variable levels: you can conduct a formal test for equal variances and also plot the residuals vs predicted values as a way to graphically verify this assumption\n\n\n\n\nPROC GLM\n\n\nYou can use \nPROC GLM\n to verify the ANOVA assumptions and perform the ANOVA test. It fits a general linear model of which ANOVA is a special case and also displays the sums of squares associated with each hypothesis it tests.\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);     /* print each plot on a separated page */\n    CLASS variable(s);\n    MODEL dependents=intependents \n/options\n;\n    MEANS effects / HOVTEST \n/options\n;    \nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nHOVTEST\n: homogeneity of variance test option (Levene's test by default) + plot of residuals vs predicted values (means)\n\n\n\n\n\n\n\n\nIf the \nbetween-group variability\n is significantly larger than the \nwithin-group variability\n, you reject the null that all the group means are equal\n\n\nYou partition out the variability using sums of squares: \n\n\nBetween-group\n variation: also called Model Sum of Squares (SSM): $\\sum n_i (\\overline Y_i- \\overline {\\overline Y})^2$\n\n\nWithin-group\n variation: also called Error Sum of Squares (SSE): $\\sum \\sum (Y_{ij}- \\overline Y_i)^2$\n\n\nTotal\n variation: also called the Total Sum of Squares (SST): $\\sum \\sum (Y_{ij}- \\overline {\\overline Y})^2$\n\n\n\n\n\n\nSSM\n and \nSSE\n represent pieces of \nSST\n: the SSM is the variability explanied by the predictor variable levels and SSE the variability not explained by the predictor variable levels\n\n\nYou want the larger piece of the total to be better represented by what you can explain (SSM) vs what you can't explain (SSE) \n\n\n\n\nANOVA with Data from a Randomized Block Design\n\n\nIn an \nobservational study\n, you often examine what already occurred, and therefore have little control over factors contributing to the outcome. In a \ncontrolled experiment\n, you can manipulate the \nfactors of interest\n and can more reasonably claim causation.\n\n\n\n\nThe variation due to the \nnuisance factors\n (fundamental to the probabilistic model but are no longer of interest) is part of the random variation that the error sum of squares accounts for.\n\n\nIncluding a \nblocking variable\n in the model is in essence like adding a second predictor variable to the model in terms of the way you write it\n\n\nThe way you set up your experiment and data collection is what defines it as a blocking factor\n\n\nAlthough you're not specifically interested in its effect, \ncontrolling the blocking variable makes it easier to detect an effect of the factor of interest\n\n\nIn a model that does not include a blocking variable, its effects are lumped into the error term of the model (unaccounted for variation)\n\n\nWhen you include a blocking variable in your ANOVA model, any effects caused by the nuisance factors that are common within a sector are accounted for in the \nmodel sum of squares rather than the error sum of squares\n\n\n\n\nYou make two more assumptions when you include a blocking factor in the model:\n\n\n\n\nPrimary variable levels are \nrandomly assigned\n within each block\n\n\nThe effects of the primary variable are \nconstant across the levels\n of the blocking factor (the effects don't depend on the block they are in, there are \nno interactions\n with the blocking variable)\n\n\n\n\n\n\nNote\n\n\nLevene's test for homogeneity is \nonly available for one-way ANOVA models\n, so in this case, you have to use the Residuals by Predicted plot.\n\n\n\n\nPROC GLM\n\n\n1\n2\n3\n4\n5\n6\nPROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);   /* print each plot on a separated page */\n    CLASS variable(s) blocking-factor(s);\n    MODEL dependents=intependents blocking-factor(s)\n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nRule of thumb\n: if the \nF-value is \n 1\n, then it helped to add the blocking factor in your model \n\n\nIf you compare the MSE (\nMean Square\n in the table) without and with including the blocking variable in the model,  there is a drop of its value meaning that \nyou have been able to account for a bit more of the unexplained variability due to the nuisance factors\n helping o have more precise estimates of the effect of your primary variable\n\n\nIt is also reflected in the \nR-Square\n value that is increased when a blocking factor is added to the model\n\n\nThanks to adding a blocking variable to your model you can get your primary variable to be significant\n\n\nThe \nType III SS\n at the bottom of the output tests for the difference due to each variable, controlling for or adjusting for the other variable\n\n\n\n\nANOVA Post Hoc Tests\n\n\nThis test is used to determine which means differ from other means and control the error rate using \nmultiple comparison method\n.\n\n\nAssuming the null hypothesis is true for your different comparisons, the probability that you conclude a difference exist at least one time when there really  isn't a difference increases with the more tests you perform. So \nthe chance that you make a Type I error increases each time you conduct a statistical test\n.\n\n\n\n\nThe \ncomparisonwise error rate (CER)\n is the probability of a Type I error on a single pairwise test (\n)\n\n\nThe \nexperimentwise error rate (EER)\n is the probability of making at least one Type I error when performing the whole set of comparisons. It takes into consideration the number of pairwise comparisons you make, so it increases as the number of tests increase: \n\n \n EER=1-(1-\\alpha)^{comparisons} \n \n\n\n\n\nTukey's Multiple Comparison Method\n\n\n\n\nThis method, which is also known as the \nHonestly Significant Difference\n test, is a popular multiple comparison test that \ncontrols the EER\n\n\nThis tests compares all possible pairs of means, so \nit can only be used when you make pairwise comparisons\n\n\nThis method controls $EER=\\alpha$ when \nall possible pairwise comparisons are considered\n and controls $EER\n\\alpha$ when fewer than all pairwise comparisons are considered\n\n\n\n\nDunnett's Multiple Comparison Method\n\n\n\n\nThis method is a specialised multiple comparison test that allows you to \ncompare a single control group to all other groups\n\n\nIt controls $EER \\le \\alpha$ when all groups are compared to the reference group (control)\n\n\nIt accounts for the correlation that exists between the comparisons and \nyou can conduct one-sided tests\n of hypothesis against the reference group\n\n\n\n\nPROC GLM\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPROC GLM DATA=SAS-data-set;\n    CLASS variable(s);\n    MODEL dependents=intependents \n/options\n;\n    LSMEANS effects \n/options-test-1\n;  \n    LSMEANS effects \n/options-test-2\n;\n    [...]\n    LSMEANS effects \n/options-test-n\n;  \nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nPDIFF=ALL\n requests p-values for the differences between \nALL\n the means and a \ndiffogram\n is produced automatically displaying all pairwise least square means differences and indicating which are significant\n\n\nIt can be undestood as a least squares mean by least squares mean plot\n\n\nThe point estimates for differences between the means for each pairwise comparison can be found at the intersections of the gray grid lines (intersection of appropriate indexes)\n\n\nThe red/blue diagonal lines show the \nconfidence intervals for the true differences of the means\n for each pairwise comparison\n\n\nThe grey 45$^{\\circ}$ reference line represents equality of the means (if the confidence interval crosses over it, then there is no significant difference between the two groups and the diagonal line for the pair will be \ndashed and red\n; if the difference is significant the line will be \nsolid and blue\n)\n\n\n\n\n\n\n\n\nThe \nADJUST=\n option specifies the adjustment method for multiple comparisons\n\n\nIf you don't specify an option SAS uses the \nTukey method by default\n, if you specify \nADJUST=Dunnett\n the GLM procedure produces multiple comparisons using \nDunnett's method\n and a \ncontrol plot\n \n\n\nThe control plot displays the least squares mean and confidence limits of each group compared to the reference group \n\n\nThe middle \nhorizontal line represents its least square mean value\n (you can see the arithmetic mean value un the \nupper right corner\n of the graph)\n\n\nThe \nshaded area\n goes from the \nlower decision limit (LDL)\n to the \nupper decision limit (UDL)\n\n\nThere is a vertical line for each group that you're comparing to the reference (control) group. If a \nvertical line extends past the shaded area\n, then the group represented by the line is \nsignificantly different\n (small p-value) than the reference group \n\n\n\n\n\n\n\n\n\n\n\n\nPDIFF=CONTROLU('value')\n specifies the control group for the Dunnett's case: the direction of the sign in Ha is the same as the direction you are testing, so this is a \none-sided upper-tailed t-test\n\n\nIf you specify \nADJUST=T\n SAS will make no adjustments for multiple comparisons: is not recommended as there's a tendency to find \nmore significant pairwise differences than might actually exist\n\n\n\n\nTwo-Way ANOVA with Interactions\n\n\nWhen you have a continuous response variable and \ntwo categorical predictor variables\n, you use the \ntwo-way ANOVA model\n\n\n\n\nEffect\n: the magnitude of the expected change in the response variable presumably caused by the change in value of a predictor variable in the model\n\n\nIn addition, the variables in a model can be referred to as effects or terms\n\n\nMain effect\n: is the effect of a single predictor variable\n\n\nInteraction effects\n: when the relationship of the response variable with a predictor changes with the changing of another predictor variable (the effect of one variable depends on the value of the other variable)\n\n\n\n\n\n\nWhen you consider an ANOVA with more than one predictor variable, it's called \nn-way ANOVA\n where \nn\n represents the number of predictor variables\n\n\n\n\nThe analysis in a \nrandomized block design\n is actually a \nspecial type of two-way ANOVA\n in which you have one factor of interest and one blocking factor\n\n\nWhen you analyze a two-way ANOVA with interactions, you first look at any tests for \ninteractions among the factors\n\n\nIf there is \nno interaction between the factors\n you can interpret the tests for the individual factor effects to determine their significance/non-significance\n\n\nIf an \ninteraction exists between any factors\n, the tests for the individual factor effects might be misleading due to masking of these effect by the interaction (this is specially true for unbalanced data with different number of observations for each combination of groups)\n\n\n\n\n\n\nWhen the interaction is not statistically significant \nyou can analyze the main effect with the model in its current form\n (generally the method you use when you analyze designed experiments)\n\n\nEven when you analyze designed experiments, some statisticians might suggest that if the interaction is not significant, \nyou can delete the interaction effect from your model, rerun the model and then just analyze the main effects\n increasing the power of the main effects test\n\n\nIf the \ninteraction term is significant\n, it is good practice to keep the main effect terms that make up the interaction in the model, whether they are significant or not (this preserves model hierarchy)\n\n\nYou have to make the \nsame three assumptions used in the ANOVA test\n\n\nThe interaction terms are also called \nproduct terms\n or \ncrossed effects\n\n\n\n\nPROC GLM\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC GLM DATA=SAS-data-set;\n    CLASS independent1 independent2;\n    MODEL dependent = independent1 independent2 independent1*independent2;\n    or\n    MODEL dependent = independent1 | independent2;\nRUN;\nQUIT;\n\n\n\n\n\n\nThis program is \nfitting to this model\n:\n\n\n\n\nY_{ijk}=\\mu + \\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+\\epsilon_{ijk}\n\n\n\n\n dependent = overall mean + intependent1 + independent2 + interaction12 + unaccounted for variation \n\n\n\n\nIn \nmost situations\n you will want to use the \nType III SS\n\n\nThe \nType I SS (sequential)\n are the sums of squares you obtain from fitting the effects in the order you specify in the model \n\n\nThe \nType III SS (marginal)\n are the sums of squares you obtain from fitting each effect after all the other terms in the model, that is the sums of squares for each effect corrected for the other terms in the model\n\n\nWhen examining these results you first have to \nlook at the interaction term and if it's significant\n (p-value), the \nmain effects don't tell you the whole story\n. It that is the case, you don't need to worry all that much about the significance of the main effects at this point for two reasons:\n\n\nYou know that the effect of each variable1 level changes for the different variable2 levels\n\n\nYou want to include the main effects in the model, whether they are significant or not, to preserve model hierarchy\n\n\n\n\n\n\nYou can analyze the interaction between terms by looking at the \ninteraction plot\n that SAS produces by default when you include an interaction term in the model\n\n\nTo analyze and interpret the effect of one of the interacting variables you need to add the \nLSMEANS\n statement to your program\n\n\n\n\n1\n2\n3\n4\n5\n6\nPROC GLM DATA=SAS-data-set ORDER=INTERNAL PLOTS(ONLY)=INTPLOT;\n    CLASS independent1 independent2;\n    MODEL dependent = intependent1 independent2 independent1*independent2;\n    LSMEANS independent1*independent2 / SLICE= independent1;\nRUN;\nQUIT;\n\n\n\n\n\n\nSAS creates two types of mean plots when you use the \nLSMEANS\n statement with an interaction term:\n\n\n\n\nThe first plot displays the \nleast squares mean (LS-Mean) for every effect level\n \n\n\nThe second plot contains the same information rearranged so you can \nlook a little closer at the combination levels\n\n\n\n\n\n\nSTORE\n statement\n\n\nYou can add a \nSTORE\n statement to save your analysis results in an \nitem store\n (a binary file format that cannot be modified). This allows you to \nrun post-processing analysis\n on the stored results even if you no longer have access to the original data set. The \nSTORE\n statement applies to the following SAS/STAT procedures: \nGENMOD\n, \nGLIMMIX\n, \nGLM\n, \nGLMSELECT\n, \nLOGISTIC\n, \nMIXED\n, \nORTHOREG\n, \nPHREG\n, \nPROBIT\n, \nSURVEYLOGISTIC\n, \nSURVEYPHREG\n, and \nSURVEYREG\n.\n\n\n1\n2\nSTORE \nOUT\n=\nitem-store-name\n    \n/ LABEL=\nlabel\n;\n\n\n\n\n\n\n\n\nitem-store-name\n is a usual one- or two-level SAS name, similar to the names that are used for SAS data sets\n\n\nlabel\n identifies the estimate on the output (is optional)\n\n\n\n\n\n\nPROC PLM\n\n\nTo perform post-fitting statistical analysis and plotting for the contents of the store item, you use \nPROC PLM\n. The statements and options that are available vary depending upon which procedure you used to produce the item store.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nPROC PLM RESTORE=item-store-specification \noptions\n;\n    EFFECTPLOT INTERACTION(SLICEBY=variable) \nplot-type\n \n(plot-definition\n \noptions)\n / CLM \n/ options\n;\n    LSMEANS \nmodel-effects\n \n/ options\n;\n    LSMESTIMATE model-effect \nlabel\n values\n        \ndivisor\n=n\n,...\nlabel\n values\n        \ndivisor\n=n\n \n/ options\n;\n    SHOW options;\n    SLICE model-effect / SLICEBY=variable ADJUST=tukey \n/ options\n;\n    WHERE expression;\nRUN;\n\n\n\n\n\n\n\n\nRESTORE\n specifies the source item store for processing\n\n\nEFFECTPLOT\n produces a display of the fitted model and provides options for changing and enhancing the displays\n\n\nLSMEANS\n computes and compares least squares means (LS-means) of fixed effects\n\n\nLSMESTIMATE\n provides custom hypothesis tests among least squares means\n\n\nSHOW\n uses \nODS\n to display contents of the item store. This statement is useful for verifying that the contents of the item store apply to the analysis and for generating \nODS\n tables.\n\n\nSLICE\n provides a general mechanism for performing a partitioned analysis of the LS-means for an interaction (analysis of simple effects) and it uses the same options as the \nLSMEANS\n statement\n\n\nWHERE\n is used in the PLM procedure when the item store contains \nBY-variable\n information and you want to apply the \nPROC PLM\n statements to only a subset of the BY groups", 
            "title": "Analysis of Variance (ANOVA)"
        }, 
        {
            "location": "/statistics/anova/#graphical-analysis-of-associations", 
            "text": "Before analyzing your data, you need to have a general idea of any associations between  predictor variables  and  response variables  An  association  exists between two variables when the expected value of one variable differs at different levels of the other variable  One method for doing this is to conduct a  graphical analysis  of your data  Associations between  categorical  predictor variable and a  continuous  response variable can be explored with  SGPLOT  to product  box plots  (box-and-whisker plots) ( X  predictor variable vs  Y  response variable)  If the  regression line  conecting the means of Y at each value of X is not horizontal  there might be an association  between them  If the  regression line  is horizontal  there is no association : knowing the value of X doesn't tell you anything about the value of Y", 
            "title": "Graphical Analysis of Associations"
        }, 
        {
            "location": "/statistics/anova/#proc-sgplot", 
            "text": "1\n2\n3\n4 PROC SGPLOT DATA=SAS-data-set;\n    VBOX response-variable / CATEGORY=predictor-variable\n    CONNECT=MEAN DATALABEL=outlier-ID-variable;\nRUN;", 
            "title": "PROC SGPLOT"
        }, 
        {
            "location": "/statistics/anova/#two-sample-t-tests", 
            "text": "You can use a  one-sample t-test  to determine if the mean of a population is equal to a particular value or not  When you collect a random sample of independent observations from two different populations, you can perform a  two-sample t-test   When you compare the means of two populations using a  two-sample t-test  you make three assumptions:   The data contains independent observations  The distributions of the two populations are normal (check histograms and normal probability/Q-Q plots)  The variances in these normal distributions are equal ( F-test  is the formal way to verify this assumption)   $F$ statistic: $F=max(s_1^2,s_2^2)/min(s_1^2,s_2^2) \\ge 1$  $H_0$:  $_1^2$ $=$   $_2^2\\rightarrow F \\approx 1$  $H_a$:  $_1^2$ $\\ne$   $_2^2\\rightarrow F\\gt 1$  The  Pr F  value in the  Equality of Variances  table represents the  p-value  of the F-test for equal variances   Two-sided Tests", 
            "title": "Two-Sample t-Tests"
        }, 
        {
            "location": "/statistics/anova/#proc-ttest", 
            "text": "PROC TTEST  performs a two-sided two-sample t-test by default (confidence limits and ODS graphics included)  It  automatically test the assumption of equal variances  and provides an exact two-sample t-test ( pooled ) when the assumptions are met and an approximate t-test ( scatterthwaite ) when it is not met   The pooled and scatterthwaite t-tests are equal when the variances are equal   1\n2\n3\n4\n5 PROC TTEST DATA=SAS-data-set  options \n    plots(shownull)=interval;         \\* shownull = vertical reference line at the mean value of H0 *\\\n    CLASS variable;                   \\* Classification variable *\\\n    VAR variable(s);                  \\* Continuous response variables *\\\nRUN;    One-sided Tests   It  can increase the power  of a statistical test, meaning that if you are right about the direction of the true difference, you will more likely detect a significant difference with a one-sided test than with a tow-sided test  The difference between the mean values for the null hypothesis will be defined by the alphabetical order of the classification variables (e.g.: female - male)   1\n2\n3\n4\n5 PROC TTEST DATA=SAS-data-set \n    plots(only shownull)=interval H0=0 SIDES=u;     \\* only = suppress the default plots; u/l = upper/lower-tailed t-test  *\\\n    CLASS variable;                                 \\* Classification variable *\\\n    VAR variable(s);                                \\* Continuous response variables *\\\nRUN;", 
            "title": "PROC TTEST"
        }, 
        {
            "location": "/statistics/anova/#one-way-anova", 
            "text": "When you want to determine whether there are significant differences between the  means of two or more populations , you can use analysis of variance (ANOVA).   You have a continuous dependent ( response ) variable and a categorical independent ( predictor ) variable  You can have  many levels of the predictor variable , but you can have  only one predictor variable  The  squared value of the t statistic  for a two-sample t-test is equal to the  F statistic  of a one-way ANOVA with two populations  With ANOVA the $H_0$ is that all of the population means are equal and $H_a$ is that not all the population means are equal (at least one mean is different)   To perform an ANOVA test you make three assumptions:   You have a  good, random, representative sample  The  error terms are normally distributed  The  residuals  (each observation minus its group mean) are estimates of the error term in the model so you verify this assumption by examining diagnostic plots of the residuals (if they are approximately normal, the error terms will be too)  If your sample sizes are reasonably large and approximately equal across groups, then only severe departures from normality are considered a problem  Residuals always sum to 0, regardless of the number of observations.    The  error terms have equal variances  across the predictor variable levels: you can conduct a formal test for equal variances and also plot the residuals vs predicted values as a way to graphically verify this assumption", 
            "title": "One-Way ANOVA"
        }, 
        {
            "location": "/statistics/anova/#proc-glm", 
            "text": "You can use  PROC GLM  to verify the ANOVA assumptions and perform the ANOVA test. It fits a general linear model of which ANOVA is a special case and also displays the sums of squares associated with each hypothesis it tests.  1\n2\n3\n4\n5\n6\n7 PROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);     /* print each plot on a separated page */\n    CLASS variable(s);\n    MODEL dependents=intependents  /options ;\n    MEANS effects / HOVTEST  /options ;    \nRUN;\nQUIT;    HOVTEST : homogeneity of variance test option (Levene's test by default) + plot of residuals vs predicted values (means)     If the  between-group variability  is significantly larger than the  within-group variability , you reject the null that all the group means are equal  You partition out the variability using sums of squares:   Between-group  variation: also called Model Sum of Squares (SSM): $\\sum n_i (\\overline Y_i- \\overline {\\overline Y})^2$  Within-group  variation: also called Error Sum of Squares (SSE): $\\sum \\sum (Y_{ij}- \\overline Y_i)^2$  Total  variation: also called the Total Sum of Squares (SST): $\\sum \\sum (Y_{ij}- \\overline {\\overline Y})^2$    SSM  and  SSE  represent pieces of  SST : the SSM is the variability explanied by the predictor variable levels and SSE the variability not explained by the predictor variable levels  You want the larger piece of the total to be better represented by what you can explain (SSM) vs what you can't explain (SSE)", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#anova-with-data-from-a-randomized-block-design", 
            "text": "In an  observational study , you often examine what already occurred, and therefore have little control over factors contributing to the outcome. In a  controlled experiment , you can manipulate the  factors of interest  and can more reasonably claim causation.   The variation due to the  nuisance factors  (fundamental to the probabilistic model but are no longer of interest) is part of the random variation that the error sum of squares accounts for.  Including a  blocking variable  in the model is in essence like adding a second predictor variable to the model in terms of the way you write it  The way you set up your experiment and data collection is what defines it as a blocking factor  Although you're not specifically interested in its effect,  controlling the blocking variable makes it easier to detect an effect of the factor of interest  In a model that does not include a blocking variable, its effects are lumped into the error term of the model (unaccounted for variation)  When you include a blocking variable in your ANOVA model, any effects caused by the nuisance factors that are common within a sector are accounted for in the  model sum of squares rather than the error sum of squares   You make two more assumptions when you include a blocking factor in the model:   Primary variable levels are  randomly assigned  within each block  The effects of the primary variable are  constant across the levels  of the blocking factor (the effects don't depend on the block they are in, there are  no interactions  with the blocking variable)    Note  Levene's test for homogeneity is  only available for one-way ANOVA models , so in this case, you have to use the Residuals by Predicted plot.", 
            "title": "ANOVA with Data from a Randomized Block Design"
        }, 
        {
            "location": "/statistics/anova/#proc-glm_1", 
            "text": "1\n2\n3\n4\n5\n6 PROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);   /* print each plot on a separated page */\n    CLASS variable(s) blocking-factor(s);\n    MODEL dependents=intependents blocking-factor(s) /options ;\nRUN;\nQUIT;    Rule of thumb : if the  F-value is   1 , then it helped to add the blocking factor in your model   If you compare the MSE ( Mean Square  in the table) without and with including the blocking variable in the model,  there is a drop of its value meaning that  you have been able to account for a bit more of the unexplained variability due to the nuisance factors  helping o have more precise estimates of the effect of your primary variable  It is also reflected in the  R-Square  value that is increased when a blocking factor is added to the model  Thanks to adding a blocking variable to your model you can get your primary variable to be significant  The  Type III SS  at the bottom of the output tests for the difference due to each variable, controlling for or adjusting for the other variable", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#anova-post-hoc-tests", 
            "text": "This test is used to determine which means differ from other means and control the error rate using  multiple comparison method .  Assuming the null hypothesis is true for your different comparisons, the probability that you conclude a difference exist at least one time when there really  isn't a difference increases with the more tests you perform. So  the chance that you make a Type I error increases each time you conduct a statistical test .   The  comparisonwise error rate (CER)  is the probability of a Type I error on a single pairwise test ( )  The  experimentwise error rate (EER)  is the probability of making at least one Type I error when performing the whole set of comparisons. It takes into consideration the number of pairwise comparisons you make, so it increases as the number of tests increase:     EER=1-(1-\\alpha)^{comparisons}      Tukey's Multiple Comparison Method   This method, which is also known as the  Honestly Significant Difference  test, is a popular multiple comparison test that  controls the EER  This tests compares all possible pairs of means, so  it can only be used when you make pairwise comparisons  This method controls $EER=\\alpha$ when  all possible pairwise comparisons are considered  and controls $EER \\alpha$ when fewer than all pairwise comparisons are considered   Dunnett's Multiple Comparison Method   This method is a specialised multiple comparison test that allows you to  compare a single control group to all other groups  It controls $EER \\le \\alpha$ when all groups are compared to the reference group (control)  It accounts for the correlation that exists between the comparisons and  you can conduct one-sided tests  of hypothesis against the reference group", 
            "title": "ANOVA Post Hoc Tests"
        }, 
        {
            "location": "/statistics/anova/#proc-glm_2", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 PROC GLM DATA=SAS-data-set;\n    CLASS variable(s);\n    MODEL dependents=intependents  /options ;\n    LSMEANS effects  /options-test-1 ;  \n    LSMEANS effects  /options-test-2 ;\n    [...]\n    LSMEANS effects  /options-test-n ;  \nRUN;\nQUIT;    PDIFF=ALL  requests p-values for the differences between  ALL  the means and a  diffogram  is produced automatically displaying all pairwise least square means differences and indicating which are significant  It can be undestood as a least squares mean by least squares mean plot  The point estimates for differences between the means for each pairwise comparison can be found at the intersections of the gray grid lines (intersection of appropriate indexes)  The red/blue diagonal lines show the  confidence intervals for the true differences of the means  for each pairwise comparison  The grey 45$^{\\circ}$ reference line represents equality of the means (if the confidence interval crosses over it, then there is no significant difference between the two groups and the diagonal line for the pair will be  dashed and red ; if the difference is significant the line will be  solid and blue )     The  ADJUST=  option specifies the adjustment method for multiple comparisons  If you don't specify an option SAS uses the  Tukey method by default , if you specify  ADJUST=Dunnett  the GLM procedure produces multiple comparisons using  Dunnett's method  and a  control plot    The control plot displays the least squares mean and confidence limits of each group compared to the reference group   The middle  horizontal line represents its least square mean value  (you can see the arithmetic mean value un the  upper right corner  of the graph)  The  shaded area  goes from the  lower decision limit (LDL)  to the  upper decision limit (UDL)  There is a vertical line for each group that you're comparing to the reference (control) group. If a  vertical line extends past the shaded area , then the group represented by the line is  significantly different  (small p-value) than the reference group        PDIFF=CONTROLU('value')  specifies the control group for the Dunnett's case: the direction of the sign in Ha is the same as the direction you are testing, so this is a  one-sided upper-tailed t-test  If you specify  ADJUST=T  SAS will make no adjustments for multiple comparisons: is not recommended as there's a tendency to find  more significant pairwise differences than might actually exist", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#two-way-anova-with-interactions", 
            "text": "When you have a continuous response variable and  two categorical predictor variables , you use the  two-way ANOVA model   Effect : the magnitude of the expected change in the response variable presumably caused by the change in value of a predictor variable in the model  In addition, the variables in a model can be referred to as effects or terms  Main effect : is the effect of a single predictor variable  Interaction effects : when the relationship of the response variable with a predictor changes with the changing of another predictor variable (the effect of one variable depends on the value of the other variable)    When you consider an ANOVA with more than one predictor variable, it's called  n-way ANOVA  where  n  represents the number of predictor variables   The analysis in a  randomized block design  is actually a  special type of two-way ANOVA  in which you have one factor of interest and one blocking factor  When you analyze a two-way ANOVA with interactions, you first look at any tests for  interactions among the factors  If there is  no interaction between the factors  you can interpret the tests for the individual factor effects to determine their significance/non-significance  If an  interaction exists between any factors , the tests for the individual factor effects might be misleading due to masking of these effect by the interaction (this is specially true for unbalanced data with different number of observations for each combination of groups)    When the interaction is not statistically significant  you can analyze the main effect with the model in its current form  (generally the method you use when you analyze designed experiments)  Even when you analyze designed experiments, some statisticians might suggest that if the interaction is not significant,  you can delete the interaction effect from your model, rerun the model and then just analyze the main effects  increasing the power of the main effects test  If the  interaction term is significant , it is good practice to keep the main effect terms that make up the interaction in the model, whether they are significant or not (this preserves model hierarchy)  You have to make the  same three assumptions used in the ANOVA test  The interaction terms are also called  product terms  or  crossed effects", 
            "title": "Two-Way ANOVA with Interactions"
        }, 
        {
            "location": "/statistics/anova/#proc-glm_3", 
            "text": "1\n2\n3\n4\n5\n6\n7 PROC GLM DATA=SAS-data-set;\n    CLASS independent1 independent2;\n    MODEL dependent = independent1 independent2 independent1*independent2;\n    or\n    MODEL dependent = independent1 | independent2;\nRUN;\nQUIT;   This program is  fitting to this model :   Y_{ijk}=\\mu + \\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+\\epsilon_{ijk}    dependent = overall mean + intependent1 + independent2 + interaction12 + unaccounted for variation    In  most situations  you will want to use the  Type III SS  The  Type I SS (sequential)  are the sums of squares you obtain from fitting the effects in the order you specify in the model   The  Type III SS (marginal)  are the sums of squares you obtain from fitting each effect after all the other terms in the model, that is the sums of squares for each effect corrected for the other terms in the model  When examining these results you first have to  look at the interaction term and if it's significant  (p-value), the  main effects don't tell you the whole story . It that is the case, you don't need to worry all that much about the significance of the main effects at this point for two reasons:  You know that the effect of each variable1 level changes for the different variable2 levels  You want to include the main effects in the model, whether they are significant or not, to preserve model hierarchy    You can analyze the interaction between terms by looking at the  interaction plot  that SAS produces by default when you include an interaction term in the model  To analyze and interpret the effect of one of the interacting variables you need to add the  LSMEANS  statement to your program   1\n2\n3\n4\n5\n6 PROC GLM DATA=SAS-data-set ORDER=INTERNAL PLOTS(ONLY)=INTPLOT;\n    CLASS independent1 independent2;\n    MODEL dependent = intependent1 independent2 independent1*independent2;\n    LSMEANS independent1*independent2 / SLICE= independent1;\nRUN;\nQUIT;   SAS creates two types of mean plots when you use the  LSMEANS  statement with an interaction term:   The first plot displays the  least squares mean (LS-Mean) for every effect level    The second plot contains the same information rearranged so you can  look a little closer at the combination levels", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#store-statement", 
            "text": "You can add a  STORE  statement to save your analysis results in an  item store  (a binary file format that cannot be modified). This allows you to  run post-processing analysis  on the stored results even if you no longer have access to the original data set. The  STORE  statement applies to the following SAS/STAT procedures:  GENMOD ,  GLIMMIX ,  GLM ,  GLMSELECT ,  LOGISTIC ,  MIXED ,  ORTHOREG ,  PHREG ,  PROBIT ,  SURVEYLOGISTIC ,  SURVEYPHREG , and  SURVEYREG .  1\n2 STORE  OUT = item-store-name\n     / LABEL= label ;    item-store-name  is a usual one- or two-level SAS name, similar to the names that are used for SAS data sets  label  identifies the estimate on the output (is optional)", 
            "title": "STORE statement"
        }, 
        {
            "location": "/statistics/anova/#proc-plm", 
            "text": "To perform post-fitting statistical analysis and plotting for the contents of the store item, you use  PROC PLM . The statements and options that are available vary depending upon which procedure you used to produce the item store.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 PROC PLM RESTORE=item-store-specification  options ;\n    EFFECTPLOT INTERACTION(SLICEBY=variable)  plot-type   (plot-definition   options)  / CLM  / options ;\n    LSMEANS  model-effects   / options ;\n    LSMESTIMATE model-effect  label  values\n         divisor =n ,... label  values\n         divisor =n   / options ;\n    SHOW options;\n    SLICE model-effect / SLICEBY=variable ADJUST=tukey  / options ;\n    WHERE expression;\nRUN;    RESTORE  specifies the source item store for processing  EFFECTPLOT  produces a display of the fitted model and provides options for changing and enhancing the displays  LSMEANS  computes and compares least squares means (LS-means) of fixed effects  LSMESTIMATE  provides custom hypothesis tests among least squares means  SHOW  uses  ODS  to display contents of the item store. This statement is useful for verifying that the contents of the item store apply to the analysis and for generating  ODS  tables.  SLICE  provides a general mechanism for performing a partitioned analysis of the LS-means for an interaction (analysis of simple effects) and it uses the same options as the  LSMEANS  statement  WHERE  is used in the PLM procedure when the item store contains  BY-variable  information and you want to apply the  PROC PLM  statements to only a subset of the BY groups", 
            "title": "PROC PLM"
        }, 
        {
            "location": "/statistics/regression/", 
            "text": "Chapter summary in SAS\n\n\n\n\nExploratory Data Analysis\n\n\nA useful set of techniques for investigating your data is known as \nexploratory data analysis\n.\n\n\nPROC SGCATTER\n: Scatter Plots\n\n\n1\n2\n3\nPROC SGSCATTER DATA=SAS-data-base;\n    PLOT variableY*(variableX1 variableX2) / REG;\nRUN;\n\n\n\n\n\n\n\n\nIf you have \nso many observations\n that the scatter plot of the whole data set is difficult to interpret, you might run \nPROC SGSCATTER\n on a \nrandom sample of observations\n\n\n\n\nPROC CORR\n: Correlation Analysis\n\n\nThere are two ways of calculating correlations: the \nPearson correlation coefficient\n (specially used for normal distributed data) and the \nSpearman's rank correlation coefficient\n (which is best fitted when your data presents outliers).\n\n\nThe Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). Spearman's coefficient is appropriate for both continuous and discrete ordinal variables.\n\n\nThe closer the \nPearson\n correlation coefficient is to +1/-1, the stronger the positive/negative linear relationship is between the two variables. The closer the correlation coefficient is to 0, the weaker the linear relationship and if it is 0 variables are uncorrelated.\n\n\n\n\nWhen you interpret the correlation, be cautious about the effect of \nlarge sample sizes\n: even a correlation of 0.01 can be statistically significant with a large enough sample size and you would almost always reject the hypothesis $H_0$: $\\rho =0$, even if the value of your correlation is small for all practical purposes\n\n\nSome \ncommon errors\n on interpreting correlations are concluding a \ncause-and-effect relationship\n between the variables misinterpreting the kind of relationship between the variables and failing to recognize the influence of outliers on the correlation\n\n\nThe variables might be related but not causally\n\n\nCorrelation coefficients can be large because both variables are affected by other variables\n\n\nVariables might be strongly correlated by chance\n\n\n\n\n\n\nJust because the correlation coefficient is close to 0 doesn't mean that no relationship exists between the two variables: they might have a \nnon-linear relationship\n\n\nAnother common error is failing to recognize the \ninfluence of outliers\n on the correlation\n\n\nIf you have an outlier you should report both correlation coefficients (with and without the outlier) to report how influential the unusual data point is in your analysis\n\n\n\n\n\n\n\n\nThe \nPROC CORR\n also produces \nscatter plots\n or a \nscatter plot matrix\n.\n\n\n1\n2\n3\n4\n5\nPROC CORR DATA=SAS-data-set RANK|NOSIMPLE PLOTS(ONLY)=MATRIX(NVAR=ALL HISTOGRAM)|SCATTER(NVAR=ALL ELLIPSE=NONE) \noptions\n;\n    VAR variable(s)X;\n    WITH variable(s)Y;\n    ID variable4label;\nRUN;\n\n\n\n\n\n\nSimple Linear Regression\n\n\nYou use correlation analysis to determine the strength of the linear relationship between continuous response variables. Now you need to go a step further and \ndefine the linear relationship itself\n: $Y= \\beta_0+\\beta_1 \\cdot X+\\epsilon$\n\n\n\n\n$Y$ is the response variable \n\n\n$X$ is the predictor variable\n\n\n$\\beta_0$ is the intercept parameter\n\n\n$\\beta_1$ is the slope parameter\n\n\n$\\epsilon$ is the error term\n\n\n\n\nThe method of \nleast squares\n produces parameter estimates $\\hat \\beta_0$ and $\\hat \\beta_1$ with certain \noptimum properties\n which make them the Best Linear Unbiased Estimators (\nBLUE\n):\n\n\n\n\nThey are \nunbiased estimates\n of the population parameters\n\n\nThey have \nminimum variance\n\n\n\n\nTo find out how much better is the model that takes the predictor variable into account than a model that ignores the predictor variable, you can compare the \nsimple linear regression model\n to a \nbaseline model\n ($Y= \\bar Y$ independent of $X$). For your comparison, you calculate the \nexplained\n, \nunexplained\n and \ntotal variability\n in the simple linear regression model.\n\n\n\n\nThe \nexplained variability (SSM)\n is the difference between the regression line and the mean of the response variable: $\\sum(\\hat Y_i-\\bar Y)^2$\n\n\nThe \nunexplained variability (SSE)\n is the difference between the observed values and the regression line: $\\sum(Y_i-\\hat Y_i)^2$\n\n\nThe \ntotal variability\n is the difference between the observed values and the mean of the response variable: $\\sum(Y_i-\\bar Y)^2$\n\n\n\n\nIf we consider \nhypothesis testing\n for linear regression:\n\n\n\n\n$H_0$: the regression model does not fit the data better than the baseline model (slope $= 0$)\n\n\n$H_a$: the regression model does fit the data better than the baseline model (slope $= \\hat\\beta_1 \\ne 0$)\n\n\n\n\nThese \nassumptions\n underlie the hypothesis test for the regression model and have to be met for a simple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):\n\n\n\n\nThe mean of the response variable is linearly related to the value of the predictor variable\n\n\nThe error terms are normally distributed with a mean of 0\n\n\nThe error terms have equal variances\n\n\nThe error terms are independent at each value of the predictor variable\n\n\n\n\nPROC REG\n\n\n1\n2\n3\n4\n5\nPROC REG DATA=SAS-data-set \noptions\n;\n    MODEL dependent=regressor / CLM CLI \n/options\n;\n    ID regressor;\nRUN;\nQUIT;\n\n\n\n\n\n\nTo asses the level of precision around the mean estimates you can produce \nconfidence intervals\n around the means. Confidence intervals become wider as you move away from the mean of the predictor variable. The wider the confidence interval the less precise it is. You might also want to construct \nprediction intervals\n for a single observation. A prediction interval is wider than a confidence interval because \nsingle observations have more variability than sample means\n.\n\n\nFor producing \npredicted values\n with \nPROC REG\n:\n\n\n\n\nCreate a data set containing the values of the independent variables for which you want to make predictions\n\n\nConcatenate the new data set with the original data set\n\n\nFit a simple linear regression model to the new data set and specify the \nP\n option in the \nMODEL\n statement\n\n\n\n\nBecause the concatenated observations contain \nmissing values\n for the response variable, \nPROC REG\n does not include these observations when fitting the regression model. However, \nPROC REG\n does \nproduce predicted values\n for these observations.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nDATA SAS-predictions-data-set;\n    INPUT dependent @@;\n    DATALINES;\n[new values separated with blanks]\n;\nRUN;\n\nDATA SAS-new-data-set;\n    SET SAS-predictions-data-set SAS-original-data-set;\nRUN;\n\nPROC REG DATA=SAS-new-data-set;\n    MODEL dependent=regressor / P;\n    ID regressor;\nRUN;\nQUIT;\n\n\n\n\n\n\nWhen you use a model to predict future values of the response variable given certain values of the predictor variable, you must \nstay within (or near) the range of values for the predictor variable used to create the model\n. The relationship between the predictor variable and the response variable might be different beyond the range of the data.\n\n\nIf you have a large data set and have already fitted the regression model, you can predict values more efficiently by using \nPROC REG\n and \nPROC SCORE\n:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nPROC REG DATA=SAS-original-data-set NOPRINT OUTEST=SAS-estimates-data-set;\n    MODEL dependent=regressor \n/options\n;\n    ID regressor;\nRUN;\nQUIT;\n\nPROC SCORE DATA=SAS-predictions-data-set\n        SCORE=SAS-estimates-data-set\n        OUT=SAS-scored-data-set\n        TYPE=PARMS\n        \noptions\n;\n    VAR variable(s);\nRUN;\nQUIT;\n\n\n\n\n\n\nMultiple Regression\n\n\nIn \nmultiple regression\n you can model the relationship between the response variable and \nmore than one predictor variable\n. It is a powerful tool for both \nanalytical or explanatory analysis and for prediction\n.\n\n\n$Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k+\\epsilon$ ($k+1$ parameters)\n\n\nAdvantages\n\n\n\n\nMultiple linear regression is a more powerful tool\n\n\nYou can determine whether a relationship exists between the response variable and more than one predictor variable at the same time\n\n\n\n\nDisadvantages\n\n\n\n\nYou need to perform a selection process to decide which model to use\n\n\nThe more predictors you have, the more complicated interpreting the model becomes\n\n\n\n\nIf we consider \nhypothesis testing\n for linear regression:\n\n\n\n\n$H_0$: the regression model does not fit the data better than the baseline model $(\\beta_1=\\beta_2=...=\\beta_k= 0)$\n\n\n$H_a$: the regression model does fit the data better than the baseline model (at least one $\\beta_i \\ne 0$)\n\n\n\n\nThese \nassumptions\n have to be met for a multiple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):\n\n\n\n\nA linear function of the $X$s accurately models the mean of the $Y$s\n\n\nThe error terms are normally distributed with a mean of 0\n\n\nThe error terms have constant variances\n\n\nThe error terms are independent at each value of the predictor variable\n\n\n\n\nThe \nregular $R^2$\n values never decrease when you add more terms to the model, but the \nadjusted $R^2$\n value takes into account the number of terms in the model by including a penalty for the complexity of the model. The \nadjusted $R^2$\n value increases only if new terms that you add significantly improve the model enough to warrant increasing the complexity of the model. It enables proper comparison between models with different parameter counts. When an \nadjusted $R^2$ increases by removing a variable\n from the models, it strongly implies that the removed \nvariable was not necessary\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nPROC REG DATA=SAS-data-set \noptions\n;\n    MODEL dependent=regressor1 regressor2 \n/options\n;\nRUN;\nQUIT;\n\nPROC GLM DATA=SAS-data-set\n    PLOTS(ONLY)=(CONTOURFIT);\n    MODEL dependent=regressor1 regressor2;\n    STORE OUT=SAS-multiple-data-set;\nRUN;\nQUIT;\n\nPROC PLM RESTORE=SAS-multiple-data-set PLOTS=ALL;\n    EFFECTPLOT CONTOUR (Y=regressor1 X=regressor2);\n    EFFECTPLOT SLICEFIT (X=regressor2 SLICEBY=regressor1=250 to 1000 by 250);\nRUN;\n\n\n\n\n\n\n\n\nIn \nPROC GLM\n, when you run a linear regression model with only two predictor variables, the output includes a contour fit plot by default. We specify \nCONTOURFIT\n to tell SAS to overlay the contour plot with a scatter plot of the observed data\n\n\n\n\n\n\nThe plot shows \npredicted values\n of the response variable as \ngradations of the background color\n from blue, representing low values, to red, representing high values. The \ndots\n, which are similarly coloured, represent the \nactual data\n. Observations that are perfectly fit would show the same color within the circle as outside the circle. The \nlines on the graph\n help you read the actual predictions at even intervals.\n\n\n\n\nThe \nCONTOUR\n option displays a contour plot of predicted values against two continuous covariates\n\n\nThe \nSLICEFIT\n option displys a curve of predicted values vs a continuous variable grouped by the levels of another effect\n\n\n\n\nClearly the \nPROC GLM\n contour fit plot is \nmore useful\n. However, if you do not have access to the original data set and can run \nPROC PLM\n only on the item store, this plot still gives you an idea of the relationship between the predictor variables and predicted values.\n\n\nModel Building and Interpretation\n\n\nThe brute force approach to find a good model is to start including all the predictor variables available and rerun the model \nremoving the least significant remaining term\n each time \nuntil\n you're left with a model where \nonly significant terms remain\n. With a small number of predictor variables a manual approach isn't too difficult but with a large number of predictor variables it's very tedious. Fortunately, if you specify the model selection technique to use, SAS finds good candidate models in an automatic way.\n\n\n\n\nAll-possible regression methods\n\n\nSAS computes all possible models and ranks the results. Then, to evaluate the models, you compare statistics side by side ($R^2$, adjusted $R^2$ and $C_p$ statistic).\n\n\n\n\n\n\nMallows' $C_p$\n statistic helps you detect model bias if you are underfitting/overfitting the model, it is a simple indicator of effective variable selection within a model\n\n\n\n\n\n\nTo select the best model for prediction (most accurate model for predicting future values of $Y$), you should use the \nMallows' criterion\n:  $C_p \\le p$, which is the \nnumber of parameters\n in the model including the intercept\n\n\n\n\nTo select the best model for parameter estimation (analytical or explanatory analysis), you should use \nHocking's criterion\n: $C_p\\le2p-p_{full}+1$\n\n\n\n\n1\n2\n3\n4\nPROC REG DATA=SASdata-set PLOTS(ONLY)=(CP) \noptions\n;\n    \nlabel:\n MODEL dependent=regressors  / SELECTION=CP RSQUARE ADJRSQ BEST=n \n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nBEST\n prints an specific number of the best candidate models according to a few different statistical criteria\n\n\nSELECTION\n option is used to specify the method used to select the model (\nCP\n, \nRSQUARE\n and \nADJRSQ\n to calculate with the all-possible regression model; the first statistic determines the sorting order)\n\n\nFor this all-possible regression model, we add the label \nALL_REG:\n\n\nWith \nPLOTS=(CP)\n we produce a plot:\n\n\n\n\n\n\nEach \nstar\n represents the \nbest model\n for a given number of parameters. The solid \nblue line\n represents \nMallows' criterion\n for $C_p$, so using this line helps us find a good candidate model for prediction. Because we want the \nsmallest model possible\n, we start at the left side of the graph, with the fewest number of parameters moving to the right until we find the \nfirst model that falls below the solid blue line\n. To find models for parameter estimation we have to look for models that falls below the \nred solid line\n which represent the \nHocking's criterion\n for $C_p$ parameter estimation. If we hover over the star, we can see which variables are included in this model.\n\n\n\n\nStepwise selection methods\n\n\nHere you choose a selection method (\nstepwise\n, \nforward\n or \nbackward\n approaches) and SAS constructs a model based on that method. When you have a \nlarge number of potential predictor variables\n, the stepwise regression methods might be a better option. You can use either the \nREG\n procedure or the \nGLMSELECT\n procedure to perform stepwise selection methods\n\n\n\n\nForward selection\n starts with no predictor variables in the model\n\n\nIt selects the best one-variable model\n\n\nIt selects the best two-variable model that includes the variable from the first model (after a variable is added to the model, it stays in even if it becomes insignificant later)\n\n\nIt keeps adding variables, one at a time, until no significant terms are left to add\n\n\n\n\n\n\nBackward selection/elimination\n starts with all predictor variables in the model\n\n\nIt removes variables one at a time, starting with the most non-significant variable (after a variable is removed from the model, it cannot reenter)\n\n\nIt stops when only significant terms are left in the model\n\n\n\n\n\n\nStepwise selection\n combines aspects of both forward and backward selection\n\n\nIt starts with no predictor variables in the model and starts adding variables, one at a time, as in forward selection\n\n\nHowever, as in backward selection, stepwise selection can drop non-significant variables, one at a time\n\n\nIt stops when everything in the model is currently significant and everything not in the model is not significant\n\n\n\n\n\n\n\n\nStatisticians in general agree on first using \nstepwise methods\n to identify several good candidates models and then applying your \nsubject matter expertise\n to choose the best model. Because the techniques for selecting or eliminating variables differ between the three selection methods, \nthey don't always produce the same final model\n. There is no one method that is best and \nyou need to be cautious\n when reporting statistical quantities produced by these methods:\n\n\n\n\nUsing automated model selections results in \nbiases in parameter estimates\n, \npredictions\n and \nstandard errors\n\n\nIncorrect\n calculation of \ndegrees of freedom\n\n\np-values\n that tend to err on the side of \noverestimating significance\n\n\n\n\nHow can you \navoid these issues\n?\n\n\n\n\nYou can hold out some of your data in order to perform an honest assessment of how well your model performs on a different sample of data (\nholdout/validation data\n) than you use to develop the model (\ntraining data\n)\n\n\nOther honest assessment approaches include \ncross-validation\n (if your data set is not large enough to split) or \nbootstraping\n (a resampling method that tries to approximate the distribution of the parameter estimates to estimate the standard error and p-values)\n\n\n\n\nPROC GLMSELECT\n\n\n1\n2\n3\n4\n5\nPROC GLMSELECT DATA=SAS-data-set \noptions\n;\n    CLASS variables;\n    \nlabel:\n MODEL dependent(s) = regressor(s) / \n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nThe \nSELECTION\n option specifies the method to be used to select the model (\nFORWARD\n | \nBACKWARD\n | \nSTEPWISE\n = default value)\n\n\nThe \nSELECT\n option specifies the criterion to be used to determine which variable to add/remove from the model (\nSL\n = significance level as the selection criterion)\n\n\nThe \nSLENTRY\n option determines the significance level for a variable to enter the model (default = 0.5 for forward and 0.15 for stepwise)\n\n\nThe \nSLSTAY\n option determines the significance level for a variable to stay in the model (default = 0.1 for backward and 0.15 for stepwise)\n\n\nYou can display p-values in the \nParameter Estimates\n table by including the \nSHOWPVALUES\n option int he MODEL statement\n\n\nThe \nDETAILS\n option specifies the level of detail produced (\nALL\n | \nSTEPS\n | \nSUMMARY\n)\n\n\n\n\n\n\nRecommendations to decide which model is best for your needs:\n\n\n\n\nRun all model selection methods\n\n\nLook for commonalities across the results \n\n\nNarrow down your choice of models by using your subject matter knowledge\n\n\n\n\nInformation Criterion and Other Selection Options\n\n\nThere are other selection criteria that you can use to select variables for a model as well as evaluate competing models. These statistics are collectively referred to as \ninformation criteria\n. Each information criterion searched for a model that minimizes the \nunexplained variability\n with as \nfew effects in the model as possible\n. The model with the \nsmaller information criterion is considered to be better\n. For types are available in \nPROC GLMSELECT\n:\n\n\n\n\nAkaike's information criterion (\nSELECT=AIC\n)\n\n\nCorrecterd Akaike's information criterion (\nSELECT=AICC\n)\n\n\nSawa Bayesian information criterion (\nSELECT=BIC\n)\n\n\nSchwarz Bayesian information criterion (\nSELECT=SBC\n, it could be called \nBIC\n in some other SAS procedures)\n\n\n\n\nThe calculations of all information criteria begin the same way:\n\n\n\n\nFirst you calculate $n\\cdot log(SSE/n)$ \n\n\nThen, each criterion adds a penalty that represents the complexity of the model (each type of information criterion invokes a different penalty component)\n\n\nAIC\n: $2p+n+2$\n\n\nAICC\n: $n(n+p)/(n-p-2)$\n\n\nBIC\n: $2(p+2)1-2q^2$\n\n\nSBC\n: $p\\cdot log(n)$", 
            "title": "Regression"
        }, 
        {
            "location": "/statistics/regression/#exploratory-data-analysis", 
            "text": "A useful set of techniques for investigating your data is known as  exploratory data analysis .", 
            "title": "Exploratory Data Analysis"
        }, 
        {
            "location": "/statistics/regression/#proc-sgcatter-scatter-plots", 
            "text": "1\n2\n3 PROC SGSCATTER DATA=SAS-data-base;\n    PLOT variableY*(variableX1 variableX2) / REG;\nRUN;    If you have  so many observations  that the scatter plot of the whole data set is difficult to interpret, you might run  PROC SGSCATTER  on a  random sample of observations", 
            "title": "PROC SGCATTER: Scatter Plots"
        }, 
        {
            "location": "/statistics/regression/#proc-corr-correlation-analysis", 
            "text": "There are two ways of calculating correlations: the  Pearson correlation coefficient  (specially used for normal distributed data) and the  Spearman's rank correlation coefficient  (which is best fitted when your data presents outliers).  The Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). Spearman's coefficient is appropriate for both continuous and discrete ordinal variables.  The closer the  Pearson  correlation coefficient is to +1/-1, the stronger the positive/negative linear relationship is between the two variables. The closer the correlation coefficient is to 0, the weaker the linear relationship and if it is 0 variables are uncorrelated.   When you interpret the correlation, be cautious about the effect of  large sample sizes : even a correlation of 0.01 can be statistically significant with a large enough sample size and you would almost always reject the hypothesis $H_0$: $\\rho =0$, even if the value of your correlation is small for all practical purposes  Some  common errors  on interpreting correlations are concluding a  cause-and-effect relationship  between the variables misinterpreting the kind of relationship between the variables and failing to recognize the influence of outliers on the correlation  The variables might be related but not causally  Correlation coefficients can be large because both variables are affected by other variables  Variables might be strongly correlated by chance    Just because the correlation coefficient is close to 0 doesn't mean that no relationship exists between the two variables: they might have a  non-linear relationship  Another common error is failing to recognize the  influence of outliers  on the correlation  If you have an outlier you should report both correlation coefficients (with and without the outlier) to report how influential the unusual data point is in your analysis     The  PROC CORR  also produces  scatter plots  or a  scatter plot matrix .  1\n2\n3\n4\n5 PROC CORR DATA=SAS-data-set RANK|NOSIMPLE PLOTS(ONLY)=MATRIX(NVAR=ALL HISTOGRAM)|SCATTER(NVAR=ALL ELLIPSE=NONE)  options ;\n    VAR variable(s)X;\n    WITH variable(s)Y;\n    ID variable4label;\nRUN;", 
            "title": "PROC CORR: Correlation Analysis"
        }, 
        {
            "location": "/statistics/regression/#simple-linear-regression", 
            "text": "You use correlation analysis to determine the strength of the linear relationship between continuous response variables. Now you need to go a step further and  define the linear relationship itself : $Y= \\beta_0+\\beta_1 \\cdot X+\\epsilon$   $Y$ is the response variable   $X$ is the predictor variable  $\\beta_0$ is the intercept parameter  $\\beta_1$ is the slope parameter  $\\epsilon$ is the error term   The method of  least squares  produces parameter estimates $\\hat \\beta_0$ and $\\hat \\beta_1$ with certain  optimum properties  which make them the Best Linear Unbiased Estimators ( BLUE ):   They are  unbiased estimates  of the population parameters  They have  minimum variance   To find out how much better is the model that takes the predictor variable into account than a model that ignores the predictor variable, you can compare the  simple linear regression model  to a  baseline model  ($Y= \\bar Y$ independent of $X$). For your comparison, you calculate the  explained ,  unexplained  and  total variability  in the simple linear regression model.   The  explained variability (SSM)  is the difference between the regression line and the mean of the response variable: $\\sum(\\hat Y_i-\\bar Y)^2$  The  unexplained variability (SSE)  is the difference between the observed values and the regression line: $\\sum(Y_i-\\hat Y_i)^2$  The  total variability  is the difference between the observed values and the mean of the response variable: $\\sum(Y_i-\\bar Y)^2$   If we consider  hypothesis testing  for linear regression:   $H_0$: the regression model does not fit the data better than the baseline model (slope $= 0$)  $H_a$: the regression model does fit the data better than the baseline model (slope $= \\hat\\beta_1 \\ne 0$)   These  assumptions  underlie the hypothesis test for the regression model and have to be met for a simple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):   The mean of the response variable is linearly related to the value of the predictor variable  The error terms are normally distributed with a mean of 0  The error terms have equal variances  The error terms are independent at each value of the predictor variable", 
            "title": "Simple Linear Regression"
        }, 
        {
            "location": "/statistics/regression/#proc-reg", 
            "text": "1\n2\n3\n4\n5 PROC REG DATA=SAS-data-set  options ;\n    MODEL dependent=regressor / CLM CLI  /options ;\n    ID regressor;\nRUN;\nQUIT;   To asses the level of precision around the mean estimates you can produce  confidence intervals  around the means. Confidence intervals become wider as you move away from the mean of the predictor variable. The wider the confidence interval the less precise it is. You might also want to construct  prediction intervals  for a single observation. A prediction interval is wider than a confidence interval because  single observations have more variability than sample means .  For producing  predicted values  with  PROC REG :   Create a data set containing the values of the independent variables for which you want to make predictions  Concatenate the new data set with the original data set  Fit a simple linear regression model to the new data set and specify the  P  option in the  MODEL  statement   Because the concatenated observations contain  missing values  for the response variable,  PROC REG  does not include these observations when fitting the regression model. However,  PROC REG  does  produce predicted values  for these observations.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 DATA SAS-predictions-data-set;\n    INPUT dependent @@;\n    DATALINES;\n[new values separated with blanks]\n;\nRUN;\n\nDATA SAS-new-data-set;\n    SET SAS-predictions-data-set SAS-original-data-set;\nRUN;\n\nPROC REG DATA=SAS-new-data-set;\n    MODEL dependent=regressor / P;\n    ID regressor;\nRUN;\nQUIT;   When you use a model to predict future values of the response variable given certain values of the predictor variable, you must  stay within (or near) the range of values for the predictor variable used to create the model . The relationship between the predictor variable and the response variable might be different beyond the range of the data.  If you have a large data set and have already fitted the regression model, you can predict values more efficiently by using  PROC REG  and  PROC SCORE :   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 PROC REG DATA=SAS-original-data-set NOPRINT OUTEST=SAS-estimates-data-set;\n    MODEL dependent=regressor  /options ;\n    ID regressor;\nRUN;\nQUIT;\n\nPROC SCORE DATA=SAS-predictions-data-set\n        SCORE=SAS-estimates-data-set\n        OUT=SAS-scored-data-set\n        TYPE=PARMS\n         options ;\n    VAR variable(s);\nRUN;\nQUIT;", 
            "title": "PROC REG"
        }, 
        {
            "location": "/statistics/regression/#multiple-regression", 
            "text": "In  multiple regression  you can model the relationship between the response variable and  more than one predictor variable . It is a powerful tool for both  analytical or explanatory analysis and for prediction .  $Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k+\\epsilon$ ($k+1$ parameters)  Advantages   Multiple linear regression is a more powerful tool  You can determine whether a relationship exists between the response variable and more than one predictor variable at the same time   Disadvantages   You need to perform a selection process to decide which model to use  The more predictors you have, the more complicated interpreting the model becomes   If we consider  hypothesis testing  for linear regression:   $H_0$: the regression model does not fit the data better than the baseline model $(\\beta_1=\\beta_2=...=\\beta_k= 0)$  $H_a$: the regression model does fit the data better than the baseline model (at least one $\\beta_i \\ne 0$)   These  assumptions  have to be met for a multiple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):   A linear function of the $X$s accurately models the mean of the $Y$s  The error terms are normally distributed with a mean of 0  The error terms have constant variances  The error terms are independent at each value of the predictor variable   The  regular $R^2$  values never decrease when you add more terms to the model, but the  adjusted $R^2$  value takes into account the number of terms in the model by including a penalty for the complexity of the model. The  adjusted $R^2$  value increases only if new terms that you add significantly improve the model enough to warrant increasing the complexity of the model. It enables proper comparison between models with different parameter counts. When an  adjusted $R^2$ increases by removing a variable  from the models, it strongly implies that the removed  variable was not necessary .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 PROC REG DATA=SAS-data-set  options ;\n    MODEL dependent=regressor1 regressor2  /options ;\nRUN;\nQUIT;\n\nPROC GLM DATA=SAS-data-set\n    PLOTS(ONLY)=(CONTOURFIT);\n    MODEL dependent=regressor1 regressor2;\n    STORE OUT=SAS-multiple-data-set;\nRUN;\nQUIT;\n\nPROC PLM RESTORE=SAS-multiple-data-set PLOTS=ALL;\n    EFFECTPLOT CONTOUR (Y=regressor1 X=regressor2);\n    EFFECTPLOT SLICEFIT (X=regressor2 SLICEBY=regressor1=250 to 1000 by 250);\nRUN;    In  PROC GLM , when you run a linear regression model with only two predictor variables, the output includes a contour fit plot by default. We specify  CONTOURFIT  to tell SAS to overlay the contour plot with a scatter plot of the observed data    The plot shows  predicted values  of the response variable as  gradations of the background color  from blue, representing low values, to red, representing high values. The  dots , which are similarly coloured, represent the  actual data . Observations that are perfectly fit would show the same color within the circle as outside the circle. The  lines on the graph  help you read the actual predictions at even intervals.   The  CONTOUR  option displays a contour plot of predicted values against two continuous covariates  The  SLICEFIT  option displys a curve of predicted values vs a continuous variable grouped by the levels of another effect   Clearly the  PROC GLM  contour fit plot is  more useful . However, if you do not have access to the original data set and can run  PROC PLM  only on the item store, this plot still gives you an idea of the relationship between the predictor variables and predicted values.", 
            "title": "Multiple Regression"
        }, 
        {
            "location": "/statistics/regression/#model-building-and-interpretation", 
            "text": "The brute force approach to find a good model is to start including all the predictor variables available and rerun the model  removing the least significant remaining term  each time  until  you're left with a model where  only significant terms remain . With a small number of predictor variables a manual approach isn't too difficult but with a large number of predictor variables it's very tedious. Fortunately, if you specify the model selection technique to use, SAS finds good candidate models in an automatic way.   All-possible regression methods  SAS computes all possible models and ranks the results. Then, to evaluate the models, you compare statistics side by side ($R^2$, adjusted $R^2$ and $C_p$ statistic).    Mallows' $C_p$  statistic helps you detect model bias if you are underfitting/overfitting the model, it is a simple indicator of effective variable selection within a model    To select the best model for prediction (most accurate model for predicting future values of $Y$), you should use the  Mallows' criterion :  $C_p \\le p$, which is the  number of parameters  in the model including the intercept   To select the best model for parameter estimation (analytical or explanatory analysis), you should use  Hocking's criterion : $C_p\\le2p-p_{full}+1$   1\n2\n3\n4 PROC REG DATA=SASdata-set PLOTS(ONLY)=(CP)  options ;\n     label:  MODEL dependent=regressors  / SELECTION=CP RSQUARE ADJRSQ BEST=n  /options ;\nRUN;\nQUIT;    BEST  prints an specific number of the best candidate models according to a few different statistical criteria  SELECTION  option is used to specify the method used to select the model ( CP ,  RSQUARE  and  ADJRSQ  to calculate with the all-possible regression model; the first statistic determines the sorting order)  For this all-possible regression model, we add the label  ALL_REG:  With  PLOTS=(CP)  we produce a plot:    Each  star  represents the  best model  for a given number of parameters. The solid  blue line  represents  Mallows' criterion  for $C_p$, so using this line helps us find a good candidate model for prediction. Because we want the  smallest model possible , we start at the left side of the graph, with the fewest number of parameters moving to the right until we find the  first model that falls below the solid blue line . To find models for parameter estimation we have to look for models that falls below the  red solid line  which represent the  Hocking's criterion  for $C_p$ parameter estimation. If we hover over the star, we can see which variables are included in this model.   Stepwise selection methods  Here you choose a selection method ( stepwise ,  forward  or  backward  approaches) and SAS constructs a model based on that method. When you have a  large number of potential predictor variables , the stepwise regression methods might be a better option. You can use either the  REG  procedure or the  GLMSELECT  procedure to perform stepwise selection methods   Forward selection  starts with no predictor variables in the model  It selects the best one-variable model  It selects the best two-variable model that includes the variable from the first model (after a variable is added to the model, it stays in even if it becomes insignificant later)  It keeps adding variables, one at a time, until no significant terms are left to add    Backward selection/elimination  starts with all predictor variables in the model  It removes variables one at a time, starting with the most non-significant variable (after a variable is removed from the model, it cannot reenter)  It stops when only significant terms are left in the model    Stepwise selection  combines aspects of both forward and backward selection  It starts with no predictor variables in the model and starts adding variables, one at a time, as in forward selection  However, as in backward selection, stepwise selection can drop non-significant variables, one at a time  It stops when everything in the model is currently significant and everything not in the model is not significant     Statisticians in general agree on first using  stepwise methods  to identify several good candidates models and then applying your  subject matter expertise  to choose the best model. Because the techniques for selecting or eliminating variables differ between the three selection methods,  they don't always produce the same final model . There is no one method that is best and  you need to be cautious  when reporting statistical quantities produced by these methods:   Using automated model selections results in  biases in parameter estimates ,  predictions  and  standard errors  Incorrect  calculation of  degrees of freedom  p-values  that tend to err on the side of  overestimating significance   How can you  avoid these issues ?   You can hold out some of your data in order to perform an honest assessment of how well your model performs on a different sample of data ( holdout/validation data ) than you use to develop the model ( training data )  Other honest assessment approaches include  cross-validation  (if your data set is not large enough to split) or  bootstraping  (a resampling method that tries to approximate the distribution of the parameter estimates to estimate the standard error and p-values)", 
            "title": "Model Building and Interpretation"
        }, 
        {
            "location": "/statistics/regression/#proc-glmselect", 
            "text": "1\n2\n3\n4\n5 PROC GLMSELECT DATA=SAS-data-set  options ;\n    CLASS variables;\n     label:  MODEL dependent(s) = regressor(s) /  /options ;\nRUN;\nQUIT;    The  SELECTION  option specifies the method to be used to select the model ( FORWARD  |  BACKWARD  |  STEPWISE  = default value)  The  SELECT  option specifies the criterion to be used to determine which variable to add/remove from the model ( SL  = significance level as the selection criterion)  The  SLENTRY  option determines the significance level for a variable to enter the model (default = 0.5 for forward and 0.15 for stepwise)  The  SLSTAY  option determines the significance level for a variable to stay in the model (default = 0.1 for backward and 0.15 for stepwise)  You can display p-values in the  Parameter Estimates  table by including the  SHOWPVALUES  option int he MODEL statement  The  DETAILS  option specifies the level of detail produced ( ALL  |  STEPS  |  SUMMARY )    Recommendations to decide which model is best for your needs:   Run all model selection methods  Look for commonalities across the results   Narrow down your choice of models by using your subject matter knowledge", 
            "title": "PROC GLMSELECT"
        }, 
        {
            "location": "/statistics/regression/#information-criterion-and-other-selection-options", 
            "text": "There are other selection criteria that you can use to select variables for a model as well as evaluate competing models. These statistics are collectively referred to as  information criteria . Each information criterion searched for a model that minimizes the  unexplained variability  with as  few effects in the model as possible . The model with the  smaller information criterion is considered to be better . For types are available in  PROC GLMSELECT :   Akaike's information criterion ( SELECT=AIC )  Correcterd Akaike's information criterion ( SELECT=AICC )  Sawa Bayesian information criterion ( SELECT=BIC )  Schwarz Bayesian information criterion ( SELECT=SBC , it could be called  BIC  in some other SAS procedures)   The calculations of all information criteria begin the same way:   First you calculate $n\\cdot log(SSE/n)$   Then, each criterion adds a penalty that represents the complexity of the model (each type of information criterion invokes a different penalty component)  AIC : $2p+n+2$  AICC : $n(n+p)/(n-p-2)$  BIC : $2(p+2)1-2q^2$  SBC : $p\\cdot log(n)$", 
            "title": "Information Criterion and Other Selection Options"
        }, 
        {
            "location": "/statistics/inference/", 
            "text": "Chapter summary in SAS\n\n\nHow to \nverify the assumptions\n and \ndiagnose problems\n that you encounter in \nlinear regression\n?\n\n\nExamining Residuals\n\n\nYou can use the \nresidual values\n (difference between each observed value of $Y$ and its predicted value) from the regression analysis to verify the \nassumptions of the linear regression\n. Residuals are estimates of the errors, so you can \nplot the residuals to check the assumptions of the errors\n.\n\n\n\n\nYou can plot residuals vs the predicted values to check for \nviolations of equal variances\n\n\nYou can also use this plot to check for \nviolations of linearity and independence\n\n\nYou can plot the residuals vs the values of the independent variables to \nfurther examine any violations of equal variances\n (you can see which predictor contributes to the violation of the assumption)\n\n\nYou can use a histogram or a normal probability plot of the residuals to determine whether or not the \nerrors are normally distributed\n\n\n\n\nYou want to see a \nrandom scatter of the residual values\n above and below the reference line at 0. If you see \npatterns or trends\n in the residual values, the assumptions might not be valid and the models might have problems.\n\n\n\n\n\n\nNote\n\n\nTo take autocorrelation (correlated over time) into account, you might need to use a regression procedure such as \nPROC AUTOREG\n\n\n\n\nYou can also use these plots to \ndetect outliers\n, which often reflect data errors or unusual circumstances. They can affect your regression results, so you want to know whether any outliers are present and causing problems and investigate if they result from \ndata entry error or some other problem\n that you can correct.\n\n\nPROC REG\n\n\n1\n2\n3\n4\n5\nPROC REG DATA=SAS-data-set PLOTS(ONLY)=(QQ RESIDUALBYPREDICTED RESIDUALS)\noptions\n;\n    \nlabel:\n MODEL dependent=regressor(s) \n/options\n;\n    ID variable4identification;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nQQ\n requests a residual quantile-quantile plot to assess the normality of the residual error\n\n\nRESIDUALBYPREDICTED\n requests a plot of residuals by predicted values to verify the equal variance assumption, the independence assumption and model adequacy\n\n\nRESIDUALS\n requests a panel of plots of residuals by the predictor variables in the model: if any of the \nResidual by Regressors\n plots show signs of unequal variance, we can determine which predictor variable is involved in the problem\n\n\n\n\nIdentifying Influential Observations\n\n\nAn influential observation is different from an outlier. An \noutlier\n is an unusual observation that has a large residual compare to the rest of the points. An \ninfluential observation\n can sometimes have a large residual compared to the rest of the points, but it is an observation so far away from the rest of the data that it singlehandedly exerts influence on the slope of the regression line.\n\n\n\n\nUsing \nSTUDENT\n residuals to detect outliers\n\n\nAlso known as \nstudientized or standardized residuals\n, the \nSTUDENT\n residuals are calculated by dividing the \nresidual by their standard errors\n, so you can think of them as roughly equivalent to a z-score. \n\n\n\n\nFor \nrelatively small sample sizes\n, if the absolute value of the \nSTUDENT\n \nresidual is $\n2$\n, you can suspect that the corresponding observation is an outlier\n\n\nFor \nlarge sample sizes\n, it's very likely that even more \nSTUDENT\n \nresiduals greater than $\\pm2$\n will occur just by chance, so you should typically use a larger cutoff value of $\n3$\n\n\n\n\nUsing \nCOOKSD\n statistics to detect influential observations\n\n\nFor each observation, the Cook's D statistic is \ncalculated as if that observation weren't in the data set\n as well as the set of parameter estimates with all the observations in your regression analysis. \n\n\n\n\nIf any observation has a Cook's D \nstatistic $\n4/n$\n that observation is influential\n\n\nThe Cook's D statistic is most useful for identifying influential observations when the purpose of your model is \nparameter estimation\n\n\n\n\nUsing \nRSTUDENT\n residuals to detect influential observations\n\n\nRSTUDENT\n residuals are similar to \nSTUDENT\n residuals. For each observation, the \nRSTUDENT\n residual is the \nresidual divided by the standard error estimated with the current observation deleted\n.\n\n\n\n\nIf the RSTUDENT residual is different from the \nSTUDENT\n residual, the observation is probably influential\n\n\nIf the absolute value of the \nRSTUDENT\n residuals is $\n2$ or $\n3$, you've probably detected an influential observation\n\n\n\n\nUsing \nDFFITS\n statistics to detect influential observations\n\n\nDFFITS\n measures the impact that each observation has on its own predicted value. For each observation, \nDFFITS\n is \ncalculated using two predicted values\n:\n\n\n\n\nThe first predicted value is calculated from a model using the entire data set to estimate model parameters\n\n\nThe second predicted value is calculated from a model using the data set with that particular observation removed to estimate model parameters\n\n\nThe difference between the two predicted values is divided by the standard error of the predicted value, without the observation\n\n\n\n\nIf the \nstandardized difference\n between these predicted values \nis large\n, that particular observation has a \nlarge effect on the model fit\n.\n\n\n\n\nThe \ngeneral cutoff\n value is $2$\n\n\nThe more \nprecise cutoff\n is $2 \\cdot sqrt(p/n)$\n\n\nIf the absolute value of DFFITS for any observation is $\n$ cutoff value, you've detected an influential observation\n\n\nDFFITS\n is most useful for \npredictive models\n\n\n\n\nUsing \nDFBETAS\n statistics to explore the influenced predictor variable\n\n\nTo help identifying which parameter the observation might be influencing most you can use \nDFBETAS\n (difference in betas). It measure the change in each parameter estimate. \n\n\n\n\nOne \nDFBETAS\n is calculated per predictor variable per observation\n\n\nEach value is calculated by taking the estimated coefficient for that particular predictor variable \nusing all the data\n, subtracting the estimated coefficient for that particular predictor variable with the \ncurrent observation removed\n and dividing by its standard error\n\n\nLarge \nDFBETAS\n indicate observations that are influential in estimating a given parameter:\n\n\nThe \ngeneral cutoff\n value is $2$\n\n\nThe more \nprecise cutoff\n is $2 \\cdot sqrt(1/n)$\n\n\n\n\n\n\n\n\nPROC GLMSELECT\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\nPROC GLMSELECT DATA=SAS-data-set \noptions\n;\n    \nlabel:\n MODEL dependent(s) = regressor(s) / \n/options\n;\nRUN;\nQUIT;\n\nODS OUTPUT RSTUDENTBYPREDICTED=name-rstud-data-set\n           COOKSDPLOT=name-cooksd-data-set\n           DFFITSPLOT=name-dffits-data-set\n           DFBETASPANEL=name-dfbs-data-set;\n\nPROC REG DATA=SAS-data-set PLOTS(ONLY LABEL)=\n                                (RSTUDENTBYPREDICTED \n                                 COOKSD \n                                 DFFITS \n                                 DFBETAS) \noptions\n;\n    \nlabel:\n MODEL dependent=\n_GLSIND \n/options\n;\n    ID variable4identification;\nRUN;\nQUIT;\n\nDATA influential;\n    MERGE name-rstud-data-set\n          name-cooksd-data-set\n          name-dffits-data-set\n          name-dfbs-data-set;\n    BY observation;\n\n    IF (ABS(RSTUDENT)\n3) OR (COOKSDLABEL NE \n \n) OR DFFITSOUT THEN FLAG=1;\n    ARRAY DFBETAS{*} _DFBETASOUT: ;\n    DO I=2 TO DIM(DFBETAS);\n        IF DFBETAS{I} THEN FLAG=1;\n    END;\n\n    IF ABS(RSTUDENT)\n=3 THEN RSTUDENT=.;\n    IF COOKSDLABEL EQ \n \n THEN COOKSD=.;\n\n    IF FLAG=1;\n    DROP I FLAG;\nRUN;\n\nPROC PRINT DATA=influential;\n    ID observation;\n    VAR RSTUDENT COOKSD DFFITSOUT _DFBETASOUT: ;\nRUN;\n\n\n\n\n\n\n\n\nPROC GLMSELECT\n automatically creates the \n_GLSIND\n macro variable which stores the list of effects that are in the model whose variable order can be checked in the \nInfluence Diagnostics\n panel\n\n\nThe \nODS\n statement takes the data that creates each of the requested plots and saves it in the specified data set\n\n\nThe \nLABEL\n option includes a label for the extreme observations in the plot (labeled with the observation numbers if there is not ID specified)\n\n\n\n\nHaving \ninfluential observations doesn't violate regression assumptions\n, but it's a major nuisance that you need to address:\n\n\n\n\nRecheck\n for data entry errors\n\n\nIf the data appears to be valid, \nconsider whether you have an adequate model\n (a different model might fit the data better). Divide the number of influential observations you detect by the number of observations in you data set: if the result is \n$\n5\\%$ you probably have the wrong model\n.\n\n\nDetermine whether the influential observation is \nvalid but just unusual\n\n\nAs a general rule you should \nnot exclude data\n (some unusual observations contain important information)\n\n\nIf you choose to exclude some observations, include in your report a \ndescription of the types of observations that you excluded and why\n and discuss the limitation of the conclusions given the exclusions\n\n\n\n\nDetecting Collinearity\n\n\nCollinearity (or multicollinearity) is a problem that you face in multiple regression. It occurs when two or more \npredictor variables are highly correlated with each other\n (\nredundant information\n among them, the predictor variables explain much of the same variation in the response). Collinearity doesn't violate the assumptions of multiple regression.\n\n\n\n\nCollinearity can \nhide significant effects\n (if you include only one of the collinear variables in the model it is significant but when there are more than one included none of them are significant)\n\n\nCollinearity \nincreases the variance\n of the parameter estimates, making them \nunstable\n (the data points don't spread out enough in the space to provide stable support for the plane defined by the model) and, in turn, this \nincreases the prediction error\n of the model\n\n\n\n\nWhen an overall model is highly significant but the individual variables don't tell the same story, it's a \nwarning sign of collinearity\n. When the \nstandard error for an estimate is larger than the parameter estimate\n itself, it's not going to be statistically significant. The SE tells us how variable the corresponding parameter estimate is: when the standard errors are high, the \nmodel lacks stability\n.\n\n\n1\n2\n3\n4\nPROC REG DATA=SAS-data-set \noptions\n;\n    \nlabel:\n MODEL dependent = regressors / VIF \n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nThe \nVIF\n (variance inflation factor, $VIF_i=1/(1-R_i^2)$) option measures the magnitude of collinearity in a model (VIF$\n10$ for any predictor in the model, those predictors are probably involved in collinearity)\n\n\nOther options are \nCOLLIN\n (includes the intercept when analyzing collinearity and helps identify the predictors that are causing the problem) and \nCOLLINOINT\n (requests the same analysis as \nCOLLIN\n but excludes the intercept)\n\n\n\n\nEffective modeling cycle\n\n\n\n\nYou want to get to know your data by \nperforming preliminary analysis\n: \n\n\nPlot your data\n\n\nCalculate descriptive statistics \n\n\nPerform correlation analysis\n\n\n\n\n\n\nIdentify some \ngood candidate models\n using \nPROC REG\n: \n\n\nFirst check for collinearity \n\n\nUse all-possible regression or stepwise selection methods and subject matter knowledge to select model candidates\n\n\nIdentify the good ones with the Mallows' (prediction) or Hocking's (explanatory) criterion for $C_p$\n\n\n\n\n\n\n\n\nCheck and validate your assumtions\n by creating residual plots and conducting a few other statistical tests\n\n\n\n\n\n\nDeal with any \nproblems in your data\n: \n\n\n\n\nDetermine whether any influential observations might be throwing off your model calculations\n\n\nDetermine whether any variables are collinear\n\n\n\n\n\n\n\n\nRevise your model\n\n\n\n\n\n\nValidate your model\n with data not used to build the  model (prediction testing)", 
            "title": "Model Post-Fitting for Inference"
        }, 
        {
            "location": "/statistics/inference/#examining-residuals", 
            "text": "You can use the  residual values  (difference between each observed value of $Y$ and its predicted value) from the regression analysis to verify the  assumptions of the linear regression . Residuals are estimates of the errors, so you can  plot the residuals to check the assumptions of the errors .   You can plot residuals vs the predicted values to check for  violations of equal variances  You can also use this plot to check for  violations of linearity and independence  You can plot the residuals vs the values of the independent variables to  further examine any violations of equal variances  (you can see which predictor contributes to the violation of the assumption)  You can use a histogram or a normal probability plot of the residuals to determine whether or not the  errors are normally distributed   You want to see a  random scatter of the residual values  above and below the reference line at 0. If you see  patterns or trends  in the residual values, the assumptions might not be valid and the models might have problems.    Note  To take autocorrelation (correlated over time) into account, you might need to use a regression procedure such as  PROC AUTOREG   You can also use these plots to  detect outliers , which often reflect data errors or unusual circumstances. They can affect your regression results, so you want to know whether any outliers are present and causing problems and investigate if they result from  data entry error or some other problem  that you can correct.", 
            "title": "Examining Residuals"
        }, 
        {
            "location": "/statistics/inference/#proc-reg", 
            "text": "1\n2\n3\n4\n5 PROC REG DATA=SAS-data-set PLOTS(ONLY)=(QQ RESIDUALBYPREDICTED RESIDUALS) options ;\n     label:  MODEL dependent=regressor(s)  /options ;\n    ID variable4identification;\nRUN;\nQUIT;    QQ  requests a residual quantile-quantile plot to assess the normality of the residual error  RESIDUALBYPREDICTED  requests a plot of residuals by predicted values to verify the equal variance assumption, the independence assumption and model adequacy  RESIDUALS  requests a panel of plots of residuals by the predictor variables in the model: if any of the  Residual by Regressors  plots show signs of unequal variance, we can determine which predictor variable is involved in the problem", 
            "title": "PROC REG"
        }, 
        {
            "location": "/statistics/inference/#identifying-influential-observations", 
            "text": "An influential observation is different from an outlier. An  outlier  is an unusual observation that has a large residual compare to the rest of the points. An  influential observation  can sometimes have a large residual compared to the rest of the points, but it is an observation so far away from the rest of the data that it singlehandedly exerts influence on the slope of the regression line.", 
            "title": "Identifying Influential Observations"
        }, 
        {
            "location": "/statistics/inference/#using-student-residuals-to-detect-outliers", 
            "text": "Also known as  studientized or standardized residuals , the  STUDENT  residuals are calculated by dividing the  residual by their standard errors , so you can think of them as roughly equivalent to a z-score.    For  relatively small sample sizes , if the absolute value of the  STUDENT   residual is $ 2$ , you can suspect that the corresponding observation is an outlier  For  large sample sizes , it's very likely that even more  STUDENT   residuals greater than $\\pm2$  will occur just by chance, so you should typically use a larger cutoff value of $ 3$", 
            "title": "Using STUDENT residuals to detect outliers"
        }, 
        {
            "location": "/statistics/inference/#using-cooksd-statistics-to-detect-influential-observations", 
            "text": "For each observation, the Cook's D statistic is  calculated as if that observation weren't in the data set  as well as the set of parameter estimates with all the observations in your regression analysis.    If any observation has a Cook's D  statistic $ 4/n$  that observation is influential  The Cook's D statistic is most useful for identifying influential observations when the purpose of your model is  parameter estimation", 
            "title": "Using COOKSD statistics to detect influential observations"
        }, 
        {
            "location": "/statistics/inference/#using-rstudent-residuals-to-detect-influential-observations", 
            "text": "RSTUDENT  residuals are similar to  STUDENT  residuals. For each observation, the  RSTUDENT  residual is the  residual divided by the standard error estimated with the current observation deleted .   If the RSTUDENT residual is different from the  STUDENT  residual, the observation is probably influential  If the absolute value of the  RSTUDENT  residuals is $ 2$ or $ 3$, you've probably detected an influential observation", 
            "title": "Using RSTUDENT residuals to detect influential observations"
        }, 
        {
            "location": "/statistics/inference/#using-dffits-statistics-to-detect-influential-observations", 
            "text": "DFFITS  measures the impact that each observation has on its own predicted value. For each observation,  DFFITS  is  calculated using two predicted values :   The first predicted value is calculated from a model using the entire data set to estimate model parameters  The second predicted value is calculated from a model using the data set with that particular observation removed to estimate model parameters  The difference between the two predicted values is divided by the standard error of the predicted value, without the observation   If the  standardized difference  between these predicted values  is large , that particular observation has a  large effect on the model fit .   The  general cutoff  value is $2$  The more  precise cutoff  is $2 \\cdot sqrt(p/n)$  If the absolute value of DFFITS for any observation is $ $ cutoff value, you've detected an influential observation  DFFITS  is most useful for  predictive models", 
            "title": "Using DFFITS statistics to detect influential observations"
        }, 
        {
            "location": "/statistics/inference/#using-dfbetas-statistics-to-explore-the-influenced-predictor-variable", 
            "text": "To help identifying which parameter the observation might be influencing most you can use  DFBETAS  (difference in betas). It measure the change in each parameter estimate.    One  DFBETAS  is calculated per predictor variable per observation  Each value is calculated by taking the estimated coefficient for that particular predictor variable  using all the data , subtracting the estimated coefficient for that particular predictor variable with the  current observation removed  and dividing by its standard error  Large  DFBETAS  indicate observations that are influential in estimating a given parameter:  The  general cutoff  value is $2$  The more  precise cutoff  is $2 \\cdot sqrt(1/n)$", 
            "title": "Using DFBETAS statistics to explore the influenced predictor variable"
        }, 
        {
            "location": "/statistics/inference/#proc-glmselect", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44 PROC GLMSELECT DATA=SAS-data-set  options ;\n     label:  MODEL dependent(s) = regressor(s) /  /options ;\nRUN;\nQUIT;\n\nODS OUTPUT RSTUDENTBYPREDICTED=name-rstud-data-set\n           COOKSDPLOT=name-cooksd-data-set\n           DFFITSPLOT=name-dffits-data-set\n           DFBETASPANEL=name-dfbs-data-set;\n\nPROC REG DATA=SAS-data-set PLOTS(ONLY LABEL)=\n                                (RSTUDENTBYPREDICTED \n                                 COOKSD \n                                 DFFITS \n                                 DFBETAS)  options ;\n     label:  MODEL dependent= _GLSIND  /options ;\n    ID variable4identification;\nRUN;\nQUIT;\n\nDATA influential;\n    MERGE name-rstud-data-set\n          name-cooksd-data-set\n          name-dffits-data-set\n          name-dfbs-data-set;\n    BY observation;\n\n    IF (ABS(RSTUDENT) 3) OR (COOKSDLABEL NE    ) OR DFFITSOUT THEN FLAG=1;\n    ARRAY DFBETAS{*} _DFBETASOUT: ;\n    DO I=2 TO DIM(DFBETAS);\n        IF DFBETAS{I} THEN FLAG=1;\n    END;\n\n    IF ABS(RSTUDENT) =3 THEN RSTUDENT=.;\n    IF COOKSDLABEL EQ     THEN COOKSD=.;\n\n    IF FLAG=1;\n    DROP I FLAG;\nRUN;\n\nPROC PRINT DATA=influential;\n    ID observation;\n    VAR RSTUDENT COOKSD DFFITSOUT _DFBETASOUT: ;\nRUN;    PROC GLMSELECT  automatically creates the  _GLSIND  macro variable which stores the list of effects that are in the model whose variable order can be checked in the  Influence Diagnostics  panel  The  ODS  statement takes the data that creates each of the requested plots and saves it in the specified data set  The  LABEL  option includes a label for the extreme observations in the plot (labeled with the observation numbers if there is not ID specified)   Having  influential observations doesn't violate regression assumptions , but it's a major nuisance that you need to address:   Recheck  for data entry errors  If the data appears to be valid,  consider whether you have an adequate model  (a different model might fit the data better). Divide the number of influential observations you detect by the number of observations in you data set: if the result is  $ 5\\%$ you probably have the wrong model .  Determine whether the influential observation is  valid but just unusual  As a general rule you should  not exclude data  (some unusual observations contain important information)  If you choose to exclude some observations, include in your report a  description of the types of observations that you excluded and why  and discuss the limitation of the conclusions given the exclusions", 
            "title": "PROC GLMSELECT"
        }, 
        {
            "location": "/statistics/inference/#detecting-collinearity", 
            "text": "Collinearity (or multicollinearity) is a problem that you face in multiple regression. It occurs when two or more  predictor variables are highly correlated with each other  ( redundant information  among them, the predictor variables explain much of the same variation in the response). Collinearity doesn't violate the assumptions of multiple regression.   Collinearity can  hide significant effects  (if you include only one of the collinear variables in the model it is significant but when there are more than one included none of them are significant)  Collinearity  increases the variance  of the parameter estimates, making them  unstable  (the data points don't spread out enough in the space to provide stable support for the plane defined by the model) and, in turn, this  increases the prediction error  of the model   When an overall model is highly significant but the individual variables don't tell the same story, it's a  warning sign of collinearity . When the  standard error for an estimate is larger than the parameter estimate  itself, it's not going to be statistically significant. The SE tells us how variable the corresponding parameter estimate is: when the standard errors are high, the  model lacks stability .  1\n2\n3\n4 PROC REG DATA=SAS-data-set  options ;\n     label:  MODEL dependent = regressors / VIF  /options ;\nRUN;\nQUIT;    The  VIF  (variance inflation factor, $VIF_i=1/(1-R_i^2)$) option measures the magnitude of collinearity in a model (VIF$ 10$ for any predictor in the model, those predictors are probably involved in collinearity)  Other options are  COLLIN  (includes the intercept when analyzing collinearity and helps identify the predictors that are causing the problem) and  COLLINOINT  (requests the same analysis as  COLLIN  but excludes the intercept)", 
            "title": "Detecting Collinearity"
        }, 
        {
            "location": "/statistics/inference/#effective-modeling-cycle", 
            "text": "You want to get to know your data by  performing preliminary analysis :   Plot your data  Calculate descriptive statistics   Perform correlation analysis    Identify some  good candidate models  using  PROC REG :   First check for collinearity   Use all-possible regression or stepwise selection methods and subject matter knowledge to select model candidates  Identify the good ones with the Mallows' (prediction) or Hocking's (explanatory) criterion for $C_p$     Check and validate your assumtions  by creating residual plots and conducting a few other statistical tests    Deal with any  problems in your data :    Determine whether any influential observations might be throwing off your model calculations  Determine whether any variables are collinear     Revise your model    Validate your model  with data not used to build the  model (prediction testing)", 
            "title": "Effective modeling cycle"
        }, 
        {
            "location": "/statistics/categorical-data/", 
            "text": "Chapter summary in SAS\n\n\nWhen you response variable is categorical, you need to use a different kind of regression analysis: \nlogistic regression\n.\n\n\nDescribing Categorical Data\n\n\nWhen you examine the distribution of a \ncategorical variable\n, you want to know the \nvalues\n of the variable and the \nfrequency or count\n of each value in the data (\none-way frequency able\n).\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable1 variable2 variable3 \n/options\n;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\nTo look for a possible \nassociation\n between two or more categorical variables, you can create a \ncrosstabulation\n/\ncontingency table\n (when it displays statistics for two variables is also called \ntwo-way frequency able\n).\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns \n/options\n;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\nTwo distribution plots are associated with a frequency or crosstabulation table: a \nfrequency plot\n, \nPLOTS=(FREQPLOT)\n, and a \ncumulative frequency plot\n.\n\n\nIn \nPROC FREQ\n output, the default order for character values is \nalphaumeric\n. To reorder the values of an ordinal variable in your \nPROC FREQ\n output you can:\n\n\n\n\nCreate a \nnew variable\n in which the values are stored in logical order\n\n\nApply a \ntemporary format\n to the original variable\n\n\nHow to \nreplace the variable's name with the variable's label in \nPROC FREQ\n output\n\n\n\n\n1\n2\n3\n4\noptions validvarname=any;\nPROC FREQ DATA=SAS-data-set (RENAME=(variable1=\nLabel variable 1\nn variable1=\nLabel variable 1\nn));\n    TABLES \nLabel variable 1\nn;\nRUN;\n\n\n\n\n\n\n\n\nCount the distinct values of a variable\n: The question of how to count distinct values of a \nCLASS\n or \nBY\n variable using either \nPROC MEANS\n or \nPROC SUMMARY\n is asked frequently. While neither of these procedures has this ability, \nPROC SQL\n can count these values using the \nDISTINCT\n option and \nPROC FREQ\n using the \nNLEVELS\n option.\n\n\n\n\nTests of Association\n\n\nPearson Chi-square Test\n\n\nTo perform a \nformal test of association\n between two categorical variables, you use the (Pearson) \nchi-square test\n which measures the difference between the observed cell frequencies and the cell frequencies that are expected if there is no association between variables ($H_0$ is true): \n$Expected=Row \\ total\\cdot Column\\ total/Total \\ sample \\ size$\n\n\n\n\nIf the \nsample size decreases\n, the \nchi-square value decreases\n and the \np-value for the chi-square statistic increases\n\n\nHypothesis testing: \n$H_0$\n: no association; \n$H_a$\n: association\n\n\n\n\nCramer's V statistic\n\n\nIt is one \nmeasure of strength of an association\n between two categorical variables:\n\n\n\n\nFor two-by-two tables, Cramer's V is in the range of -1 to 1\n\n\nFor larger tables, Cramer's V is int he range of 0 to 1 \n\n\nValues farther away from 0 indicate a relatively strong association between the variables\n\n\n\n\nTo measure the strength of the association between a binary predictor variable and a binary outcome variable, you can use an \nodds ratio\n: $Odds \\ Ratio=\\frac{Odds \\ of \\ Outcome \\ in \\ Group \\ B}{Odds \\ of \\ Outcome \\ in \\ Group \\ A}$; $Odds=p_{event}/(1-p_{event})$\n\n\n\n\nThe value of the odds ratio can range from 0 to $\\infty$; it cannot be negative\n\n\nWhen the odds ratio is 1 , there is no association between variables\n\n\nWhen the odds ratio \n1/\n1, the group in the numerator/denominator is more likely to have the outcome\n\n\nThe odds ratio is approximately the same \nregardless of the sample size\n\n\nTo estimate the true odds ratio while taking into account the variability of the sample statistic, you can calculate \nconfidence intervals\n\n\nYou can use an odds ratio to \ntest for significance\n between two categorical variables\n\n\nOdds ratio expressed as percent difference: $(odd \\ ratio -1) \\cdot 100$\n\n\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED \n/options\n;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\n\n\nCHISQ\n produces the Pearson chi-square test of association, the likelihood-ratio chi-square and the Mantel-Haenszel: $\\sum \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$\n\n\nEXPECTED\n prints the expected cell frequencies\n\n\nCELLCHI2\n prints each cell's contribution to the total chi-square statistic: $ \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$\n\n\nNOCOL\n/\nNOROW\n suppresses the printing of the column/row percentages\n\n\nNOPERCENT\n supresses the printing of the cell percentages\n\n\nRELRISK\n (relative risk) prints a table that contains risk ratios (probability ratios) and odds ratios; \nPROC FREQ\n uses the \nclassification in the first column\n of the crosstabulation table as the \noutcome of interest\n and the first/second row in the numerator/denominator\n\n\n\n\nMantel-Haenszel chi-square test\n\n\nFor \nordinal associations\n, the \nMantel-Haenszel\n chi-square test is a more powerful test.\n\n\n\n\nThe levels must be in a \nlogical order\n for the test results to be meaningful\n\n\nHypothesis testing: \n$H_0$\n: no ordinal association; \n$H_a$\n: ordinal association\n\n\nSimilarly to the Pearson case, the Mantel-Haenszel chi-square statistic/p-value indicate whether an association exists but not its magnitude and they depend on and reflect the sample size\n\n\n\n\nTo measure the \nstrength of the association\n between two ordinal variables you can use the \nSpearman correlation\n statistic.\n\n\n\n\nYou should only use it if both variables are ordinal and are in logical order\n\n\nIs considered to be a rank correlation because it provides a degree of association between the ranks of the ordinal variables\n\n\nThis statistic has a \nrange between -1 and +1\n: values close to -1/+1 indicate that there is a relatively high degree of negative/positive correlation and values close to 0 indicate a weak correlation\n\n\nIt is \nnot affected by the sample size\n\n\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED \n/options\n;\n  \nadditional statements\n\nRUN;\n\n\n\n\n\n\n\n\nMEASURES\n produces the Spearman correlation statistic along with other measurement of association\n\n\nCL\n produces confidence bounds for the statistics that the MEASURES option requests\n\n\nThe confidence bounds are valid only if the sample size is large (\n25)\n\n\nThe asymptotic standard error (\nASE\n) is used for large samples and is used to calculate the confidence intervals for various measures of association (including the Spearman correlation coefficient)\n\n\n\n\nIntroduction to Logistic Regression\n\n\nLogistic Regression is a generalized linear model (like Linear Regression or ANOVA) that you can use to predict a categorical response/outcome based on one or more continuous/categorical predictor variables. There are three models:\n\n\n\n\nLinear vs Logistic\n\n\nLinear Regression Model\n\n\n\n\nAssumes that the expected value of the response continuous variable ($Y$) has a linear relationship with the predictor variable ($X$)\n\n\nThe conditional mean of the response hast the linear form $E(Y|X)=\\beta_0+\\beta_1X$ and it ranges $(-\\infty,+\\infty)$\n\n\n\n\n\n\nWhy not to use Linear Regression to model a binary response variable\n\n\nFollowing the Linear Regression Model scheme, the response variable is calculated as \n\n\n$Y_i=\\beta_0+\\beta_1\\cdot X_i+\\epsilon_i$, \n\n\nwhere $\\beta_0$ and $\\beta_1$ are obtained by the method of least squares. \n\n\n\n\nThis model \nassumes that the data is continuous\n, which is not true for the case of binary data\n\n\nThis model \nassumes that the mean of the response is $\\beta_0+\\beta_1\\cdot X$\n, while for binary data the mean of the response is the probability of a success\n\n\nIf the response variable has only two levels, you cannot \nassume the constant variance and normality\n that are required for linear regression\n\n\n\n\n\n\nBinary Logistic Regression Model\n\n\n\n\nThe predictor variable ($X$) is used to estimate the probability of a specific outcome ($p$) for which you need to use a nonlinear function\n\n\nThe mean of the response is a probability, which is between $(0, 1)$. \n\n\nThe \nInverse Logit Function\n binds the linear predictor between $0$ and $1$ is defined as $p_i=(1+e^{-(\\beta_0+\\beta_1 X_i)})^{-1}$\n\n\nThis model applies a \nLogit Transformation\n to the probabilities $logit(p_i)=ln\\left ( \\frac{p_i}{1-p_i} \\right ) = \\beta_0+\\beta_1X_i$, so that the transformed probabilities and predictor variables end up with a linear relationship\n\n\nThe logit is the \nnatural log of the odds\n (the probability of the event occurring divided by the probability of the event not occurring)\n\n\nWe make the \nassumption that the logit transformation of the probabilities results in a linear relationship with the predictor variables\n (we can use a linear function $X$ to model the logit in order to indireclty model the probability)\n\n\nThe logit of the probability transforms the probability into a linear function, which has no lower or upper bounds. So a \nlogit has no lower or upper bounds\n.\n\n\n\n\nPROC LOGISTIC\n\n\nTo model categorical data yu use the \nLOGISTIC\n procedure. Some of the most common statements of this procedure are shown here:\n\n\n1\n2\n3\n4\n5\n6\nPROC LOGISTIC DATA=SAS-data-set \noptions\n;\n\n    CLASS variable \n(variable_options)\n ... \n/ options\n;\n\n    MODEL response \n(variable_options)\n = predictors \n/ options\n;\n\n    UNITS independent1=list... \n/ options\n;\n    ODDSRATIO \nlabel\n variable \n/ options\n;\nRUN;\n\n\n\n\n\n\n\n\nCLASS\n is used to define the classification (categorical) predictor variables (if any); this statement must precede the \nMODEL\n statement\n\n\nCLODDS = PL\n (profile likelihood) | \nWALD\n (default) | \nBOTH\n is an example of a general option that you can specify in the \nMODEL\n statement which computes confidence intervals for the odds ratios of all predictor variables and also enables the production of the odds ratio plot\n\n\n\n\n\n\nExample\n\n\nPROC LOGISTIC DATA=statdata.sales_inc PLOTS(ONLY)=(EFFECT ODDSRATIO);\n    CLASS gender;\n    MODEL purchase(EVENT='1')=gender / CLODDS=PL;\nRUN;\n\n\n\n\nClassification Variables Parametrization\n\n\nWhen the predictor variable is categorical, the assumption of linearity cannot be met. To get past the obstacle of nonlinearity, the \nCLASS\n statement creates a set of one or more \ndesign variables\n (also called dummy variables). \nPROC LOGISTIC\n uses these variables, and not the original ones, in model calculations.\n\n\nDifferent parametrization methods for the classification variables will produce the same results regarding the significance of the categorical predictors, but understanding the parametrization method helps to interpret the results accurately.\n\n\nHere we present two of the most common methods of parameterizing (\nPARAM =\n) the classification variables. For both of them:\n\n\n\n\nThe default \nreference level\n is the level that has the highest ranked value (or the last value) when the levels are sorted in ascending alphanumeric order\n\n\nThe number of design variables (or $\\beta$) that are created are the number of levels of the classification variable -1\n\n\n\n\nEffect coding (default)\n\n\nAlso called \ndeviation from the mean coding\n, it compares the effect of each level of the variable to the \naverage effect of all levels\n. \n\n\n\n\nExample\n\n\nUsing this parametrization scheme the model will be described as follows\n\n\n$logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$\n\n\n\n\n\n\n$\\beta_0$ is the average value of the logit across all income levels\n\n\n$\\beta_1$ is the difference between the logit for income level 1 and $\\beta_0$\n\n\n$\\beta_2$ is the difference between the logit for income level 2 and $\\beta_0$\n\n\n\n\nHere's the Analysis of Maximum Likelihood Estimates table that \nPROC LOGISTIC\n generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences from the overall mean value over all levels. \n\n\n\n\nThe p-values indicate whether each particular level is significant compared to the average effect of all levels. The p-values for $\\beta_1$ and $\\beta_2$ not significant meaning that the effect of those levels is not different than the average effect of low, medium and high income.\n\n\n\n\nReference cell coding\n\n\nIt compares the effect of each level of the predictor to the effect of another \nlevel that is the designated reference level\n.\n\n\n\n\nExample\n\n\nTo use this scheme the classification variable has to be defined in the following way\n\n\nCLASS gender (PARAM=REF REF='Male');\n\n\nUsing this parametrization scheme the model will be described as follows\n\n\n$logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$\n\n\n\n\n\n\n$\\beta_0$ is the intercept, but not in terms of where you cross the $Y$ axis, instead is the value of the logit of the probability when income is high (or at the reference level)\n\n\n$\\beta_1$ is the difference between the logit of the probability for low and high income\n\n\n$\\beta_2$ is the difference between the logit of the probability for medium and high income\n\n\n\n\nHere's the Analysis of Maximum Likelihood Estimates table that \nPROC LOGISTIC\n generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences with respect to the reference level. \n\n\n\n\nThe p-values indicate whether each particular level is significant compared to the reference level. The p-value for $\\beta_1\n0.05$ is significant meaning that the effect of a low income is statistically different than the effect of a high income on the probability that people will spend at least $100\\$$. The same applies to $\\beta_2$.\n\n\n\n\nFitting a Binary Logistic Regression\n\n\n1\n2\n3\n4\n5\nPROC LOGISTIC DATA=statdata.sales_inc PLOTS(ONLY)=(EFFECT);\n    CLASS Gender (PARAM=REF REF=\nMale\n);\n    MODEL Purchase(event=\n1\n) = Gender;\n    title1 \nLOGISTIC MODEL (1): Purchase = Gender\n;\nRUN;\n\n\n\n\n\n\nWe look at the first few tables to make sure that the model is set up the way we want\n\n\n\n\nThe \nModel Information\n table describes the data set, the response variable, the number of response levels, the type of model, the algorithm used to obtain the parameter estimates, and the number of observations read and used.\n\n\nThe \nResponse Profile\n table shows the values of the response variable, listed according to their ordered value and frequency. By default, \nPROC LOGISTIC\n orders the values of the response variable alphanumercally and bases the logistic regression model on the probability of the lowest value. However, we set the \nEVENT=1\n, the highest value, so this model is based on the probability that \nPurchase=1\n.\n\n\nBelow this table, we see the probability that \nPROC LOGISTIC\n is modeling, as shown in the log.\n\n\nThe \nClass Level Information\n table displays the predictor variable in the \nCLASS\n statement \nGender\n (in the model we fixed \n'Male'\n as the reference level, so the design variable is 1 when \nGender='Female'\n and 0 when \nGender='Male'\n).\n\n\nThe \nModel Convergence Status\n simply indicates that the convergence criterion was met. There are a number of options to control the convergence criterion, but the default is the gradient convergence criterion with a default value of $10^{-8}$.\n\n\nThe \nModel Fit Statistic\n table reports the resuls of three tests (for the model with the intercept only and the model with the intercept and the predictor variables): AIC, SC and -2$\\cdot$Log(likelihood). AIC and SC are \ngoodness-of-fit measures\n that you can use to compare one model to another (lower values indicate more desirable model) and are not dependent on the number of terms in the model.\n\n\nAkaike's Information Criterion (AIC)\n: it adjusts for the number of predictor valriables. It is the best statitstic to come up with the best \nexplanatory model\n.\n\n\nSchwarz's Bayesian Criterion (SC)\n: it adjusts for the number of predictor variables and the number of observations. This test uses a bigger penalty for extra variables and therefore favors more parsimonious models. It is the best statitstic to come up with the best \npredictive model\n.\n\n\n\n\n\n\nThe \nTesting Global Null Hypothesis: BETA=0\n table provides three statistics to test $H_0$ that all the regression coefficients in the model are 0. The \nLikelihood Ratio\n is the most reliable test, specially for small sample sizes. It is similar to the overall F test in linear regression.\n\n\nThe \nType 3 Analysis of Effects\n table is generated when \nCLASS\n specifies a categorical predictor variable. The \nWald Chi-Square\n statistic tests the listed effect. When there is only one predictor variable in the model, the value listed in the table will be idential to the Wald test in the \nTesting Global Null Hypothesis\n table.\n\n\nThe \nAnalysis of Maximum Likelihood Estimates\n table lists the estimated model parameters, their standard errors, Wald test statistics and corresponding p-values. The parameter estimates are the estimated coefficients of the fitted logistic regression model. We can use these estimates to construct the logistic regression equation $logit(\\beta)=\\beta_0+\\beta_1 \\cdot Categorical \\ predictor$.\n\n\nThe \nOdds Ratio Estimates\n table shows the OR ratio for the modeled event. Notice that \nPROC LOGISTIC\n calculates Wald confidence limits by default.\n\n\nThe \nAssociation of Predicted Probabilities and Observed Responses\n table lists several goodness-of-fit measures.\n\n\nThe \nEffect plot\n shows the levels of the \nCLASS\n predictor variable vs the probability of the desired outcome. \n\n\n\n\nIterpreting the Odds Ratio for a Categorical Predictor\n\n\nLet's see how to calculate the odds and the odds ratio from the logistic regression model. Here is the logistic regression model that predicts the logit of $p$:\n\n\n$logit(\\hat p)=ln(odds)=ln\\left ( \\frac{p_i}{1-p_i} \\right )=\\beta_0 + \\beta_1 \\cdot Gender$\n\n\nAccording to our example the variable \nGender\n is codified in a way that \nFemales=1\n and \nMales=0\n, so the OR can be written:\n\n\n$odds_{females}=e^{\\beta_0+\\beta_1}$\n\n\n$odds_{males}=e^{\\beta_0}$\n\n\n$odds \\ ratio = \\frac{e^{\\beta_0+\\beta_1}}{e^{\\beta_0}}=e^{\\beta_1}$\n\n\nIf the 95% confidence interval does not include 1, the OR is significant at the 0.05 level indicating an association between the predictor and response variables of your model.\n\n\nIterpreting the Odds Ratio for a Continuous Predictor\n\n\nFor a continuous predictor variable, the OR measures the \nincrease or decrease in odds associated with a one-unit difference\n of the predictor variable by default. $OR - 1 = %$ of greater odds for having one-unit of difference.\n\n\nComparing Pairs to Assess the Fit of a Logistic Regression Model\n\n\nPROC LOGISTIC\n calculates several different goodness-of-fit measures and displayed in the \nAssociation of Predicted Probabilities and Observed Responses\n table.\n\n\nOne of these goodness-of-fit methods is comparing pairs (\nPairs\n). To start, \nPROC LOGISTIC\n creates two groups of observations, one for each value of the response variable. Then, the procedure selects pairs of observations, one from each group, until no more pairs can be selected. \nPROC LOGISTIC\ndetermines whether each pair is concordant, discordant or tied.\n\n\n\n\nA pair is \nconcordant\n if the \nmodel predicts it correclty\n, i.e. if the observation with the desired outcome has a \nhigher predicted probability\n, based on the model, than the observation without the outcome.\n\n\nA pair is \ndiscordant\n if the \nmodel does not predict it correctly\n, i.e. if the observation with the desired outcome has a \nlower predicted probability\n, based on the model, than the observation without the outcome.\n\n\nA pair is \ntied\n if it is neither concordant not discordant, i.e. the \nprobabilities are the same\n and the model can not distinguished between them.\n\n\n\n\nThe left column of the \nAssociation of Predicted Probabilities and Observed Responses\n table lists the percentage of pairs of each type. At the bottom is the total number of observation pairs on which the percentages are based, i.e. the number of pairs of observations with different outcome values $(N_{event=0} \\cdot N_{event=1})$.\n\n\nMore complex models have more than two predicted probabilities. However, regardless of the model's complexity, the same comparisons are made across all pairs of observations with different outcomes.\n\n\nYou can use these results as goodness-of-fit measures to compare one model to another. In general, higher percentage of concordant pairs and lower percentages of discordant and tied pairs indicate a more desirable model.\n\n\nThis table also shows the four rank correlation indices that are computed from the numbers of concordant $(n_c)$, discordant $(n_d)$ and tied $(n_t)$ pairs of observations:\n\n\n\n\nSomers' D\n (Gini coefficient)\n, defined as $(n_c-n_d)/(n_c+n_d+n_t)$\n\n\nGoodman-Kruskal \nGamma\n, defined as $(n_c-n_d)/(n_c+n_d)$\n\n\nKendall's \nTau-a\n, defined as $(n_c-n_d)/(0.5 \\cdot N(N-1))$, with $N$ being the sum of observation frequencies in the data\n\n\nThe concordance index \nc\n is the most commonly used of these values and estimates the probability of an observation with the desired outcome having a higher predicted probability than an observation without the desired outcome and is defined as $c=\\frac{n_c+0.5 \\cdot n_t}{n_c+n_d+n_t}$. Note that the concordance index, c, also gives an estimate of the \narea under the receiver operating characteristic (ROC) curve\n when the response is binary.\n\n\n\n\n\n\nNote\n\n\nMore information about these parameters \nhere\n.\n\n\n\n\nIn general, a model with higher values of these indices has better predictive ability than a model with lower values.\n\n\nMultiple Logistic Regression Model\n\n\nIntroduction\n\n\nSometimes you want to create a statistical model that explains the relationships among multiple predictors and a categorical response. You might want \n\n\n\n\nto examine the \neffect of each individual predictor\n on the response regardless of the levels of the other predictors \n\n\nto perform a more complex analysis that takes into account the \ninteractions between the predictors\n\n\n\n\nIn order to do this you will explore\n\n\n\n\nhow to define and explain the \nadjusted odds ratio\n\n\nhow to fit a multiple logistic regression model using the \nbackward elimination method\n\n\nhow to fit a multiple logistic regression \nmodel with interactions\n\n\n\n\nMultiple Logistic Regression\n\n\nAmultiple logistic regression model characterized the relationship between a categorical response variable and multiple predictor variables. The predictor variables can be continuous or categorical or both.\n\n\n$logit(p)=\\beta_0+\\beta_1 X_1 +...+\\beta_k X_k$\n\n\nThe goal of multiple logistic regression, like multiple linear regression, is to find the subset of variables that best explains the variability of the response variable. Models that are \nparsimonious or simple\n are more likely to be numerically stable and are also easier to generalize.\n\n\nThe Backward Elimination Method of Variable Selection\n\n\nThis method starts with a full model (a model that contains all of the main effects or predictor variables). Using an iterative process, the backward elimination method identifies and eliminates the nonsignificant predictor variables, one at a time. At each step, this method removes the least significant variable of the nonsignificant terms (the variable with the largest p-value).\n\n\nThe smaller your significance level, the more evidence you need to keep a predictor variable in the model. This results in a more parsimonious model. \n\n\nAdjusted Odds Ratios\n\n\nOne major difference between a multiple logistic model and a logistic regression model with only one predictor variable is that the odds ratios are reported differently. Multiple logistic regression uses \nadjusted odds ratios\n. An adjusted odds ratio measures the effect of a single predictor variable on a response variable while holding all the other predictor variables constant. \n\n\nThe adjusted odds ratio \nassumes that the OR for a predictor variable is the same regardless of the level of the other predictor variables\n. If that assumption is not true, then you need to fit \na more complex model that also considers the interactions\n between predictor variables.\n\n\nThe \nMODEL\n statement can include the \nCLODDS=\n option which enables the production of the OR plot (\n\nPLOTS(ONLY)=(EFFECT ODDSRATIO)\n). This option can be set to \nPL\n so that the procedure calculates profile-likelihood confidence limits for the OR of all predictor variables. These limits are based on the log likelihood and are generally preferred, especially for \nsmaller sample sizes\n.\n\n\nSpecifiying the Variable Selection Method in the \nMODEL\n Statement\n\n\nTo specify the method that \nPROC LOGISTIC\n uses to select variable in a multiple logistic regression model, you add the \nSELECTION=\n option to the \nMODEL\n statement. The possible values of the \nSELECTION=\n statement are\n\n\n\n\nNONE | N\n (default): no selection method is used and the complete model is fitted\n\n\nBACKWARD | B\n: backward elimination\n\n\nFORWARD | F\n: forward selection\n\n\nSTEPWISE | S\n: stepwise selection\n\n\nSCORE\n: best subset selection\n\n\n\n\n1\n2\n3\n4\nPROC LOGISTIC DATA=SAS-data-set \noptions\n;\n    CLASS variable \n(variable_options)\n ... \n/ options\n;\n    MODEL response \n(variable_options)\n = predictors \n/ options SELECTION\n;\nRUN;\n\n\n\n\n\n\nBy default the procedure uses a $\\alpha=0.05$ significance level to determine which variables remain in the model. If you want to change the significance level, you can use the \nSLSTAY | SLS =\n option in the \nMODEL\n statement.\n\n\nThe \nUNITS\n Statement\n\n\nThe \nUNITS\nstatement enables you to obtain customized \nodds ratio estimates\n for a specified unit of change in one or more continuous predictor variables. For each continuous predictor (or independent variable) that you want to modify, you specify the variable name, an equal sign, and a list of \none or more units of change\n, separated by spaces, that are of interest for that variable. A unit of change can be a number, a standard deviation $(SD)$, or a number multiplied by the standard deviation $(n \\times SD)$. \n\n\nThe \nUNITS\n statement is optional. If you want to use the units to change that are reflected in the stored data values, you do not need to include the \nUNITS\n statement.\n\n\nComparing the Binary and Multiple Logistic Regression Models\n\n\nSpecifying a Formatted Value as a Reference Level\n\n\nInteraction betweeen Variables\n\n\nThe Backward Elimination Method with Interactions in the Model\n\n\nSpecifying Interactions in the \nMODEL\n Statement\n\n\nThe \nODDSRATIO\n Statement\n\n\nComparing the Multiple Logistic Regression Models\n\n\nInteraction Plots\n\n\nSaving Analysis Results with the \nSTORE\n Statement\n\n\n\n\nIdentifying the use and types of logistic regression\n\n\nFitting a binary logistic regression model using the \nLOGISTIC\n procedure\n\n\nExplaining effect and reference cell coding for classification variables\n\n\nExplaining the standard output from the \nLOGISTIC\n procedure", 
            "title": "Categorical Data Analysis"
        }, 
        {
            "location": "/statistics/categorical-data/#describing-categorical-data", 
            "text": "When you examine the distribution of a  categorical variable , you want to know the  values  of the variable and the  frequency or count  of each value in the data ( one-way frequency able ).  1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable1 variable2 variable3  /options ;\n     additional statements \nRUN;   To look for a possible  association  between two or more categorical variables, you can create a  crosstabulation / contingency table  (when it displays statistics for two variables is also called  two-way frequency able ).  1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns  /options ;\n     additional statements \nRUN;   Two distribution plots are associated with a frequency or crosstabulation table: a  frequency plot ,  PLOTS=(FREQPLOT) , and a  cumulative frequency plot .  In  PROC FREQ  output, the default order for character values is  alphaumeric . To reorder the values of an ordinal variable in your  PROC FREQ  output you can:   Create a  new variable  in which the values are stored in logical order  Apply a  temporary format  to the original variable  How to  replace the variable's name with the variable's label in  PROC FREQ  output   1\n2\n3\n4 options validvarname=any;\nPROC FREQ DATA=SAS-data-set (RENAME=(variable1= Label variable 1 n variable1= Label variable 1 n));\n    TABLES  Label variable 1 n;\nRUN;    Count the distinct values of a variable : The question of how to count distinct values of a  CLASS  or  BY  variable using either  PROC MEANS  or  PROC SUMMARY  is asked frequently. While neither of these procedures has this ability,  PROC SQL  can count these values using the  DISTINCT  option and  PROC FREQ  using the  NLEVELS  option.", 
            "title": "Describing Categorical Data"
        }, 
        {
            "location": "/statistics/categorical-data/#tests-of-association", 
            "text": "", 
            "title": "Tests of Association"
        }, 
        {
            "location": "/statistics/categorical-data/#pearson-chi-square-test", 
            "text": "To perform a  formal test of association  between two categorical variables, you use the (Pearson)  chi-square test  which measures the difference between the observed cell frequencies and the cell frequencies that are expected if there is no association between variables ($H_0$ is true): \n$Expected=Row \\ total\\cdot Column\\ total/Total \\ sample \\ size$   If the  sample size decreases , the  chi-square value decreases  and the  p-value for the chi-square statistic increases  Hypothesis testing:  $H_0$ : no association;  $H_a$ : association", 
            "title": "Pearson Chi-square Test"
        }, 
        {
            "location": "/statistics/categorical-data/#cramers-v-statistic", 
            "text": "It is one  measure of strength of an association  between two categorical variables:   For two-by-two tables, Cramer's V is in the range of -1 to 1  For larger tables, Cramer's V is int he range of 0 to 1   Values farther away from 0 indicate a relatively strong association between the variables   To measure the strength of the association between a binary predictor variable and a binary outcome variable, you can use an  odds ratio : $Odds \\ Ratio=\\frac{Odds \\ of \\ Outcome \\ in \\ Group \\ B}{Odds \\ of \\ Outcome \\ in \\ Group \\ A}$; $Odds=p_{event}/(1-p_{event})$   The value of the odds ratio can range from 0 to $\\infty$; it cannot be negative  When the odds ratio is 1 , there is no association between variables  When the odds ratio  1/ 1, the group in the numerator/denominator is more likely to have the outcome  The odds ratio is approximately the same  regardless of the sample size  To estimate the true odds ratio while taking into account the variability of the sample statistic, you can calculate  confidence intervals  You can use an odds ratio to  test for significance  between two categorical variables  Odds ratio expressed as percent difference: $(odd \\ ratio -1) \\cdot 100$   1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED  /options ;\n     additional statements \nRUN;    CHISQ  produces the Pearson chi-square test of association, the likelihood-ratio chi-square and the Mantel-Haenszel: $\\sum \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$  EXPECTED  prints the expected cell frequencies  CELLCHI2  prints each cell's contribution to the total chi-square statistic: $ \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$  NOCOL / NOROW  suppresses the printing of the column/row percentages  NOPERCENT  supresses the printing of the cell percentages  RELRISK  (relative risk) prints a table that contains risk ratios (probability ratios) and odds ratios;  PROC FREQ  uses the  classification in the first column  of the crosstabulation table as the  outcome of interest  and the first/second row in the numerator/denominator", 
            "title": "Cramer's V statistic"
        }, 
        {
            "location": "/statistics/categorical-data/#mantel-haenszel-chi-square-test", 
            "text": "For  ordinal associations , the  Mantel-Haenszel  chi-square test is a more powerful test.   The levels must be in a  logical order  for the test results to be meaningful  Hypothesis testing:  $H_0$ : no ordinal association;  $H_a$ : ordinal association  Similarly to the Pearson case, the Mantel-Haenszel chi-square statistic/p-value indicate whether an association exists but not its magnitude and they depend on and reflect the sample size   To measure the  strength of the association  between two ordinal variables you can use the  Spearman correlation  statistic.   You should only use it if both variables are ordinal and are in logical order  Is considered to be a rank correlation because it provides a degree of association between the ranks of the ordinal variables  This statistic has a  range between -1 and +1 : values close to -1/+1 indicate that there is a relatively high degree of negative/positive correlation and values close to 0 indicate a weak correlation  It is  not affected by the sample size   1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED  /options ;\n   additional statements \nRUN;    MEASURES  produces the Spearman correlation statistic along with other measurement of association  CL  produces confidence bounds for the statistics that the MEASURES option requests  The confidence bounds are valid only if the sample size is large ( 25)  The asymptotic standard error ( ASE ) is used for large samples and is used to calculate the confidence intervals for various measures of association (including the Spearman correlation coefficient)", 
            "title": "Mantel-Haenszel chi-square test"
        }, 
        {
            "location": "/statistics/categorical-data/#introduction-to-logistic-regression", 
            "text": "Logistic Regression is a generalized linear model (like Linear Regression or ANOVA) that you can use to predict a categorical response/outcome based on one or more continuous/categorical predictor variables. There are three models:", 
            "title": "Introduction to Logistic Regression"
        }, 
        {
            "location": "/statistics/categorical-data/#linear-vs-logistic", 
            "text": "", 
            "title": "Linear vs Logistic"
        }, 
        {
            "location": "/statistics/categorical-data/#linear-regression-model", 
            "text": "Assumes that the expected value of the response continuous variable ($Y$) has a linear relationship with the predictor variable ($X$)  The conditional mean of the response hast the linear form $E(Y|X)=\\beta_0+\\beta_1X$ and it ranges $(-\\infty,+\\infty)$    Why not to use Linear Regression to model a binary response variable  Following the Linear Regression Model scheme, the response variable is calculated as   $Y_i=\\beta_0+\\beta_1\\cdot X_i+\\epsilon_i$,   where $\\beta_0$ and $\\beta_1$ are obtained by the method of least squares.    This model  assumes that the data is continuous , which is not true for the case of binary data  This model  assumes that the mean of the response is $\\beta_0+\\beta_1\\cdot X$ , while for binary data the mean of the response is the probability of a success  If the response variable has only two levels, you cannot  assume the constant variance and normality  that are required for linear regression", 
            "title": "Linear Regression Model"
        }, 
        {
            "location": "/statistics/categorical-data/#binary-logistic-regression-model", 
            "text": "The predictor variable ($X$) is used to estimate the probability of a specific outcome ($p$) for which you need to use a nonlinear function  The mean of the response is a probability, which is between $(0, 1)$.   The  Inverse Logit Function  binds the linear predictor between $0$ and $1$ is defined as $p_i=(1+e^{-(\\beta_0+\\beta_1 X_i)})^{-1}$  This model applies a  Logit Transformation  to the probabilities $logit(p_i)=ln\\left ( \\frac{p_i}{1-p_i} \\right ) = \\beta_0+\\beta_1X_i$, so that the transformed probabilities and predictor variables end up with a linear relationship  The logit is the  natural log of the odds  (the probability of the event occurring divided by the probability of the event not occurring)  We make the  assumption that the logit transformation of the probabilities results in a linear relationship with the predictor variables  (we can use a linear function $X$ to model the logit in order to indireclty model the probability)  The logit of the probability transforms the probability into a linear function, which has no lower or upper bounds. So a  logit has no lower or upper bounds .", 
            "title": "Binary Logistic Regression Model"
        }, 
        {
            "location": "/statistics/categorical-data/#proc-logistic", 
            "text": "To model categorical data yu use the  LOGISTIC  procedure. Some of the most common statements of this procedure are shown here:  1\n2\n3\n4\n5\n6 PROC LOGISTIC DATA=SAS-data-set  options ;     CLASS variable  (variable_options)  ...  / options ;     MODEL response  (variable_options)  = predictors  / options ;     UNITS independent1=list...  / options ;\n    ODDSRATIO  label  variable  / options ;\nRUN;    CLASS  is used to define the classification (categorical) predictor variables (if any); this statement must precede the  MODEL  statement  CLODDS = PL  (profile likelihood) |  WALD  (default) |  BOTH  is an example of a general option that you can specify in the  MODEL  statement which computes confidence intervals for the odds ratios of all predictor variables and also enables the production of the odds ratio plot    Example  PROC LOGISTIC DATA=statdata.sales_inc PLOTS(ONLY)=(EFFECT ODDSRATIO);\n    CLASS gender;\n    MODEL purchase(EVENT='1')=gender / CLODDS=PL;\nRUN;", 
            "title": "PROC LOGISTIC"
        }, 
        {
            "location": "/statistics/categorical-data/#classification-variables-parametrization", 
            "text": "When the predictor variable is categorical, the assumption of linearity cannot be met. To get past the obstacle of nonlinearity, the  CLASS  statement creates a set of one or more  design variables  (also called dummy variables).  PROC LOGISTIC  uses these variables, and not the original ones, in model calculations.  Different parametrization methods for the classification variables will produce the same results regarding the significance of the categorical predictors, but understanding the parametrization method helps to interpret the results accurately.  Here we present two of the most common methods of parameterizing ( PARAM = ) the classification variables. For both of them:   The default  reference level  is the level that has the highest ranked value (or the last value) when the levels are sorted in ascending alphanumeric order  The number of design variables (or $\\beta$) that are created are the number of levels of the classification variable -1", 
            "title": "Classification Variables Parametrization"
        }, 
        {
            "location": "/statistics/categorical-data/#effect-coding-default", 
            "text": "Also called  deviation from the mean coding , it compares the effect of each level of the variable to the  average effect of all levels .    Example  Using this parametrization scheme the model will be described as follows  $logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$    $\\beta_0$ is the average value of the logit across all income levels  $\\beta_1$ is the difference between the logit for income level 1 and $\\beta_0$  $\\beta_2$ is the difference between the logit for income level 2 and $\\beta_0$   Here's the Analysis of Maximum Likelihood Estimates table that  PROC LOGISTIC  generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences from the overall mean value over all levels.    The p-values indicate whether each particular level is significant compared to the average effect of all levels. The p-values for $\\beta_1$ and $\\beta_2$ not significant meaning that the effect of those levels is not different than the average effect of low, medium and high income.", 
            "title": "Effect coding (default)"
        }, 
        {
            "location": "/statistics/categorical-data/#reference-cell-coding", 
            "text": "It compares the effect of each level of the predictor to the effect of another  level that is the designated reference level .   Example  To use this scheme the classification variable has to be defined in the following way  CLASS gender (PARAM=REF REF='Male');  Using this parametrization scheme the model will be described as follows  $logit(p)=\\beta_0+\\beta_1\\cdot D_{Low \\ Income} + \\beta_2\\cdot D_{Medium \\ Income}$    $\\beta_0$ is the intercept, but not in terms of where you cross the $Y$ axis, instead is the value of the logit of the probability when income is high (or at the reference level)  $\\beta_1$ is the difference between the logit of the probability for low and high income  $\\beta_2$ is the difference between the logit of the probability for medium and high income   Here's the Analysis of Maximum Likelihood Estimates table that  PROC LOGISTIC  generates for this example on which the parameter estimates ($\\beta_0$, $\\beta_1$, $\\beta_2$) and p-values reflect differences with respect to the reference level.    The p-values indicate whether each particular level is significant compared to the reference level. The p-value for $\\beta_1 0.05$ is significant meaning that the effect of a low income is statistically different than the effect of a high income on the probability that people will spend at least $100\\$$. The same applies to $\\beta_2$.", 
            "title": "Reference cell coding"
        }, 
        {
            "location": "/statistics/categorical-data/#fitting-a-binary-logistic-regression", 
            "text": "1\n2\n3\n4\n5 PROC LOGISTIC DATA=statdata.sales_inc PLOTS(ONLY)=(EFFECT);\n    CLASS Gender (PARAM=REF REF= Male );\n    MODEL Purchase(event= 1 ) = Gender;\n    title1  LOGISTIC MODEL (1): Purchase = Gender ;\nRUN;   We look at the first few tables to make sure that the model is set up the way we want   The  Model Information  table describes the data set, the response variable, the number of response levels, the type of model, the algorithm used to obtain the parameter estimates, and the number of observations read and used.  The  Response Profile  table shows the values of the response variable, listed according to their ordered value and frequency. By default,  PROC LOGISTIC  orders the values of the response variable alphanumercally and bases the logistic regression model on the probability of the lowest value. However, we set the  EVENT=1 , the highest value, so this model is based on the probability that  Purchase=1 .  Below this table, we see the probability that  PROC LOGISTIC  is modeling, as shown in the log.  The  Class Level Information  table displays the predictor variable in the  CLASS  statement  Gender  (in the model we fixed  'Male'  as the reference level, so the design variable is 1 when  Gender='Female'  and 0 when  Gender='Male' ).  The  Model Convergence Status  simply indicates that the convergence criterion was met. There are a number of options to control the convergence criterion, but the default is the gradient convergence criterion with a default value of $10^{-8}$.  The  Model Fit Statistic  table reports the resuls of three tests (for the model with the intercept only and the model with the intercept and the predictor variables): AIC, SC and -2$\\cdot$Log(likelihood). AIC and SC are  goodness-of-fit measures  that you can use to compare one model to another (lower values indicate more desirable model) and are not dependent on the number of terms in the model.  Akaike's Information Criterion (AIC) : it adjusts for the number of predictor valriables. It is the best statitstic to come up with the best  explanatory model .  Schwarz's Bayesian Criterion (SC) : it adjusts for the number of predictor variables and the number of observations. This test uses a bigger penalty for extra variables and therefore favors more parsimonious models. It is the best statitstic to come up with the best  predictive model .    The  Testing Global Null Hypothesis: BETA=0  table provides three statistics to test $H_0$ that all the regression coefficients in the model are 0. The  Likelihood Ratio  is the most reliable test, specially for small sample sizes. It is similar to the overall F test in linear regression.  The  Type 3 Analysis of Effects  table is generated when  CLASS  specifies a categorical predictor variable. The  Wald Chi-Square  statistic tests the listed effect. When there is only one predictor variable in the model, the value listed in the table will be idential to the Wald test in the  Testing Global Null Hypothesis  table.  The  Analysis of Maximum Likelihood Estimates  table lists the estimated model parameters, their standard errors, Wald test statistics and corresponding p-values. The parameter estimates are the estimated coefficients of the fitted logistic regression model. We can use these estimates to construct the logistic regression equation $logit(\\beta)=\\beta_0+\\beta_1 \\cdot Categorical \\ predictor$.  The  Odds Ratio Estimates  table shows the OR ratio for the modeled event. Notice that  PROC LOGISTIC  calculates Wald confidence limits by default.  The  Association of Predicted Probabilities and Observed Responses  table lists several goodness-of-fit measures.  The  Effect plot  shows the levels of the  CLASS  predictor variable vs the probability of the desired outcome.", 
            "title": "Fitting a Binary Logistic Regression"
        }, 
        {
            "location": "/statistics/categorical-data/#iterpreting-the-odds-ratio-for-a-categorical-predictor", 
            "text": "Let's see how to calculate the odds and the odds ratio from the logistic regression model. Here is the logistic regression model that predicts the logit of $p$:  $logit(\\hat p)=ln(odds)=ln\\left ( \\frac{p_i}{1-p_i} \\right )=\\beta_0 + \\beta_1 \\cdot Gender$  According to our example the variable  Gender  is codified in a way that  Females=1  and  Males=0 , so the OR can be written:  $odds_{females}=e^{\\beta_0+\\beta_1}$  $odds_{males}=e^{\\beta_0}$  $odds \\ ratio = \\frac{e^{\\beta_0+\\beta_1}}{e^{\\beta_0}}=e^{\\beta_1}$  If the 95% confidence interval does not include 1, the OR is significant at the 0.05 level indicating an association between the predictor and response variables of your model.", 
            "title": "Iterpreting the Odds Ratio for a Categorical Predictor"
        }, 
        {
            "location": "/statistics/categorical-data/#iterpreting-the-odds-ratio-for-a-continuous-predictor", 
            "text": "For a continuous predictor variable, the OR measures the  increase or decrease in odds associated with a one-unit difference  of the predictor variable by default. $OR - 1 = %$ of greater odds for having one-unit of difference.", 
            "title": "Iterpreting the Odds Ratio for a Continuous Predictor"
        }, 
        {
            "location": "/statistics/categorical-data/#comparing-pairs-to-assess-the-fit-of-a-logistic-regression-model", 
            "text": "PROC LOGISTIC  calculates several different goodness-of-fit measures and displayed in the  Association of Predicted Probabilities and Observed Responses  table.  One of these goodness-of-fit methods is comparing pairs ( Pairs ). To start,  PROC LOGISTIC  creates two groups of observations, one for each value of the response variable. Then, the procedure selects pairs of observations, one from each group, until no more pairs can be selected.  PROC LOGISTIC determines whether each pair is concordant, discordant or tied.   A pair is  concordant  if the  model predicts it correclty , i.e. if the observation with the desired outcome has a  higher predicted probability , based on the model, than the observation without the outcome.  A pair is  discordant  if the  model does not predict it correctly , i.e. if the observation with the desired outcome has a  lower predicted probability , based on the model, than the observation without the outcome.  A pair is  tied  if it is neither concordant not discordant, i.e. the  probabilities are the same  and the model can not distinguished between them.   The left column of the  Association of Predicted Probabilities and Observed Responses  table lists the percentage of pairs of each type. At the bottom is the total number of observation pairs on which the percentages are based, i.e. the number of pairs of observations with different outcome values $(N_{event=0} \\cdot N_{event=1})$.  More complex models have more than two predicted probabilities. However, regardless of the model's complexity, the same comparisons are made across all pairs of observations with different outcomes.  You can use these results as goodness-of-fit measures to compare one model to another. In general, higher percentage of concordant pairs and lower percentages of discordant and tied pairs indicate a more desirable model.  This table also shows the four rank correlation indices that are computed from the numbers of concordant $(n_c)$, discordant $(n_d)$ and tied $(n_t)$ pairs of observations:   Somers' D  (Gini coefficient) , defined as $(n_c-n_d)/(n_c+n_d+n_t)$  Goodman-Kruskal  Gamma , defined as $(n_c-n_d)/(n_c+n_d)$  Kendall's  Tau-a , defined as $(n_c-n_d)/(0.5 \\cdot N(N-1))$, with $N$ being the sum of observation frequencies in the data  The concordance index  c  is the most commonly used of these values and estimates the probability of an observation with the desired outcome having a higher predicted probability than an observation without the desired outcome and is defined as $c=\\frac{n_c+0.5 \\cdot n_t}{n_c+n_d+n_t}$. Note that the concordance index, c, also gives an estimate of the  area under the receiver operating characteristic (ROC) curve  when the response is binary.    Note  More information about these parameters  here .   In general, a model with higher values of these indices has better predictive ability than a model with lower values.", 
            "title": "Comparing Pairs to Assess the Fit of a Logistic Regression Model"
        }, 
        {
            "location": "/statistics/categorical-data/#multiple-logistic-regression-model", 
            "text": "", 
            "title": "Multiple Logistic Regression Model"
        }, 
        {
            "location": "/statistics/categorical-data/#introduction", 
            "text": "Sometimes you want to create a statistical model that explains the relationships among multiple predictors and a categorical response. You might want    to examine the  effect of each individual predictor  on the response regardless of the levels of the other predictors   to perform a more complex analysis that takes into account the  interactions between the predictors   In order to do this you will explore   how to define and explain the  adjusted odds ratio  how to fit a multiple logistic regression model using the  backward elimination method  how to fit a multiple logistic regression  model with interactions", 
            "title": "Introduction"
        }, 
        {
            "location": "/statistics/categorical-data/#multiple-logistic-regression", 
            "text": "Amultiple logistic regression model characterized the relationship between a categorical response variable and multiple predictor variables. The predictor variables can be continuous or categorical or both.  $logit(p)=\\beta_0+\\beta_1 X_1 +...+\\beta_k X_k$  The goal of multiple logistic regression, like multiple linear regression, is to find the subset of variables that best explains the variability of the response variable. Models that are  parsimonious or simple  are more likely to be numerically stable and are also easier to generalize.", 
            "title": "Multiple Logistic Regression"
        }, 
        {
            "location": "/statistics/categorical-data/#the-backward-elimination-method-of-variable-selection", 
            "text": "This method starts with a full model (a model that contains all of the main effects or predictor variables). Using an iterative process, the backward elimination method identifies and eliminates the nonsignificant predictor variables, one at a time. At each step, this method removes the least significant variable of the nonsignificant terms (the variable with the largest p-value).  The smaller your significance level, the more evidence you need to keep a predictor variable in the model. This results in a more parsimonious model.", 
            "title": "The Backward Elimination Method of Variable Selection"
        }, 
        {
            "location": "/statistics/categorical-data/#adjusted-odds-ratios", 
            "text": "One major difference between a multiple logistic model and a logistic regression model with only one predictor variable is that the odds ratios are reported differently. Multiple logistic regression uses  adjusted odds ratios . An adjusted odds ratio measures the effect of a single predictor variable on a response variable while holding all the other predictor variables constant.   The adjusted odds ratio  assumes that the OR for a predictor variable is the same regardless of the level of the other predictor variables . If that assumption is not true, then you need to fit  a more complex model that also considers the interactions  between predictor variables.  The  MODEL  statement can include the  CLODDS=  option which enables the production of the OR plot ( PLOTS(ONLY)=(EFFECT ODDSRATIO) ). This option can be set to  PL  so that the procedure calculates profile-likelihood confidence limits for the OR of all predictor variables. These limits are based on the log likelihood and are generally preferred, especially for  smaller sample sizes .", 
            "title": "Adjusted Odds Ratios"
        }, 
        {
            "location": "/statistics/categorical-data/#specifiying-the-variable-selection-method-in-the-model-statement", 
            "text": "To specify the method that  PROC LOGISTIC  uses to select variable in a multiple logistic regression model, you add the  SELECTION=  option to the  MODEL  statement. The possible values of the  SELECTION=  statement are   NONE | N  (default): no selection method is used and the complete model is fitted  BACKWARD | B : backward elimination  FORWARD | F : forward selection  STEPWISE | S : stepwise selection  SCORE : best subset selection   1\n2\n3\n4 PROC LOGISTIC DATA=SAS-data-set  options ;\n    CLASS variable  (variable_options)  ...  / options ;\n    MODEL response  (variable_options)  = predictors  / options SELECTION ;\nRUN;   By default the procedure uses a $\\alpha=0.05$ significance level to determine which variables remain in the model. If you want to change the significance level, you can use the  SLSTAY | SLS =  option in the  MODEL  statement.", 
            "title": "Specifiying the Variable Selection Method in the MODEL Statement"
        }, 
        {
            "location": "/statistics/categorical-data/#the-units-statement", 
            "text": "The  UNITS statement enables you to obtain customized  odds ratio estimates  for a specified unit of change in one or more continuous predictor variables. For each continuous predictor (or independent variable) that you want to modify, you specify the variable name, an equal sign, and a list of  one or more units of change , separated by spaces, that are of interest for that variable. A unit of change can be a number, a standard deviation $(SD)$, or a number multiplied by the standard deviation $(n \\times SD)$.   The  UNITS  statement is optional. If you want to use the units to change that are reflected in the stored data values, you do not need to include the  UNITS  statement.", 
            "title": "The UNITS Statement"
        }, 
        {
            "location": "/statistics/categorical-data/#comparing-the-binary-and-multiple-logistic-regression-models", 
            "text": "", 
            "title": "Comparing the Binary and Multiple Logistic Regression Models"
        }, 
        {
            "location": "/statistics/categorical-data/#specifying-a-formatted-value-as-a-reference-level", 
            "text": "", 
            "title": "Specifying a Formatted Value as a Reference Level"
        }, 
        {
            "location": "/statistics/categorical-data/#interaction-betweeen-variables", 
            "text": "", 
            "title": "Interaction betweeen Variables"
        }, 
        {
            "location": "/statistics/categorical-data/#the-backward-elimination-method-with-interactions-in-the-model", 
            "text": "", 
            "title": "The Backward Elimination Method with Interactions in the Model"
        }, 
        {
            "location": "/statistics/categorical-data/#specifying-interactions-in-the-model-statement", 
            "text": "", 
            "title": "Specifying Interactions in the MODEL Statement"
        }, 
        {
            "location": "/statistics/categorical-data/#the-oddsratio-statement", 
            "text": "", 
            "title": "The ODDSRATIO Statement"
        }, 
        {
            "location": "/statistics/categorical-data/#comparing-the-multiple-logistic-regression-models", 
            "text": "", 
            "title": "Comparing the Multiple Logistic Regression Models"
        }, 
        {
            "location": "/statistics/categorical-data/#interaction-plots", 
            "text": "", 
            "title": "Interaction Plots"
        }, 
        {
            "location": "/statistics/categorical-data/#saving-analysis-results-with-the-store-statement", 
            "text": "Identifying the use and types of logistic regression  Fitting a binary logistic regression model using the  LOGISTIC  procedure  Explaining effect and reference cell coding for classification variables  Explaining the standard output from the  LOGISTIC  procedure", 
            "title": "Saving Analysis Results with the STORE Statement"
        }, 
        {
            "location": "/statistics/prediction/", 
            "text": "Chapter summary in SAS\n\n\nIntroduction to Predictive Modeling\n\n\nScoring Predictive Models", 
            "title": "Model Building and Scoring for Prediction"
        }, 
        {
            "location": "/statistics/prediction/#introduction-to-predictive-modeling", 
            "text": "", 
            "title": "Introduction to Predictive Modeling"
        }, 
        {
            "location": "/statistics/prediction/#scoring-predictive-models", 
            "text": "", 
            "title": "Scoring Predictive Models"
        }, 
        {
            "location": "/statistics/miscellanea/", 
            "text": "McNemar's Test\n vs \nCohen's Kappa Coefficient\n\n\nMcNemar's test\n is a statistical test used on paired nominal data. It is applied to 2 \u00d7 2 contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (that is, whether there is \"marginal homogeneity\"). \n\n\nThe null hypothesis of marginal homogeneity states that the two marginal probabilities for each outcome are the same.\n\n\nCohen's kappa coefficient\n is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, since $\\kappa$ takes into account the possibility of the agreement occurring by chance. There is controversy surrounding Cohen\u2019s Kappa due to the difficulty in interpreting indices of agreement. Some researchers have suggested that it is conceptually simpler to evaluate disagreement between items.\n\n\nIf the raters are in complete agreement then $\\kappa=1$. If there is no agreement among the raters other than what would be expected by chance (as given by pe), $\\kappa \\le 0$.\n\n\nNote that Cohen's kappa measures agreement between \ntwo raters only\n. The Fleiss kappa is a \nmulti-rater\n generalization of Scott's pi statistic. Kappa is also used to compare performance in machine learning but the directional version known as Informedness or Youden's J statistic is argued to be more appropriate for supervised learning.\n\n\nYates' Correction for Continuity\n\n\nYates' correction for continuity (or Yates' chi-squared test) is used in certain situations when testing for independence in a contingency table. It is a correction made to account for the fact that both Pearson\u2019s chi-square test and McNemar\u2019s chi-square test are biased upwards for a 2 x 2 contingency table. An upwards bias tends to make results larger than they should be. If you are creating a 2 x 2 contingency table that uses either of these two tests, the Yates correction is usually recommended\n\n\nCochran-Mantel-Haenszel Test\n\n\nIn statistics, the Cochran\u2013Mantel\u2013Haenszel test (CMH) is a test used in the \nanalysis of stratified or matched categorical data\n. It allows an investigator to test the association between a binary predictor or treatment and a binary outcome such as case or control status while taking into account the stratification. It is often used in observational studies where random assignment of subjects to different treatments cannot be controlled, but confounding covariates can be measured.\n\n\nThe null hypothesis is that there is no association between the treatment and the outcome. More precisely, the null hypothesis is $H_{0}:R=1$ and the alternative hypothesis is $H_{1}:R\\neq 1$. It follows a $\\chi^{2}$ distribution asymptotically under $H_{0}$.\n\n\n\n\nThe CMH test is a \ngeneralization of the McNemar test\n as their test statistics are identical when the strata are pairs. Unlike the McNemar test which can only handle pairs, the CMH test \nhandles arbitrary strata size\n.\n\n\nConditional logistic regression\n is more general than the CMH test as it can handle continuous variable and perform multivariate analysis. When the CMH test can be applied, the CMH test statistic and the score test statistic of the conditional logistic regression are identical.\n\n\nThe CMH test supposes that the effect of the treatment is homogeneous in all strata. The \nBreslow-Day for homogeneous association test\n allows to test this assumption. It should be noted that this is not a concern if the strata are small e.g. pairs.\n\n\n\n\n\n\nWarning\n\n\nThe Mantel-Haenszel statistic has 1 degree of freedom and assumes that either exposure or disease are \nmeasured on an ordinal (or interval) scales\n when you have \nmore than two levels\n.\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nExample of CMH vs Fisher", 
            "title": "Miscellanea"
        }, 
        {
            "location": "/statistics/miscellanea/#mcnemars-test-vs-cohens-kappa-coefficient", 
            "text": "McNemar's test  is a statistical test used on paired nominal data. It is applied to 2 \u00d7 2 contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (that is, whether there is \"marginal homogeneity\").   The null hypothesis of marginal homogeneity states that the two marginal probabilities for each outcome are the same.  Cohen's kappa coefficient  is a statistic which measures inter-rater agreement for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, since $\\kappa$ takes into account the possibility of the agreement occurring by chance. There is controversy surrounding Cohen\u2019s Kappa due to the difficulty in interpreting indices of agreement. Some researchers have suggested that it is conceptually simpler to evaluate disagreement between items.  If the raters are in complete agreement then $\\kappa=1$. If there is no agreement among the raters other than what would be expected by chance (as given by pe), $\\kappa \\le 0$.  Note that Cohen's kappa measures agreement between  two raters only . The Fleiss kappa is a  multi-rater  generalization of Scott's pi statistic. Kappa is also used to compare performance in machine learning but the directional version known as Informedness or Youden's J statistic is argued to be more appropriate for supervised learning.", 
            "title": "McNemar's Test vs Cohen's Kappa Coefficient"
        }, 
        {
            "location": "/statistics/miscellanea/#yates-correction-for-continuity", 
            "text": "Yates' correction for continuity (or Yates' chi-squared test) is used in certain situations when testing for independence in a contingency table. It is a correction made to account for the fact that both Pearson\u2019s chi-square test and McNemar\u2019s chi-square test are biased upwards for a 2 x 2 contingency table. An upwards bias tends to make results larger than they should be. If you are creating a 2 x 2 contingency table that uses either of these two tests, the Yates correction is usually recommended", 
            "title": "Yates' Correction for Continuity"
        }, 
        {
            "location": "/statistics/miscellanea/#cochran-mantel-haenszel-test", 
            "text": "In statistics, the Cochran\u2013Mantel\u2013Haenszel test (CMH) is a test used in the  analysis of stratified or matched categorical data . It allows an investigator to test the association between a binary predictor or treatment and a binary outcome such as case or control status while taking into account the stratification. It is often used in observational studies where random assignment of subjects to different treatments cannot be controlled, but confounding covariates can be measured.  The null hypothesis is that there is no association between the treatment and the outcome. More precisely, the null hypothesis is $H_{0}:R=1$ and the alternative hypothesis is $H_{1}:R\\neq 1$. It follows a $\\chi^{2}$ distribution asymptotically under $H_{0}$.   The CMH test is a  generalization of the McNemar test  as their test statistics are identical when the strata are pairs. Unlike the McNemar test which can only handle pairs, the CMH test  handles arbitrary strata size .  Conditional logistic regression  is more general than the CMH test as it can handle continuous variable and perform multivariate analysis. When the CMH test can be applied, the CMH test statistic and the score test statistic of the conditional logistic regression are identical.  The CMH test supposes that the effect of the treatment is homogeneous in all strata. The  Breslow-Day for homogeneous association test  allows to test this assumption. It should be noted that this is not a concern if the strata are small e.g. pairs.    Warning  The Mantel-Haenszel statistic has 1 degree of freedom and assumes that either exposure or disease are  measured on an ordinal (or interval) scales  when you have  more than two levels .    Check these websites   Example of CMH vs Fisher", 
            "title": "Cochran-Mantel-Haenszel Test"
        }, 
        {
            "location": "/other-analysis/missing-data/", 
            "text": "Seealso\n\n\nCheck out \nthis review\n on techniques for treating missing data.\n\n\n\n\nMissing Data Mechanisms and Patterns\n\n\nTo use the more appropriate method to deal with your missing data, you should consider the missing data mechanism of your data which describes the process that is believed to have generated the missing values. According to \nRubin (1976)\n, there are three mechanisms under which missing data can occur:\n\n\n\n\nMissing completely at random (MCAR)\n:  neither the variables in the dataset nor the unobserved value of the variable itself predict whether a value will be missing\n\n\nMissing at random (MAR)\n: other variables (but not the variable itself) in the dataset can be used to predict missingness on a given variable\n\n\nMissing not at random (MNAR)\n: value of the unobserved variable itself predicts missingness\n\n\n\n\n\n\nSeealso\n\n\nCheck out the formal description of each missing mechanism in the \"Missing data mechanisms\" section of \nthis\n paper.\n\n\n\n\nObjetives of Imputation\n\n\nDepending on the \ntype of data and model\n you will be using, techniques such as \nmultiple imputation\n or \ndirect maximum likelihood\n may better serve your needs. The main goals of statistical analysis with missing data are:\n\n\n\n\nMinimize bias\n\n\nMaximize use of available information\n\n\nObtain appropriate estimates of uncertainty\n\n\n\n\nImputed values are \nnot\n equivalent to observed values and serve only to help estimate the covariances between variables needed for inference.\n\n\n\n\nAutomated imputations generally fall into one of six categories: \n\n\n\n\nDeterministic imputation \n\n\nModel based imputation \n\n\nDeck imputation \n\n\nMixed imputation \n\n\nExpert Systems \n\n\nNeural networks \n\n\n\n\nDeletion procedures\n\n\n\n\nComplete case analysis (listwise deletion)\n:  deleting cases in a particular dataset that are missing data on any variable of interest (for MCAR cases the power is reduced but it does not add any bias). It is a common technique because it is easy to implement and works with any type of analysis.\n\n\nAvailable case analysis (pairwise deletion)\n:  deleting cases where a variable required for a particular analysis is missing, but including those cases in analyses for which all required variables are present. One of the main drawbacks of this method is no consistent sample size because depending on the pairwise comparison examined, the sample size will change based on the amount of missing present in one or both variables. This method became popular because the loss of power due to missing information is not as substantial as with complete case analysis. Unless the mechanism of missing data is MCAR, this method will introduce bias into the parameter estimates. Therefore, this method is not recommended.\n\n\n\n\nReplacement procedures\n\n\nData replacement does not compensate for a badly designed instrument or for poor data collection. Overall, replacement procedures can be used in certain cases, as long as the researcher has a good reason for replacing.\n\n\nThe most important advantages of these procedures are the retention of the sample size (statistical power). To a greater or lesser extent, all replacement procedures are biased if there is a non-random distribution of missing values. In assessing the effectiveness of these procedures, both the accuracy of estimating the value of missing data and the accuracy of estimating the statistical effects have to be considered.\n\n\nMany different missing data replacement procedures have been developed over the years. In general, the differences between the various methods decrease with: (a) larger sample size, (b) a smaller percentage of missing values, (c) fewer missing variables and (d) a decrease in the level of the correlations between the variables. \n\n\nSingle Imputation Methods\n\n\nSingle imputation denotes that the missing value is replaced by a value. However, the imputed values are assumed to be the real values that would have been observed when the data would have been complete. When we have missing data, this is never the case. We can \nnever be completely certain about imputed values\n. \n\n\n\n\nUnconditional Mean Imputation / Mean Substitution\n: replacing the missing values for an individual variable wih it's overall estimated mean from the available cases. Its more important problem is that it will result in an artificial reduction in variability due to the fact you are imputing values at the center of the variable's distribution. This also has the unintended consequence of changing the magnitude of correlations between the imputed variable and other variables.\n\n\nRegression Imputation\n: This is a two-step approach: first, the relationships among variables are estimated, and then the regression coefficients are used to estimate the missing value. The underlying assumption of regression imputation is the existence of a linear relationship between the predictors and the missing variable. The technique also assumes that values are missing at random (i.e., a missing value is not related to the value of the predictors).\n\n\nStochastic Regression Imputation\n: In recognition of the problems with regression imputation and the reduced variability associated with this approach, researchers developed a technique to incorporate or \u201cadd back\u201d lost variability. A residual term, that is randomly drawn from a normal distribution with mean zero and variance equal to the residual variance from the regression model, is added to the predicted scores from the regression imputation thus restoring some of the lost variability. This method is superior to the previous methods as it will produce unbiased coefficient estimates under MAR. However, the standard errors produced during regression estimation while less biased then the single imputation approach, will still be attenuated.\n\n\n\n\n\n\n\n\n\n\nThe deterministic imputations are exactly at the regression predictions and ignore predictive uncertainty. In contrast, the random imputations are more variable and better capture the range of the data.\n\n\n\n\nHot-deck Imputation\n: According to this technique, the researcher should replace a missing value with the actual score from a similar case in the dataset. One form of hot-deck imputation is called \"last observation carried forward\" (LOCF), which involves sorting a dataset according to any of a number of variables, thus creating an ordered dataset. The technique then finds the first missing value and uses the cell value immediately prior to the data that are missing to impute the missing value. This method is known to increase risk of increasing bias and potentially false conclusions. For this reason LOCF is not recommended for use.\n\n\n\n\nCold-deck Imputation\n: This method replaces a missing value of an item with a constant value from an external source such as a value from a previous survey.\n\n\n\n\n\n\nSingle Imputation\n:\n\n\n\n\n\n\nMultiple imputation\n\n\n\n\nSeealso\n\n\nVisit \nthis website\n for more information.\n\n\n\n\nBayesian Methods for Completing Data\n are simply methods based on conditional probability.\n\n\nMultiple Imputation is always superior to any of the single imputation methods because:\n\n\n\n\nA single imputed value is never used\n\n\nThe variance estimates reflect the appropriate amount of uncertainty surrounding parameter estimates\n\n\n\n\nThere are several decisions to be made before performing a multiple imputation including \ndistribution\n, \nauxiliary variables\n and \nnumber of imputations\n that can affect the quality of the imputation.\n\n\n\n\nImputation phase (\nPROC MI\n)\n:  the user specifies the imputation model to be used and the number of imputed datasets to be created\n\n\nAnalysis phase (\nPROG GLM\n/\nPROC GENMOD\n)\n: runs the analytic model of interest within each of the imputed datasets\n\n\nPooling phase (\nPROC MIANALYZE\n)\n: combines all the estimates across all the imputed datasets and outputs one set of parameter estimates for the model of interest\n\n\n\n\nMVN\n vs \nFCS\n\n\nAuxiliary variables\n\n\n\n\nThey can can help improve the likelihood of meeting the MAR assumption \n\n\nThey help yield more accurate and stable estimates and thus reduce the estimated standard errors in analytic models \n\n\nIncluding them can also help to increase power\n\n\n\n\nNumber of imputations (m)\n\n\n\n\nEstimates of coefficients stabilize at much lower values of \nm\n than estimates of variances and covariances of error terms \n\n\nA larger number of imputations may also allow hypothesis tests with less restrictive assumptions (i.e., that do not assume equal fractions of missing information for all coefficients)\n\n\nMultiple runs of m imputations are recommended to assess the stability of the parameter estimates\n\n\nRecommendations: \n\n\nFor low fractions of missing information (and relatively simple analysis techniques) 5-20 imputations and 50 or more when the proportion of missing data is relatively high\n\n\nThe number of imputations should equal the percentage of incomplete cases (\nm\n=max(FMI%)), this way the error associated with estimating the regression coefficients, standard errors and the resulting p-values is considerably reduced and results in an adequate level of reproducibility\n\n\n\n\nMore comments\n\n\n\n\nYou should include the dependent variable (DV) in the imputation model unless you would like to impute independent variables (IVs) assuming they are uncorrelated with your DV\n\n\nAlthough MI can perform well up to 50% missing observations,  the larger the amount the higher the chance of finding estimation problems during the imputation process and the lower the chance of meeting the MAR assumption\n\n\n\n\nModel-based Procedures\n\n\nDirect Maximum Likelihood\n\n\nThis approach to analyzing missing data has many different forms. In its simplest form, it assumes that the observed data are a sample drawn from a multivariate normal distribution. The parameters are estimated by available data, and then missing scores are estimated based on the parameters just estimated. Contrary to the techniques discussed above, maximum likelihood procedures allow explicit modeling of missing data that is open to scientific analysis and critique. \n\n\nExpectation Maximization\n\n\nThis algorithm is an iterative process. The first iteration estimates missing data and then parameters using maximum likelihood. The second iteration re-estimates the missing data based on the new parameter estimates and then recalculates the new parameters estimates\nbased on actual and re-estimated missing data. The approach continues until there is convergence in the parameter estimates.\n\n\nSummary\n\n\nThe best technique to deal with your missing data depends on:\n\n\n\n\nThe amount of missing data (what percentage of data is missing)\n\n\nType of missing data (MAR, MCAR, NMAR)\n\n\n\n\nAccording to \nthis nice review\n, if more than 10% data is missing, the best solution is:\n\n\n\n\nMaximum likelihood imputation if data are NMAR (non-missing at random)\n\n\nMaximum likelihood and hot-deck if data are MAR (missing at random)\n\n\nPairwise deletion, hot-deck or regression if data are MCAR (missing completely at random)\n\n\n\n\nMoreover, \nmultiple imputation\n by chained equations is regarded the best imputation method by many researchers.", 
            "title": "Dealing with Missing Data"
        }, 
        {
            "location": "/other-analysis/missing-data/#missing-data-mechanisms-and-patterns", 
            "text": "To use the more appropriate method to deal with your missing data, you should consider the missing data mechanism of your data which describes the process that is believed to have generated the missing values. According to  Rubin (1976) , there are three mechanisms under which missing data can occur:   Missing completely at random (MCAR) :  neither the variables in the dataset nor the unobserved value of the variable itself predict whether a value will be missing  Missing at random (MAR) : other variables (but not the variable itself) in the dataset can be used to predict missingness on a given variable  Missing not at random (MNAR) : value of the unobserved variable itself predicts missingness    Seealso  Check out the formal description of each missing mechanism in the \"Missing data mechanisms\" section of  this  paper.", 
            "title": "Missing Data Mechanisms and Patterns"
        }, 
        {
            "location": "/other-analysis/missing-data/#objetives-of-imputation", 
            "text": "Depending on the  type of data and model  you will be using, techniques such as  multiple imputation  or  direct maximum likelihood  may better serve your needs. The main goals of statistical analysis with missing data are:   Minimize bias  Maximize use of available information  Obtain appropriate estimates of uncertainty   Imputed values are  not  equivalent to observed values and serve only to help estimate the covariances between variables needed for inference.   Automated imputations generally fall into one of six categories:    Deterministic imputation   Model based imputation   Deck imputation   Mixed imputation   Expert Systems   Neural networks", 
            "title": "Objetives of Imputation"
        }, 
        {
            "location": "/other-analysis/missing-data/#deletion-procedures", 
            "text": "Complete case analysis (listwise deletion) :  deleting cases in a particular dataset that are missing data on any variable of interest (for MCAR cases the power is reduced but it does not add any bias). It is a common technique because it is easy to implement and works with any type of analysis.  Available case analysis (pairwise deletion) :  deleting cases where a variable required for a particular analysis is missing, but including those cases in analyses for which all required variables are present. One of the main drawbacks of this method is no consistent sample size because depending on the pairwise comparison examined, the sample size will change based on the amount of missing present in one or both variables. This method became popular because the loss of power due to missing information is not as substantial as with complete case analysis. Unless the mechanism of missing data is MCAR, this method will introduce bias into the parameter estimates. Therefore, this method is not recommended.", 
            "title": "Deletion procedures"
        }, 
        {
            "location": "/other-analysis/missing-data/#replacement-procedures", 
            "text": "Data replacement does not compensate for a badly designed instrument or for poor data collection. Overall, replacement procedures can be used in certain cases, as long as the researcher has a good reason for replacing.  The most important advantages of these procedures are the retention of the sample size (statistical power). To a greater or lesser extent, all replacement procedures are biased if there is a non-random distribution of missing values. In assessing the effectiveness of these procedures, both the accuracy of estimating the value of missing data and the accuracy of estimating the statistical effects have to be considered.  Many different missing data replacement procedures have been developed over the years. In general, the differences between the various methods decrease with: (a) larger sample size, (b) a smaller percentage of missing values, (c) fewer missing variables and (d) a decrease in the level of the correlations between the variables.", 
            "title": "Replacement procedures"
        }, 
        {
            "location": "/other-analysis/missing-data/#single-imputation-methods", 
            "text": "Single imputation denotes that the missing value is replaced by a value. However, the imputed values are assumed to be the real values that would have been observed when the data would have been complete. When we have missing data, this is never the case. We can  never be completely certain about imputed values .    Unconditional Mean Imputation / Mean Substitution : replacing the missing values for an individual variable wih it's overall estimated mean from the available cases. Its more important problem is that it will result in an artificial reduction in variability due to the fact you are imputing values at the center of the variable's distribution. This also has the unintended consequence of changing the magnitude of correlations between the imputed variable and other variables.  Regression Imputation : This is a two-step approach: first, the relationships among variables are estimated, and then the regression coefficients are used to estimate the missing value. The underlying assumption of regression imputation is the existence of a linear relationship between the predictors and the missing variable. The technique also assumes that values are missing at random (i.e., a missing value is not related to the value of the predictors).  Stochastic Regression Imputation : In recognition of the problems with regression imputation and the reduced variability associated with this approach, researchers developed a technique to incorporate or \u201cadd back\u201d lost variability. A residual term, that is randomly drawn from a normal distribution with mean zero and variance equal to the residual variance from the regression model, is added to the predicted scores from the regression imputation thus restoring some of the lost variability. This method is superior to the previous methods as it will produce unbiased coefficient estimates under MAR. However, the standard errors produced during regression estimation while less biased then the single imputation approach, will still be attenuated.      The deterministic imputations are exactly at the regression predictions and ignore predictive uncertainty. In contrast, the random imputations are more variable and better capture the range of the data.   Hot-deck Imputation : According to this technique, the researcher should replace a missing value with the actual score from a similar case in the dataset. One form of hot-deck imputation is called \"last observation carried forward\" (LOCF), which involves sorting a dataset according to any of a number of variables, thus creating an ordered dataset. The technique then finds the first missing value and uses the cell value immediately prior to the data that are missing to impute the missing value. This method is known to increase risk of increasing bias and potentially false conclusions. For this reason LOCF is not recommended for use.   Cold-deck Imputation : This method replaces a missing value of an item with a constant value from an external source such as a value from a previous survey.    Single Imputation :", 
            "title": "Single Imputation Methods"
        }, 
        {
            "location": "/other-analysis/missing-data/#multiple-imputation", 
            "text": "Seealso  Visit  this website  for more information.   Bayesian Methods for Completing Data  are simply methods based on conditional probability.  Multiple Imputation is always superior to any of the single imputation methods because:   A single imputed value is never used  The variance estimates reflect the appropriate amount of uncertainty surrounding parameter estimates   There are several decisions to be made before performing a multiple imputation including  distribution ,  auxiliary variables  and  number of imputations  that can affect the quality of the imputation.   Imputation phase ( PROC MI ) :  the user specifies the imputation model to be used and the number of imputed datasets to be created  Analysis phase ( PROG GLM / PROC GENMOD ) : runs the analytic model of interest within each of the imputed datasets  Pooling phase ( PROC MIANALYZE ) : combines all the estimates across all the imputed datasets and outputs one set of parameter estimates for the model of interest", 
            "title": "Multiple imputation"
        }, 
        {
            "location": "/other-analysis/missing-data/#mvn-vs-fcs", 
            "text": "", 
            "title": "MVN vs FCS"
        }, 
        {
            "location": "/other-analysis/missing-data/#auxiliary-variables", 
            "text": "They can can help improve the likelihood of meeting the MAR assumption   They help yield more accurate and stable estimates and thus reduce the estimated standard errors in analytic models   Including them can also help to increase power", 
            "title": "Auxiliary variables"
        }, 
        {
            "location": "/other-analysis/missing-data/#number-of-imputations-m", 
            "text": "Estimates of coefficients stabilize at much lower values of  m  than estimates of variances and covariances of error terms   A larger number of imputations may also allow hypothesis tests with less restrictive assumptions (i.e., that do not assume equal fractions of missing information for all coefficients)  Multiple runs of m imputations are recommended to assess the stability of the parameter estimates  Recommendations:   For low fractions of missing information (and relatively simple analysis techniques) 5-20 imputations and 50 or more when the proportion of missing data is relatively high  The number of imputations should equal the percentage of incomplete cases ( m =max(FMI%)), this way the error associated with estimating the regression coefficients, standard errors and the resulting p-values is considerably reduced and results in an adequate level of reproducibility", 
            "title": "Number of imputations (m)"
        }, 
        {
            "location": "/other-analysis/missing-data/#more-comments", 
            "text": "You should include the dependent variable (DV) in the imputation model unless you would like to impute independent variables (IVs) assuming they are uncorrelated with your DV  Although MI can perform well up to 50% missing observations,  the larger the amount the higher the chance of finding estimation problems during the imputation process and the lower the chance of meeting the MAR assumption", 
            "title": "More comments"
        }, 
        {
            "location": "/other-analysis/missing-data/#model-based-procedures", 
            "text": "", 
            "title": "Model-based Procedures"
        }, 
        {
            "location": "/other-analysis/missing-data/#direct-maximum-likelihood", 
            "text": "This approach to analyzing missing data has many different forms. In its simplest form, it assumes that the observed data are a sample drawn from a multivariate normal distribution. The parameters are estimated by available data, and then missing scores are estimated based on the parameters just estimated. Contrary to the techniques discussed above, maximum likelihood procedures allow explicit modeling of missing data that is open to scientific analysis and critique.", 
            "title": "Direct Maximum Likelihood"
        }, 
        {
            "location": "/other-analysis/missing-data/#expectation-maximization", 
            "text": "This algorithm is an iterative process. The first iteration estimates missing data and then parameters using maximum likelihood. The second iteration re-estimates the missing data based on the new parameter estimates and then recalculates the new parameters estimates\nbased on actual and re-estimated missing data. The approach continues until there is convergence in the parameter estimates.", 
            "title": "Expectation Maximization"
        }, 
        {
            "location": "/other-analysis/missing-data/#summary", 
            "text": "The best technique to deal with your missing data depends on:   The amount of missing data (what percentage of data is missing)  Type of missing data (MAR, MCAR, NMAR)   According to  this nice review , if more than 10% data is missing, the best solution is:   Maximum likelihood imputation if data are NMAR (non-missing at random)  Maximum likelihood and hot-deck if data are MAR (missing at random)  Pairwise deletion, hot-deck or regression if data are MCAR (missing completely at random)   Moreover,  multiple imputation  by chained equations is regarded the best imputation method by many researchers.", 
            "title": "Summary"
        }, 
        {
            "location": "/other-analysis/interim-analysis/", 
            "text": "The purpose of the \nSEQDESIGN\n procedure is to design interim analyses for clinical trials. \n\n\nIn a \nfixed-sample trial\n, data about all individuals are first collected and then examined at the end of the study. In contrast, a \ngroup sequential trial\n provides for interim analyses before the completion of the trial while maintaining the specified overall Type I and Type II error probabilities.\n\n\nA group sequential trial is most useful in situations where it is important to monitor the trial to prevent unnecessary exposure of patients to an unsafe new drug, or alternatively to a placebo treatment if the new drug shows significant improvement. In most cases, if a group sequential trial \nstops early for safety concerns\n, fewer patients are exposed to the new treatment than in the fixed-sample trial. If a trial \nstops early for efficacy reasons\n, the new treatment is available sooner than it would be in a fixed-sample trial. Early stopping can also save time and resources.\n\n\nA \ngroup sequential design\n provides detailed specifications for a group sequential trial. In addition to the usual specification for a fixed-sample design, it provides the total number of stages (the number of interim stages plus a final stage) and a stopping criterion to reject, to accept, or to either reject or accept the null hypothesis at each interim stage. It also provides critical values and the sample size at each stage for the trial.\n\n\n\n\nPROC SEQDESIGN\n\n\nYou use the \nSEQDESIGN\n procedure compute the initial boundary values and required sample sizes for the trial. \n\n\nPROC SEQTEST\n\n\nYou use the \nSEQTEST\n procedure to compare the test statistic with its boundary values.\n\n\nhttp://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_seqdesign_overview01.htm\nhttps://support.sas.com/resources/papers/proceedings09/311-2009.pdf\nhttps://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_seqdesign_sect035.htm", 
            "title": "Interim Analysis Design"
        }, 
        {
            "location": "/other-analysis/interim-analysis/#proc-seqdesign", 
            "text": "You use the  SEQDESIGN  procedure compute the initial boundary values and required sample sizes for the trial.", 
            "title": "PROC SEQDESIGN"
        }, 
        {
            "location": "/other-analysis/interim-analysis/#proc-seqtest", 
            "text": "You use the  SEQTEST  procedure to compare the test statistic with its boundary values.  http://support.sas.com/documentation/cdl/en/statug/68162/HTML/default/viewer.htm#statug_seqdesign_overview01.htm\nhttps://support.sas.com/resources/papers/proceedings09/311-2009.pdf\nhttps://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_seqdesign_sect035.htm", 
            "title": "PROC SEQTEST"
        }, 
        {
            "location": "/other-analysis/normality-tests/", 
            "text": "Saphiro-Wilk normality test\n\n\nIn statistics, the \nShapiro-Wilk test\n is used to test the normality of a data set. It is considered one of the most powerful tests for normality contrast, especially for small samples ($n\n50$).\n\n\nMonte Carlo simulations have found that Shapiro\u2013Wilk has \nthe best power for a given significance\n, followed closely by Anderson\u2013Darling when comparing to the Kolmogorov\u2013Smirnov, Lilliefors, and Anderson\u2013Darling tests.\n\n\nThe null-hypothesis of this test is that the population is normally distributed. Thus,\n\n\n\n\nif the p-value is less than the chosen alpha level, then the null \nhypothesis is rejected\n and there is evidence that the data tested are \nnot from a normally distributed population\n\n\nif the p-value is greater than the chosen alpha level, then the null \nhypothesis cannot be rejected\n \n\n\n\n\nHowever, since the \ntest is biased by sample size\n, it may be statistically significant from a normal distribution in any \nlarge samples\n and a \nQ\u2013Q plot would be required for verification\n in addition to the test.\n\n\nPROC UNIVARIATE\n\n\nThe \nUNIVARIATE\n procedure provides a variety of descriptive statistics, and draws Q-Q, stem-and-leaf, normal probability, and box plots. This procedure also conducts Shapiro-Wilk, Kolmogorov-Smirnov, Anderson-Darling and Cramer-von Misers tests. \n\n\n\n\nNote\n\n\nThe Shapiro-Wilk \nW\n will be reported only if $N\n2000$. \n\n\n\n\n1\n2\n3\n4\n5\nPROC UNIVARIATE DATA=SAS-data-set NORMAL PLOT;\n  VAR variable(s);\n  QQPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  OUTPUT OUT=normality PROBN=probn;\nRUN;\n\n\n\n\n\n\n\n\nNORMAL\n performs normality tests\n\n\nPLOT\n draws a stem-and-leaf and a box plots\n\n\nQQPLOT\n draws a Q-Q plot\n\n\n\n\n\n\nNote\n\n\nYou must provide a \nVAR\n statement when you use an \nOUTPUT\n statement. To store the same statistic for several analysis variables in the \nOUT=\n data set, you specify a list of names in the \nOUTPUT\n statement. \nPROC UNIVARIATE\n makes a one-to-one correspondence between the order of the analysis variables in the \nVAR\n statement and the list of names that follow a statistic keyword.   \n\n\n\n\nPROC CAPABILITY\n\n\nLike \nUNIVARIATE\n, the \nCAPABILITY\n procedure also produces various descriptive statistics and plots. \nCAPABILITY\n can draw a P-P plot using the \nPPPLOT\n option but does not support stem-and-leaf, box, and normal probability plots (it does not have the \nPLOT\n option). \n\n\n1\n2\n3\n4\n5\n6\n7\nPROC CAPABILITY DATA=SAS-data-set NORMAL;\n  VAR variable(s);\n  QQPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  PPPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  HISTOGRAM /NORMAL(COLOR=MAROON W=4) CFILL = BLUE CFRAME = LIGR;\n  INSET MEAN STD /CFILL=BLANK FORMAT=5.2 ;\nRUN; \n\n\n\n\n\n\n\n\nNORMAL\n performs normality tests\n\n\nQQPLOT\n, \nPPPLOT\n and \nHISTOGRAM\n statements respectively draw a Q-Q plot, a P-P plot, and a histogram\n\n\nINSET\n statement adds summary statistics to graphs such as a histogram and a Q-Q plot", 
            "title": "Normality tests"
        }, 
        {
            "location": "/other-analysis/normality-tests/#saphiro-wilk-normality-test", 
            "text": "In statistics, the  Shapiro-Wilk test  is used to test the normality of a data set. It is considered one of the most powerful tests for normality contrast, especially for small samples ($n 50$).  Monte Carlo simulations have found that Shapiro\u2013Wilk has  the best power for a given significance , followed closely by Anderson\u2013Darling when comparing to the Kolmogorov\u2013Smirnov, Lilliefors, and Anderson\u2013Darling tests.  The null-hypothesis of this test is that the population is normally distributed. Thus,   if the p-value is less than the chosen alpha level, then the null  hypothesis is rejected  and there is evidence that the data tested are  not from a normally distributed population  if the p-value is greater than the chosen alpha level, then the null  hypothesis cannot be rejected     However, since the  test is biased by sample size , it may be statistically significant from a normal distribution in any  large samples  and a  Q\u2013Q plot would be required for verification  in addition to the test.", 
            "title": "Saphiro-Wilk normality test"
        }, 
        {
            "location": "/other-analysis/normality-tests/#proc-univariate", 
            "text": "The  UNIVARIATE  procedure provides a variety of descriptive statistics, and draws Q-Q, stem-and-leaf, normal probability, and box plots. This procedure also conducts Shapiro-Wilk, Kolmogorov-Smirnov, Anderson-Darling and Cramer-von Misers tests.    Note  The Shapiro-Wilk  W  will be reported only if $N 2000$.    1\n2\n3\n4\n5 PROC UNIVARIATE DATA=SAS-data-set NORMAL PLOT;\n  VAR variable(s);\n  QQPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  OUTPUT OUT=normality PROBN=probn;\nRUN;    NORMAL  performs normality tests  PLOT  draws a stem-and-leaf and a box plots  QQPLOT  draws a Q-Q plot    Note  You must provide a  VAR  statement when you use an  OUTPUT  statement. To store the same statistic for several analysis variables in the  OUT=  data set, you specify a list of names in the  OUTPUT  statement.  PROC UNIVARIATE  makes a one-to-one correspondence between the order of the analysis variables in the  VAR  statement and the list of names that follow a statistic keyword.", 
            "title": "PROC UNIVARIATE"
        }, 
        {
            "location": "/other-analysis/normality-tests/#proc-capability", 
            "text": "Like  UNIVARIATE , the  CAPABILITY  procedure also produces various descriptive statistics and plots.  CAPABILITY  can draw a P-P plot using the  PPPLOT  option but does not support stem-and-leaf, box, and normal probability plots (it does not have the  PLOT  option).   1\n2\n3\n4\n5\n6\n7 PROC CAPABILITY DATA=SAS-data-set NORMAL;\n  VAR variable(s);\n  QQPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  PPPLOT variable /NORMAL(MU=EST SIGMA=EST COLOR=RED L=1);\n  HISTOGRAM /NORMAL(COLOR=MAROON W=4) CFILL = BLUE CFRAME = LIGR;\n  INSET MEAN STD /CFILL=BLANK FORMAT=5.2 ;\nRUN;     NORMAL  performs normality tests  QQPLOT ,  PPPLOT  and  HISTOGRAM  statements respectively draw a Q-Q plot, a P-P plot, and a histogram  INSET  statement adds summary statistics to graphs such as a histogram and a Q-Q plot", 
            "title": "PROC CAPABILITY"
        }, 
        {
            "location": "/other-analysis/propensity-score/", 
            "text": "Propensity Score\n\n\nWhat is it?\n\n\nThe objective of randomization in statistics is to obtain groups that are comparable in terms of both observed and unobserved characteristics. When randomization is not possible, causal inference is complicated by the fact that a group that received a treatment or experienced an event maybe very different from another group that did not experience the event or receive the treatment. Thus, it is \nnot clear whether a difference in certain outcome of interest is due to the treatment or is the product of prior differences among groups\n. Propensity score methods were developed to facilitate the \ncreation of comparison groups that are similar in terms of the distribution of observed characteristics\n.\n\n\nThe first step involves estimating the likelihood (the propensity score) that a person would have received the treatment given certain characteristics. More formally, the propensity score is the \nconditional probability of assignment to a particular treatment given a vector of observed covariates\n. Two key assumptions of propensity scores are that \nboth the outcome of interest and the treatment assignment do not depend on unobservable characteristics\n.\n\n\nComputing the Propensity Score\n\n\nIn order to program the corresponding model in SAS, the response variable is the group/arm to which the patient belongs and the predictors are all the baseline variables which could affect the assignment to a certain group. While the model is fitted the propensity score value can be computed and kept in an output:\n\n\n1\n2\n3\n4\n5\nPROC LOGISTIC DATA=input-SAS-data-set;\n    CLASS sex site(param=ref);\n    MODEL arm = sex age weight site serologytests / FIRTH;\n    OUTPUT OUT=output-SAS-data-set PROB=ps-variable-customized-name;\nRUN;\n\n\n\n\n\n\nThis kind of analysis are commonly used in observacional studies on which the patient is not randomized to a certain group but it belongs to it due to a certain diagnostic. In order to correct the possible effect of unbalanced population groups, \nthe propensity score value can be included in the model\n as a way to isolate the effects due to the treatment from the baseline characteristics.\n\n\nPropensity Score Matching\n\n\nAfter estimating the propensity scores, they are used to group observations that are close to each other. One way of\naccomplishing this is to classify treated and untreated observations into subgroups and then separately compare the outcome\nfor each subgroup. This method is usually referred as \nsubclassification on the propensity scores\n (Rosenbaum and Rubin\n1984). The other way is to match one treated unit to one or more untreated controls, which is usually referred as \nmatching on\nthe propensity score\n (Rosenbaum and Rubin 1983).\n\n\nKey in the implementation of matching using propensity scores is to \ndecide what metric to use when evaluating the distance between scores\n (usually the absolute value or the Mahalanobis metric) and \nwhat type of algorithm to implement\n (local or global optimal).\n\n\nPair-matching Methods\n\n\nThe most common implementation is 1:1 (1 to 1) or pair-matching in which pairs of treated and untreated subjects are formed which allows to estimate for average treatment effect in the treated (ATT).\n\n\nMethods without Replacement\n\n\nWe match each untreated subject to at most one treated subject. Once an untreated subject has been matched to a treated subject, that untreated subject is no longer eligible for consideration as a match for other treated subjects.\n\n\n\n\nGlobal optimal matching: forms matched pairs so as to minimize the average within-pair difference in propensity scores\n\n\nLocal optimal, greedy or nearest available neighbor matching: selects a treated subject and then selects as a matched control subject the one whose propensity score is closest to that of the treated subject (if multiple untreated subjects are equally close to the treated subject, one of these untreated subjects is selected at random). In each iteration, the best (optimal) control is chosen, but this process does not guarantee that the total distance between propensity scores is minimized.\n\n\n\n\nFour different approaches:\n\n\n\n\nSequentially treated subjects from highest to lowest propensity score\n\n\nSequentially treated subjects from lowest to highest propensity score\n\n\nSequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on)\n\n\nTreated subjects in a random order\n\n\nLocal optimal, greedy or nearest available neighbour matching within specified caliper widths: we can match treated and untreated subjects only if the absolute difference in their propensity scores is within a prespecified maximal distance (the caliper distance, defined as a proportion of the standard deviation of the logit of the propensity score)\n\n\n\n\n\n\nNote\n\n\nAlthough the propensity score is the natural metric to use, when using caliper matching, a reduction in bias due to the use of different caliper widths has been described when matching on the logit of the propensity score.\nAlthough it is difficult to know beforehand the optimal choice of caliper width, some researchers (Rosenbuam \n Rubin, 1985; Austin, 2011) have recommended using a caliper width that is equal to 0.2 of the standard deviation of the logit of the propensity score, i.e., $0.2\\cdot\\sqrt\\left ( \\sigma^2_1+\\sigma^2_2 \\right )/2$.\n\n\n\n\n\n\nSequentially treated subjects from highest to lowest propensity score\n\n\nSequentially treated subjects from lowest to highest propensity score\n\n\nSequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on)\n\n\nTreated subjects in a random order\n\n\n\n\n\n\nNote\n\n\nOptimal matching and greedy nearest neighbor matching on the propensity score will result in all treated subjects being matched to an untreated subject (assuming that the number of untreated subjects is at least as large as the number of treated subjects). However, greedy nearest neighbor matching within specified caliper widths may not result in all treated subjects being matched to an untreated subject, because for some treated subjects, there may not be any untreated subjects who are unmatched and whose propensity score lies within the specified caliper distance of that of the treated subject. The objective of the caliper matching is to avoid bad matches.\n\n\n\n\nMethods with Replacement\n\n\nPermits the same untreated subject to be matched to multiple treated subjects (because untreated subjects are recycled or allowed to be included in multiple matched sets, the order in which the treated subjects are selected has no effect on the formation of matched pairs). Matching with replacement minimizes the propensity score distance between the matched units since each treated unit is matched to the closest control, even if the control has been selected before.\n\n\n\n\nNearest neighbor matching with replacement: matches each treated subject to the nearest untreated subject\n\n\nNearest neighbor matching within specified caliper widths with replacement: matches each treated subject to the nearest untreated subject (subject to possible caliper restrictions)\n\n\n\n\n1 to N Matching Methods\n\n\nThey include matching each treated unit to more than one control match. This can be done by creating N replicas of each treated unit and proceeding as described above. \n\n\nRadius Matching\n\n\nAll the control units within a certain distance are chosen (Dehejia and Wahba 1999).\n\n\nMahalanobis Metric Matching\n\n\nIn this type of matching, the definition of distance is changed. The similarity between the propensity score of treated and untreated units is evaluated using the multidimensional Mahalanobis metric matching:\n\n\n$D_{ij}=\\sqrt{\\left ( x_i-y_j \\right )^TS^{-1}\\left ( x_i-y_j \\right )}$\n\n\nwhere $S^{-1}$ is the pooled variance-covariance matrix and x and y are multivariate vectors. Note that if the variance-covariance\nmatrix is an identity matrix the Mahalanobis metric is reduced to the familiar Euclidean metric. Usually the Mahalanobis metric\nmatching includes the propensity score and other covariates that are considered to be important and are hoped to be balanced\n(Rosenbaum and Rubin 1985).\n\n\nPSMatching Macro\n\n\nWith the macro \nPSMatching.sas\n (Coca-Perraillon, 2006) different methods can be applied to calculate the matching once you have the propensity score. First you have to prepare the following input:\n\n\n\n\nTreatment\n data set containing the treatment cases along with the patient number \nidT\n and the corresponding propensity score \npscoreT\n\n\nControl\n data set containing the control cases along with the patient number \nidC\n and the corresponding propensity score \npscoreC\n\n\n\n\nThe parameters \ndatatreatment\n and \ndatacontrol\n refer to the Treatment and Control datasets and they do not need to be sorted.\nThe method parameter can be \nNN\n (nearest available neighbor), \ncaliper\n or \nradius\n. Caliper can be any number indicating the\nsize of the \ncaliper\n and the parameter \nreplacement\n takes the values of yes or no. All the parameters are case insensitive. If the\nmethod is \nNN\n, the caliper parameter is ignored.\n\n\n1\n%PSMatching(datatreatment=treatment, datacontrol=control, method=caliper, numberofcontrols=1, caliper=0.2, replacement=no);\n\n\n\n\n\n\n\nHere is the macro code:\n\n\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100\n%macro\n \nPSMatching\n(\ndatatreatment\n=\n,\n \ndatacontrol\n=\n,\n \nmethod\n=\n,\n \nnumberofcontrols\n=\n,\n \ncaliper\n=\n,\nreplacement\n=\n);\n\n\n\n/* Create copies of the treated units if N \n 1 */\n;\n\n\ndata\n \n_Treatment0\n(\ndrop\n=\n \ni\n);\n\n\nset\n \nTreatment\n;\n\n\ndo\n \ni\n=\n \n1\n \nto\n \nnumberofcontrols\n;\n\n\nRandomNumber\n=\n \nranuni\n(\n12345\n);\n\n\noutput\n;\n\n\nend\n;\n\n\nrun\n;\n\n\n\n/* Randomly sort both datasets */\n\n\nproc\n \nsort\n \ndata\n=\n \n_Treatment0\n \nout\n=\n \n_Treatment\n(\ndrop\n=\n \nRandomNumber\n);\n\n\nby\n \nRandomNumber\n;\n\n\nrun\n;\n\n\ndata\n \n_Control0\n;\n\n\nset\n \nControl\n;\n\n\nRandomNumber\n=\n \nranuni\n(\n45678\n);\n\n\nrun\n;\n\n\nproc\n \nsort\n \ndata\n=\n \n_Control0\n \nout\n=\n \n_Control\n(\ndrop\n=\n \nRandomNumber\n);\n\n\nby\n \nRandomNumber\n;\n\n\nrun\n;\n\n\ndata\n \nMatched\n(\nkeep\n \n=\n \nIdSelectedControl\n \nMatchedToTreatID\n);\n\n\nlength\n \npscoreC\n \n8\n;\n\n\nlength\n \nidC\n \n8\n;\n\n\n\n/* Load Control dataset into the hash object */\n\n\nif\n \n_N_\n=\n \n1\n \nthen\n \ndo\n;\n\n\ndeclare\n \nhash\n \nh\n(\ndataset\n:\n \n_Control\n,\n \nordered\n:\n \nno\n);\n\n\ndeclare\n \nhiter\n \niter\n(\nh\n);\n\n\nh\n.\ndefineKey\n(\nidC\n);\n\n\nh\n.\ndefineData\n(\npscoreC\n,\n \nidC\n);\n\n\nh\n.\ndefineDone\n();\n\n\ncall\n \nmissing\n(\nidC\n,\n \npscoreC\n);\n\n\nend\n;\n\n\n\n/* Open the treatment */\n\n\nset\n \n_Treatment\n;\n\n\n%if\n \n%upcase\n(\nmethod\n)\n \n~=\n \nRADIUS\n \n%then\n \n%do\n;\n\n\nretain\n \nBestDistance\n \n99\n;\n\n\n%end\n;\n\n\n\n/* Iterate over the hash */\n\n\nrc\n=\n \niter\n.\nfirst\n();\n\n\nif\n \n(\nrc\n=\n0\n)\n \nthen\n \nBestDistance\n=\n \n99\n;\n\n\ndo\n \nwhile\n \n(\nrc\n \n=\n \n0\n);\n\n\n\n/* Caliper */\n\n\n%if\n \n%upcase\n(\nmethod\n)\n \n=\n \nCALIPER\n \n%then\n \n%do\n;\n\n\nif\n \n(\npscoreT\n \n-\n \ncaliper\n)\n \n=\n \npscoreC\n \n=\n \n(\npscoreT\n \n+\n \ncaliper\n)\n \nthen\n \ndo\n;\n\n\nScoreDistance\n \n=\n \nabs\n(\npscoreT\n \n-\n \npscoreC\n);\n\n\nif\n \nScoreDistance\n \n \nBestDistance\n \nthen\n \ndo\n;\n\n\nBestDistance\n \n=\n \nScoreDistance\n;\n\n\nIdSelectedControl\n \n=\n \nidC\n;\n\n\nMatchedToTreatID\n \n=\n \nidT\n;\n\n\nend\n;\n\n\nend\n;\n\n\n%end\n;\n\n\n\n/* NN */\n\n\n%if\n \n%upcase\n(\nmethod\n)\n \n=\n \nNN\n \n%then\n \n%do\n;\n\n\nScoreDistance\n \n=\n \nabs\n(\npscoreT\n \n-\n \npscoreC\n);\n\n\nif\n \nScoreDistance\n \n \nBestDistance\n \nthen\n \ndo\n;\n\n\nBestDistance\n \n=\n \nScoreDistance\n;\n\n\nIdSelectedControl\n \n=\n \nidC\n;\n\n\nMatchedToTreatID\n \n=\n \nidT\n;\n\n\nend\n;\n\n\n%end\n;\n\n\n%if\n \n%upcase\n(\nmethod\n)\n \n=\n \nNN\n \nor\n \n%upcase\n(\nmethod\n)\n \n=\n \nCALIPER\n \n%then\n \n%do\n;\n\n\nrc\n \n=\n \niter\n.\nnext\n();\n\n\n\n/* Output the best control and remove it */\n\n\nif\n \n(\nrc\n \n~=\n \n0\n)\n \nand\n \nBestDistance\n \n~=\n99\n \nthen\n \ndo\n;\n\n\noutput\n;\n\n\n%if\n \n%upcase\n(\nreplacement\n)\n \n=\n \nNO\n \n%then\n \n%do\n;\n\n\nrc1\n \n=\n \nh\n.\nremove\n(\nkey\n:\n \nIdSelectedControl\n);\n\n\n%end\n;\n\n\nend\n;\n\n\n%end\n;\n\n\n\n/* Radius */\n\n\n%if\n \n%upcase\n(\nmethod\n)\n \n=\n \nRADIUS\n \n%then\n \n%do\n;\n\n\nif\n \n(\npscoreT\n \n-\n \ncaliper\n)\n \n=\n \npscoreC\n \n=\n \n(\npscoreT\n \n+\n \ncaliper\n)\n \nthen\n \ndo\n;\n\n\nIdSelectedControl\n \n=\n \nidC\n;\n\n\nMatchedToTreatID\n \n=\n \nidT\n;\n\n\noutput\n;\n\n\nend\n;\n\n\nrc\n \n=\n \niter\n.\nnext\n();\n\n\n%end\n;\n\n\nend\n;\n\n\nrun\n;\n\n\n\n/* Delete temporary tables. Quote for debugging */\n\n\nods\n \nselect\n \nnone\n;\n\n\nproc\n \ndatasets\n;\n\n\ndelete\n \n_\n:(\ngennum\n=\nall\n);\n\n\nrun\n;\n\n\nods\n \nselect\n \nall\n;\n\n\n\n%mend\n \nPSMatching\n;\n\n\n\n\n\n\n\nPROC PSMATCH\n\n\nHere\n and \nhere\n you can find the documentation of this procedure. \n\n\nStratification on the Propensity Score\n\n\nStratification on the propensity score involves \nstratifying subjects into mutually exclusive subsets based on their estimated propensity score\n. Subjects are ranked according to their estimated propensity score. Subjects are then stratified into subsets based on previously defined thresholds of the estimated propensity score. A common approach is to \ndivide subjects into five equal-size groups using the quintiles of the estimated propensity score\n. Rosenbaum and Rubin (1984) stated that stratifying on the quintiles of the propensity score eliminates approximately 90% of the bias due to measured confounders when estimating a linear treatment effect. Within each propensity score stratum, treated and untreated subjects will have roughly similar values of the propensity score. Therefore, when the propensity score has been correctly specified, the distribution of measured baseline covariates will be approximately similar between treated and untreated subjects within the same stratum.\n\n\nMore info on this topic \nhere\n.", 
            "title": "Propensity Score"
        }, 
        {
            "location": "/other-analysis/propensity-score/#propensity-score", 
            "text": "", 
            "title": "Propensity Score"
        }, 
        {
            "location": "/other-analysis/propensity-score/#what-is-it", 
            "text": "The objective of randomization in statistics is to obtain groups that are comparable in terms of both observed and unobserved characteristics. When randomization is not possible, causal inference is complicated by the fact that a group that received a treatment or experienced an event maybe very different from another group that did not experience the event or receive the treatment. Thus, it is  not clear whether a difference in certain outcome of interest is due to the treatment or is the product of prior differences among groups . Propensity score methods were developed to facilitate the  creation of comparison groups that are similar in terms of the distribution of observed characteristics .  The first step involves estimating the likelihood (the propensity score) that a person would have received the treatment given certain characteristics. More formally, the propensity score is the  conditional probability of assignment to a particular treatment given a vector of observed covariates . Two key assumptions of propensity scores are that  both the outcome of interest and the treatment assignment do not depend on unobservable characteristics .", 
            "title": "What is it?"
        }, 
        {
            "location": "/other-analysis/propensity-score/#computing-the-propensity-score", 
            "text": "In order to program the corresponding model in SAS, the response variable is the group/arm to which the patient belongs and the predictors are all the baseline variables which could affect the assignment to a certain group. While the model is fitted the propensity score value can be computed and kept in an output:  1\n2\n3\n4\n5 PROC LOGISTIC DATA=input-SAS-data-set;\n    CLASS sex site(param=ref);\n    MODEL arm = sex age weight site serologytests / FIRTH;\n    OUTPUT OUT=output-SAS-data-set PROB=ps-variable-customized-name;\nRUN;   This kind of analysis are commonly used in observacional studies on which the patient is not randomized to a certain group but it belongs to it due to a certain diagnostic. In order to correct the possible effect of unbalanced population groups,  the propensity score value can be included in the model  as a way to isolate the effects due to the treatment from the baseline characteristics.", 
            "title": "Computing the Propensity Score"
        }, 
        {
            "location": "/other-analysis/propensity-score/#propensity-score-matching", 
            "text": "After estimating the propensity scores, they are used to group observations that are close to each other. One way of\naccomplishing this is to classify treated and untreated observations into subgroups and then separately compare the outcome\nfor each subgroup. This method is usually referred as  subclassification on the propensity scores  (Rosenbaum and Rubin\n1984). The other way is to match one treated unit to one or more untreated controls, which is usually referred as  matching on\nthe propensity score  (Rosenbaum and Rubin 1983).  Key in the implementation of matching using propensity scores is to  decide what metric to use when evaluating the distance between scores  (usually the absolute value or the Mahalanobis metric) and  what type of algorithm to implement  (local or global optimal).", 
            "title": "Propensity Score Matching"
        }, 
        {
            "location": "/other-analysis/propensity-score/#pair-matching-methods", 
            "text": "The most common implementation is 1:1 (1 to 1) or pair-matching in which pairs of treated and untreated subjects are formed which allows to estimate for average treatment effect in the treated (ATT).", 
            "title": "Pair-matching Methods"
        }, 
        {
            "location": "/other-analysis/propensity-score/#methods-without-replacement", 
            "text": "We match each untreated subject to at most one treated subject. Once an untreated subject has been matched to a treated subject, that untreated subject is no longer eligible for consideration as a match for other treated subjects.   Global optimal matching: forms matched pairs so as to minimize the average within-pair difference in propensity scores  Local optimal, greedy or nearest available neighbor matching: selects a treated subject and then selects as a matched control subject the one whose propensity score is closest to that of the treated subject (if multiple untreated subjects are equally close to the treated subject, one of these untreated subjects is selected at random). In each iteration, the best (optimal) control is chosen, but this process does not guarantee that the total distance between propensity scores is minimized.   Four different approaches:   Sequentially treated subjects from highest to lowest propensity score  Sequentially treated subjects from lowest to highest propensity score  Sequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on)  Treated subjects in a random order  Local optimal, greedy or nearest available neighbour matching within specified caliper widths: we can match treated and untreated subjects only if the absolute difference in their propensity scores is within a prespecified maximal distance (the caliper distance, defined as a proportion of the standard deviation of the logit of the propensity score)    Note  Although the propensity score is the natural metric to use, when using caliper matching, a reduction in bias due to the use of different caliper widths has been described when matching on the logit of the propensity score.\nAlthough it is difficult to know beforehand the optimal choice of caliper width, some researchers (Rosenbuam   Rubin, 1985; Austin, 2011) have recommended using a caliper width that is equal to 0.2 of the standard deviation of the logit of the propensity score, i.e., $0.2\\cdot\\sqrt\\left ( \\sigma^2_1+\\sigma^2_2 \\right )/2$.    Sequentially treated subjects from highest to lowest propensity score  Sequentially treated subjects from lowest to highest propensity score  Sequentially treated subjects in the order of the best possible match (the first selected treated subject is that treated subject who is closest to an untreated subject and so on)  Treated subjects in a random order    Note  Optimal matching and greedy nearest neighbor matching on the propensity score will result in all treated subjects being matched to an untreated subject (assuming that the number of untreated subjects is at least as large as the number of treated subjects). However, greedy nearest neighbor matching within specified caliper widths may not result in all treated subjects being matched to an untreated subject, because for some treated subjects, there may not be any untreated subjects who are unmatched and whose propensity score lies within the specified caliper distance of that of the treated subject. The objective of the caliper matching is to avoid bad matches.", 
            "title": "Methods without Replacement"
        }, 
        {
            "location": "/other-analysis/propensity-score/#methods-with-replacement", 
            "text": "Permits the same untreated subject to be matched to multiple treated subjects (because untreated subjects are recycled or allowed to be included in multiple matched sets, the order in which the treated subjects are selected has no effect on the formation of matched pairs). Matching with replacement minimizes the propensity score distance between the matched units since each treated unit is matched to the closest control, even if the control has been selected before.   Nearest neighbor matching with replacement: matches each treated subject to the nearest untreated subject  Nearest neighbor matching within specified caliper widths with replacement: matches each treated subject to the nearest untreated subject (subject to possible caliper restrictions)", 
            "title": "Methods with Replacement"
        }, 
        {
            "location": "/other-analysis/propensity-score/#1-to-n-matching-methods", 
            "text": "They include matching each treated unit to more than one control match. This can be done by creating N replicas of each treated unit and proceeding as described above.", 
            "title": "1 to N Matching Methods"
        }, 
        {
            "location": "/other-analysis/propensity-score/#radius-matching", 
            "text": "All the control units within a certain distance are chosen (Dehejia and Wahba 1999).", 
            "title": "Radius Matching"
        }, 
        {
            "location": "/other-analysis/propensity-score/#mahalanobis-metric-matching", 
            "text": "In this type of matching, the definition of distance is changed. The similarity between the propensity score of treated and untreated units is evaluated using the multidimensional Mahalanobis metric matching:  $D_{ij}=\\sqrt{\\left ( x_i-y_j \\right )^TS^{-1}\\left ( x_i-y_j \\right )}$  where $S^{-1}$ is the pooled variance-covariance matrix and x and y are multivariate vectors. Note that if the variance-covariance\nmatrix is an identity matrix the Mahalanobis metric is reduced to the familiar Euclidean metric. Usually the Mahalanobis metric\nmatching includes the propensity score and other covariates that are considered to be important and are hoped to be balanced\n(Rosenbaum and Rubin 1985).", 
            "title": "Mahalanobis Metric Matching"
        }, 
        {
            "location": "/other-analysis/propensity-score/#psmatching-macro", 
            "text": "With the macro  PSMatching.sas  (Coca-Perraillon, 2006) different methods can be applied to calculate the matching once you have the propensity score. First you have to prepare the following input:   Treatment  data set containing the treatment cases along with the patient number  idT  and the corresponding propensity score  pscoreT  Control  data set containing the control cases along with the patient number  idC  and the corresponding propensity score  pscoreC   The parameters  datatreatment  and  datacontrol  refer to the Treatment and Control datasets and they do not need to be sorted.\nThe method parameter can be  NN  (nearest available neighbor),  caliper  or  radius . Caliper can be any number indicating the\nsize of the  caliper  and the parameter  replacement  takes the values of yes or no. All the parameters are case insensitive. If the\nmethod is  NN , the caliper parameter is ignored.  1 %PSMatching(datatreatment=treatment, datacontrol=control, method=caliper, numberofcontrols=1, caliper=0.2, replacement=no);    Here is the macro code:    1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n 11\n 12\n 13\n 14\n 15\n 16\n 17\n 18\n 19\n 20\n 21\n 22\n 23\n 24\n 25\n 26\n 27\n 28\n 29\n 30\n 31\n 32\n 33\n 34\n 35\n 36\n 37\n 38\n 39\n 40\n 41\n 42\n 43\n 44\n 45\n 46\n 47\n 48\n 49\n 50\n 51\n 52\n 53\n 54\n 55\n 56\n 57\n 58\n 59\n 60\n 61\n 62\n 63\n 64\n 65\n 66\n 67\n 68\n 69\n 70\n 71\n 72\n 73\n 74\n 75\n 76\n 77\n 78\n 79\n 80\n 81\n 82\n 83\n 84\n 85\n 86\n 87\n 88\n 89\n 90\n 91\n 92\n 93\n 94\n 95\n 96\n 97\n 98\n 99\n100 %macro   PSMatching ( datatreatment = ,   datacontrol = ,   method = ,   numberofcontrols = ,   caliper = , replacement = );  /* Create copies of the treated units if N   1 */ ;  data   _Treatment0 ( drop =   i );  set   Treatment ;  do   i =   1   to   numberofcontrols ;  RandomNumber =   ranuni ( 12345 );  output ;  end ;  run ;  /* Randomly sort both datasets */  proc   sort   data =   _Treatment0   out =   _Treatment ( drop =   RandomNumber );  by   RandomNumber ;  run ;  data   _Control0 ;  set   Control ;  RandomNumber =   ranuni ( 45678 );  run ;  proc   sort   data =   _Control0   out =   _Control ( drop =   RandomNumber );  by   RandomNumber ;  run ;  data   Matched ( keep   =   IdSelectedControl   MatchedToTreatID );  length   pscoreC   8 ;  length   idC   8 ;  /* Load Control dataset into the hash object */  if   _N_ =   1   then   do ;  declare   hash   h ( dataset :   _Control ,   ordered :   no );  declare   hiter   iter ( h );  h . defineKey ( idC );  h . defineData ( pscoreC ,   idC );  h . defineDone ();  call   missing ( idC ,   pscoreC );  end ;  /* Open the treatment */  set   _Treatment ;  %if   %upcase ( method )   ~=   RADIUS   %then   %do ;  retain   BestDistance   99 ;  %end ;  /* Iterate over the hash */  rc =   iter . first ();  if   ( rc = 0 )   then   BestDistance =   99 ;  do   while   ( rc   =   0 );  /* Caliper */  %if   %upcase ( method )   =   CALIPER   %then   %do ;  if   ( pscoreT   -   caliper )   =   pscoreC   =   ( pscoreT   +   caliper )   then   do ;  ScoreDistance   =   abs ( pscoreT   -   pscoreC );  if   ScoreDistance     BestDistance   then   do ;  BestDistance   =   ScoreDistance ;  IdSelectedControl   =   idC ;  MatchedToTreatID   =   idT ;  end ;  end ;  %end ;  /* NN */  %if   %upcase ( method )   =   NN   %then   %do ;  ScoreDistance   =   abs ( pscoreT   -   pscoreC );  if   ScoreDistance     BestDistance   then   do ;  BestDistance   =   ScoreDistance ;  IdSelectedControl   =   idC ;  MatchedToTreatID   =   idT ;  end ;  %end ;  %if   %upcase ( method )   =   NN   or   %upcase ( method )   =   CALIPER   %then   %do ;  rc   =   iter . next ();  /* Output the best control and remove it */  if   ( rc   ~=   0 )   and   BestDistance   ~= 99   then   do ;  output ;  %if   %upcase ( replacement )   =   NO   %then   %do ;  rc1   =   h . remove ( key :   IdSelectedControl );  %end ;  end ;  %end ;  /* Radius */  %if   %upcase ( method )   =   RADIUS   %then   %do ;  if   ( pscoreT   -   caliper )   =   pscoreC   =   ( pscoreT   +   caliper )   then   do ;  IdSelectedControl   =   idC ;  MatchedToTreatID   =   idT ;  output ;  end ;  rc   =   iter . next ();  %end ;  end ;  run ;  /* Delete temporary tables. Quote for debugging */  ods   select   none ;  proc   datasets ;  delete   _ :( gennum = all );  run ;  ods   select   all ;  %mend   PSMatching ;", 
            "title": "PSMatching Macro"
        }, 
        {
            "location": "/other-analysis/propensity-score/#proc-psmatch", 
            "text": "Here  and  here  you can find the documentation of this procedure.", 
            "title": "PROC PSMATCH"
        }, 
        {
            "location": "/other-analysis/propensity-score/#stratification-on-the-propensity-score", 
            "text": "Stratification on the propensity score involves  stratifying subjects into mutually exclusive subsets based on their estimated propensity score . Subjects are ranked according to their estimated propensity score. Subjects are then stratified into subsets based on previously defined thresholds of the estimated propensity score. A common approach is to  divide subjects into five equal-size groups using the quintiles of the estimated propensity score . Rosenbaum and Rubin (1984) stated that stratifying on the quintiles of the propensity score eliminates approximately 90% of the bias due to measured confounders when estimating a linear treatment effect. Within each propensity score stratum, treated and untreated subjects will have roughly similar values of the propensity score. Therefore, when the propensity score has been correctly specified, the distribution of measured baseline covariates will be approximately similar between treated and untreated subjects within the same stratum.  More info on this topic  here .", 
            "title": "Stratification on the Propensity Score"
        }, 
        {
            "location": "/other-analysis/roc-curve/", 
            "text": "In statistics, a \nreceiver operating characteristic curve (ROC curve)\n, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the \ntrue positive rate (TPR) against the false positive rate (FPR)\n at various threshold settings. The true-positive rate is also known as \nsensitivity\n. The false-positive rate is also known as the fall-out and can be calculated as \n(1 \u2212 specificity)\n.\n\n\nROC analysis provides tools to \nselect possibly optimal models and to discard suboptimal ones\n independently from (and prior to specifying) the cost context or the class distribution.\n\n\nMacro \nROCPLOT\n\n\nProduce a plot of the \nReceiver Operating Characteristic (ROC)\n curve associated with a fitted binary-response model. Label points on the ROC curve using statistic or input variable values. Identify optimal cutpoints on the ROC curve using several optimality criteria such as correct classification, efficiency, cost, and others. Plot optimality criteria against a selected variable.\n\n\nWhen only an ROC plot with labeled points is needed, you can often produce the desired plot in \nPROC LOGISTIC\n without this macro. Using  \nODS graphics\n, \nPROC LOGISTIC\n can plot the ROC curve of a model whether applied to the data used to fit the model or to additional data scored using the fitted model. Specify the \nPLOTS=ROC\n option in the \nPROC LOGISTIC\n statement, or specify the \nOUTROC=\n option in the \nMODEL\n and/or \nSCORE\n statements.\n\n\n1\n2\n3\nPROC LOGISTIC (\u2026) plots(only)=roc(id=obs);\n    MODEL (\u2026) / OUTROC=ROC_data;\nRUN;\n\n\n\n\n\n\nMore information on this macro \nhere\n.", 
            "title": "ROC curve"
        }, 
        {
            "location": "/other-analysis/roc-curve/#macro-rocplot", 
            "text": "Produce a plot of the  Receiver Operating Characteristic (ROC)  curve associated with a fitted binary-response model. Label points on the ROC curve using statistic or input variable values. Identify optimal cutpoints on the ROC curve using several optimality criteria such as correct classification, efficiency, cost, and others. Plot optimality criteria against a selected variable.  When only an ROC plot with labeled points is needed, you can often produce the desired plot in  PROC LOGISTIC  without this macro. Using   ODS graphics ,  PROC LOGISTIC  can plot the ROC curve of a model whether applied to the data used to fit the model or to additional data scored using the fitted model. Specify the  PLOTS=ROC  option in the  PROC LOGISTIC  statement, or specify the  OUTROC=  option in the  MODEL  and/or  SCORE  statements.  1\n2\n3 PROC LOGISTIC (\u2026) plots(only)=roc(id=obs);\n    MODEL (\u2026) / OUTROC=ROC_data;\nRUN;   More information on this macro  here .", 
            "title": "Macro ROCPLOT"
        }, 
        {
            "location": "/other-analysis/survival-analysis/", 
            "text": "Survival analysis is a branch of statistics for analyzing the expected duration of \ntime until one or more events happen\n. Survival analysis attempts to answer questions such as: what is the proportion of a population which will survive past a certain time? Of those that survive, at what rate will they die or fail? Can multiple causes of death or failure be taken into account? How do particular circumstances or characteristics increase or decrease the probability of survival? To answer such questions, it is necessary to define \nlifetime\n with \nPROC LIFETEST\n.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\noms = (lastcontact - starttreatment + 1) / 30.45\nif exdate = . then censor = 1;\nelse censor = 0;\n\nPROC LIFETEST DATA=SAS-data-set plots=(s);\n  TIME osm * censor(1);   \n    STRATA alg              /* aleatorization group, (= . they didn\nt get to randomization) */\nRUN;\n\n\n\n\n\n\n\n\nIn the \nTIME\n statement, only patients that haven't been censored are analysed \n\n\nThe \nSTRATA\n statement includes only non-missing data points (no \nWHERE\n filtering is needed) \n\n\n\n\nP-value of a Lifetest Analysis\n\n\nWe select only 2 groups from the test data set (High and Low risk):\n\n\n1\n2\n3\n4\nDATA bmt_small;\n    SET SASHELP.bmt;\n    WHERE group IN (2, 3);\nRUN;\n\n\n\n\n\n\nWe generate the \nOUTSURV\n data set:\n\n\n1\n2\n3\n4\nPROC LIFETEST DATA=bmt_small PLOTS=SURVIVAL(CL CB=HW STRATA=PANEL) METHOD=LT INTERVALS=(0 to 800 by 100) OUTSURV=bmt_param STDERR;\n   TIME t * status(0);\n   STRATA group / ORDER=INTERNAL; \nrun;\n\n\n\n\n\n\nWe calculate the p-values from this data:\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC SQL;\n    SELECT t, range(survival) AS RangeSurvival, sqrt(sum(sdf_stderr**2)) AS Squares, range(survival)/sqrt(sum(sdf_stderr**2)) AS z,\n           probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2)))) AS pz, 2 * (1-probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2))))) AS pvalue\n    FROM btm_param \n    WHERE t \n 0\n    GROUP BY t;\nQUIT;\n\n\n\n\n\n\nOther method:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nDATA bmt600 ;\n  brazo=\nA\n; valor=1; contador=32; OUTPUT;\n  brazo=\nA\n; valor=2; contador=13; OUTPUT;\n  brazo=\nB\n; valor=1; contador=18; OUTPUT;\n  brazo=\nB\n; valor=2; contador=36; OUTPUT;\nRUN;\n\nPROC FREQ DATA=bmt600;\n    TABLES brazo*valor / CHISQ MEASURES RISKDIFF PLOTS=(FREQPLOT(TWOWAY=GROUPVERTICAL SCALE=PERCENT));\n    WEIGHT contador;\nRUN;\n\n\n\n\n\n\nInformative censoring\n\n\n\n\nCheck these papers\n\n\n\n\nCensoring in survival analysis: Potential for bias\n\n\nImpact of Informative Censoring on the Kaplan-Meier Estimate of Progression-Free Survival in Phase II Clinical Trials", 
            "title": "Survival Analysis"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#p-value-of-a-lifetest-analysis", 
            "text": "We select only 2 groups from the test data set (High and Low risk):  1\n2\n3\n4 DATA bmt_small;\n    SET SASHELP.bmt;\n    WHERE group IN (2, 3);\nRUN;   We generate the  OUTSURV  data set:  1\n2\n3\n4 PROC LIFETEST DATA=bmt_small PLOTS=SURVIVAL(CL CB=HW STRATA=PANEL) METHOD=LT INTERVALS=(0 to 800 by 100) OUTSURV=bmt_param STDERR;\n   TIME t * status(0);\n   STRATA group / ORDER=INTERNAL; \nrun;   We calculate the p-values from this data:  1\n2\n3\n4\n5\n6\n7 PROC SQL;\n    SELECT t, range(survival) AS RangeSurvival, sqrt(sum(sdf_stderr**2)) AS Squares, range(survival)/sqrt(sum(sdf_stderr**2)) AS z,\n           probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2)))) AS pz, 2 * (1-probnorm(abs(range(survival)/sqrt(sum(sdf_stderr**2))))) AS pvalue\n    FROM btm_param \n    WHERE t   0\n    GROUP BY t;\nQUIT;   Other method:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 DATA bmt600 ;\n  brazo= A ; valor=1; contador=32; OUTPUT;\n  brazo= A ; valor=2; contador=13; OUTPUT;\n  brazo= B ; valor=1; contador=18; OUTPUT;\n  brazo= B ; valor=2; contador=36; OUTPUT;\nRUN;\n\nPROC FREQ DATA=bmt600;\n    TABLES brazo*valor / CHISQ MEASURES RISKDIFF PLOTS=(FREQPLOT(TWOWAY=GROUPVERTICAL SCALE=PERCENT));\n    WEIGHT contador;\nRUN;", 
            "title": "P-value of a Lifetest Analysis"
        }, 
        {
            "location": "/other-analysis/survival-analysis/#informative-censoring", 
            "text": "Check these papers   Censoring in survival analysis: Potential for bias  Impact of Informative Censoring on the Kaplan-Meier Estimate of Progression-Free Survival in Phase II Clinical Trials", 
            "title": "Informative censoring"
        }, 
        {
            "location": "/other-analysis/clustering/", 
            "text": "Introduction to Cluster Analysis\n\n\nWhen subjects are sampled, randomized or allocated by clusters, several statistical problems arise. If observations within a cluster are correlated, one of the assumptions of estimation and hypothesis testing is violated. Because of this correlation, the analyses must be modified to take into account the cluster design effect. When cluster designs are used, there are two sources of variations in the observations. The first is the \none between subjects within a cluster\n, and the second is the \nvariability among clusters\n. These two sources of variation cause the variance to inflate and must be taken into account in the analysis.\n\n\nGettin' Ready for a Cluster Analysis\n\n\nCheck for Missing Data\n\n\nVariables with missing data should be excluded from the calculation unless they can be imputed.\n\n\n1\n2\n3\n4\nDATA SAS-data-set-without-missing;\n    SET SAS-data-set-with-missing;\n    IF CMISS(OF _ALL_) THEN DELETE;\nRUN;\n\n\n\n\n\n\nDealing with Categorical Variables\n\n\nComposite variables\n\n\nOther questionnaire data like binary (yes/no questions) or a spectrum of responses can be transformed into \ncomposite variables\n to capture multiple questions into a \nranked ordinal scale\n. A \ncomposite variable\n is a variable created by combining two or more individual variables, called indicators, into a single variable. Each indicator alone doesn't provide sufficient information, but altogether they can represent the more complex concept.\n\n\nA lot of work goes into creating composite variables. The indicators of the multidimensional concept must be specified. It's important that each indicator contribute unique information to the final score. The formula for combining the indicators into a single score, called aggregating data, must be established. The computation involved will depend on the type of data that is being aggregated. To aggregate the data, raw scores might be summed, averaged, transformed, and/or weighted.\n\n\nHot encoding\n\n\nCheck \nthis website\n for a macro to generate dummy variables.\n\n\nOrdinal Categorical Variables\n\n\nIf your categorical variables have an ordinal meaning you can create an auxiliary numeric variable with indexes representing the ordinal scale and use this new variable, which can be standardized, for the analysis.\n\n\nMethods for data reduction\n\n\nYou may need to reduce the number of variables to include in the analysis. There are several methods for this:\n\n\n\n\nPrincipal Component Analysis with \nPROC FACTOR\n\n\nVariable Reduction for Modeling using \nPROC VARCLUS\n\n\n\n\nStandardize your Data\n\n\nWhen performing multivariate analysis, having variables that are measured at different scales can influence the numerical stability and precision of the estimators. Standardizing the data prior to performing statistical analysis can often prevent this problem.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nDATA SAS-data-set;\n    SET SAS-data-set-unformatted;\n    format variable1 variable2 variable3 10.4;\nRUN;\n\nPROC STANDARD DATA=SAS-data-set out=SAS-output-data-set MEAN=0 STD=1;\n    VAR variable1 variable2 variable3;\nRUN;\n\n\n\n\n\n\n\n\nWarning\n\n\nDo not forget to change the format of your numerical data and increase the number of decimal places before performing the standardization. Otherwise you may lose a lot of details on this process that can be crucial for the data analysis.\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nStandardization Procedures\n\n\nStandardization of Variables in Cluster Analysis\n\n\n\n\n\n\nSAS Procedures to Perform Cluster Analysis\n\n\nWard's minimum-variance hierarchical clustering method using agglomerative (bottom-up) approach and Ward's linkage.\n\n\n\n\nCheck these websites\n\n\n\n\nIntroduction to Clustering Procedures\n\n\n\n\n\n\nPROC CLUSTER\n: Hierarchical Cluster Analysis\n\n\nThe \nCLUSTER\n procedure \nhierarchically clusters the observations\n in a SAS data set by using one of 11 methods. The data can be coordinates or distances. \n\n\nAll methods are based on the usual agglomerative hierarchical clustering procedure. Each observation begins in a cluster by itself. The two closest clusters are merged to form a new cluster that replaces the two old clusters. Merging of the two closest clusters is repeated until only one cluster is left. The various clustering methods differ in how the distance between two clusters is computed.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\ndata t;\n    input cid $ 1-2 income educ;\ncards;\nc1 5 5\nc2 6 6\nc3 15 14\nc4 16 15\nc5 25 20\nc6 30 19\nrun;\n\nproc cluster simple noeigen method=centroid rmsstd rsquare nonorm out=tree;\nid cid;\nvar income educ;\nrun;\n\n\n\n\n\n\n\n\nThe \nSIMPLE\n option displays simple, descriptive statistics. \n\n\nThe \nNOEIGEN\n option suppresses computation of eigenvalues. Specifying the \nNOEIGEN\n option saves time if the number of variables is large, but it should be used only if the variables are nearly uncorrelated or if you are not interested in the cubic clustering criterion. \n\n\nThe \nMETHOD=\n specification determines the clustering method used by the procedure. Here, we are using \nCENTROID\n method. The \nCentroid Distance\n that appears in the output is simply the Euclidian distance between the centroid of the two clusters that are to be joined or merged. It is a measure of the homogeneity of merged clusters and the value should be small.\n\n\nThe \nRMSSTD\n option displays the root-mean-square standard deviation of each cluster. \nRMSSTD\n is the pooled standard deviation of all the variables forming the cluster. Since the objective of cluster analysis is to form homogeneous groups, the \nRMSSTD\n of a cluster should be as small as possible.\n\n\nThe \nRSQUARE\n option displays the $R^2$ (\nRSQ\n) and semipartial $R^2$ (\nSPRSQ\n) to evaluate cluster solution. \nRSQ\n measures the extent to which groups or clusters are different from each other (so, when you have just one cluster \nRSQ\n value is, intuitively, zero). Thus, the \nRSQ\n value should be high.\nSPRSQ\n is a measure of the homogeneity of merged clusters, i.e. the loss of homogeneity due to combining two groups or clusters to form a new group or cluster.  Thus, its value should be small to imply that we are merging two homogeneous groups. \n\n\nThe \nNONORM\n option prevents the distances from being normalized to unit mean or unit root mean square with most methods. \n\n\nThe values of the \nID\n variable identify observations in the displayed cluster history and in the \nOUTTREE=\n data set. If the \nID\n statement is omitted, each observation is denoted by \nOBn\n, where n is the observation number.\n\n\nThe \nVAR\n statement lists numeric variables to be used in the cluster analysis. If you omit the \nVAR\n statement, all numeric variables not listed in other statements are used.\n\n\n\n\nPROC FASTCLUS\n: Disjoint Cluster Analysis\n\n\nThe \nFASTCLUS\n procedure performs a \ndisjoint cluster analysis\n on the basis of distances computed from one or more quantitative variables. The observations are \ndivided into clusters such that every observation belongs to one and only one cluster\n; the clusters \ndo not form a tree structure\n as they do in the \nCLUSTER\n procedure. If you want separate analyses for different numbers of clusters, you can run \nPROC FASTCLUS\n once for each analysis. The \nFASTCLUS\n procedure requires time proportional to the number of observations and thus can be used with much larger data sets than \nPROC CLUSTER\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\ndata t2;\n    input cid $ 1-2 income educ;\ncards;\nc1 5 5\nc2 6 6\nc3 15 14\nc4 16 15\nc5 25 20\nc6 30 19\nrun;\n\nproc fastclus radius=0 replace=full maxclusters=3 maxiter=20 list distance;\nid cid;\nvar income educ;\nrun;\n\n\n\n\n\n\nYou must specify either the \nMAXCLUSTERS=\n or the \nRADIUS=\n argument in the \nPROC FASTCLUS\n statement.\n\n\n\n\nThe \nRADIUS=\n option establishes the minimum distance criterion for selecting new seeds. No observation is considered as a new seed unless its minimum distance to previous seeds exceeds the value given by the \nRADIUS=\n option. The default value is 0. \n\n\nThe \nMAXCLUSTERS=\n option specifies the maximum number of clusters allowed. If you omit the \nMAXCLUSTERS=\n option, a value of 100 is assumed. \n\n\n\n\nThe \nREPLACE=\n option specifies how seed replacement is performed. \n\n\n\n\nFULL\n requests default seed replacement. \n\n\nPART\n requests seed replacement only when the distance between the observation and the closest seed is greater than the minimum distance between seeds. \n\n\nNONE\n suppresses seed replacement. \n\n\nRANDOM\n selects a simple pseudo-random sample of complete observations as initial cluster seeds. \n\n\n\n\n\n\n\n\nThe \nMAXITER=\n option specifies the maximum number of iterations for recomputing cluster seeds. When the value of the \nMAXITER=\n option is greater than 0, each observation is assigned to the nearest seed, and the seeds are recomputed as the means of the clusters. \n\n\n\n\nThe \nLIST\n option lists all observations, giving the value of the \nID\n variable (if any), the number of the cluster to which the observation is assigned, and the distance between the observation and the final cluster seed. \n\n\nThe \nDISTANCE\n option computes distances between the cluster means. \n\n\nThe \nID\n variable, which can be character or numeric, identifies observations on the output when you specify the \nLIST\n option.\n\n\nThe \nVAR\n statement lists the numeric variables to be used in the cluster analysis. If you omit the \nVAR\n statement, all numeric variables not listed in other statements are used.\n\n\n\n\nThe cluster analysis may converge to a solution at the $n^{th}$ iteration because the change in cluster seeds at this iteration is less than the convergence criterion.  Note that a zero change in the centroid of the cluster seeds for the $n^{th}$ iteration implies that the reallocation did not result in any reassignment of observations.\n\n\nThe statistics used for the evaluation of the cluster solution are the same as in the hierarchical cluster analysis.\n\n\nThe cluster solution can also be \nevaluated with respect to each clustering variable\n. If the measurement scales are not the same, then for each variable one should obtain the \nratio\n of the respective \nWithin STD\n to the \nTotal STD\n, and compare this ratio across the variables.\n\n\n\n\nInteresting Examples\n\n\n\n\nMultivariate Statistical Analysis in SAS: Segmentation and Classification of Behavioral Data\n\n\n\n\n\n\nMixed Clustering\n\n\nOn large data sets a useful methodology consists first in summarizing the observations in a large enough number of clusters (100 may be a standard value) and then applying a hierarchical clustering technique for aggregating these groups (Example \nhere\n).\n\n\nThis procedure has the advantages of the hierarchical method for showing an optimal number of clusters and solves the difficulty of the too high initial number of observations by first clustering them, using a non hierarchical method, in a smaller number of clusters. This number is a parameter of the procedure; it must be high enough in order not to impose a prior partitionning of the data.\n\n\nPROC VARCLUS\n: Variable Clustering\n\n\nThe \nVARCLUS\n procedure divides a set of numeric variables into disjoint or hierarchical clusters. \nPROC VARCLUS\n tries to maximize the variance that is explained by the cluster components, summed over all the clusters.\n\n\nIn an ordinary principal component analysis, all components are computed from the same variables, and the first principal component is orthogonal to the second principal component and to every other principal component. In \nPROC VARCLUS\n, each cluster component is computed from a set of variables that is different from all the other cluster components. The first principal component of one cluster might be correlated with the first principal component of another cluster. Hence, the \nPROC VARCLUS\n algorithm is a type of oblique component analysis.\n\n\nPROC VARCLUS\n can be used as a \nvariable-reduction method\n. A large set of variables can often be replaced by the set of cluster components with little loss of information. A given number of cluster components does not generally explain as much variance as the same number of principal components on the full set of variables, but the cluster components are usually easier to interpret than the principal components, even if the latter are rotated.\n\n\n1\n2\n3\nPROC VARCLUS DATA=SAS-data-set MAXEIGEN=0.7 OUTTREE=fortree short noprint;  \n    VAR variable1 variable2 variable3;\nRUN;\n\n\n\n\n\n\nPROC TREE\n\n\nThe \nTREE\n procedure produces a tree diagram from a \ndata set created by the \nCLUSTER\n or \nVARCLUS\n procedure\n that contains the results of \nhierarchical clustering\n as a tree structure.\n\n\n1\n2\n3\n4\nPROC TREE DATA=tree OUT=clus3 NCLUSTERS=3;\n    ID id-variable;\n    COPY variable1 variable2 variable3;\nRUN;\n\n\n\n\n\n\nThe \nTREE\n procedure produces a tree diagram, also known as a dendrogram or phenogram, using a data set created by the \nCLUSTER\n procedure. The \nCLUSTER\n procedure creates output data sets that contain the results of \nhierarchical clustering as a tree structure\n. The \nTREE\n procedure uses the output data set to produce a diagram of the tree structure.\n\n\n\n\nThe \nNCLUSTERS=\n option specifies the number of clusters desired in the \nOUT=\n data set.\n\n\nThe \nID\n variable is used to identify the objects (leaves) in the tree on the output. The \nID\n variable can be a character or numeric variable of any length. \n\n\nThe \nCOPY\n statement specifies one or more character or numeric variables to be copied to the \nOUT=\n data set.\n\n\n\n\nChoosing the Optimal Number of Clusters for the Analysis\n\n\nIn most cases, before using a clustering technique you have no prior idea of the number of clusters which will give the better differenciation of the data. The main objective is to summarize the data in the best way possible, i.e. getting a compromise between a good degree of differentiation and a not too high number of clusters.\n\n\nFor hierarchical clustering try the Sarle's Cubic Clustering Criterion in PROC CLUSTER:\nplot \nCCC\n versus the number of clusters and look for peaks where \nccc\n \n 3 or look for local peaks of pseudo-F statistic (\nPSF\n) combined with a small value of the pseudo-t^2 statistic (\nPST2\n) and a larger pseudo t^2 for the next cluster fusion.\nFor K-Means clustering use this approach on a sample of your data to determine the max limit for k and assign it to the maxc= option in PROC FASTCLUS on the complete data. \n\n\nFor K-means cluster analysis, one can use \nPROC FASTCLUS\n like\n\nPROC FASTCLUS DATA=SAS-data-set OUT=out MAXC=4 MAXITER=20;\n\nand change the number defined by \nMAXC=\n, and run a number of times, then compare the \nPseduo F\n and \nCCC\n values, to see which number of clusters gives peaks.\n\n\nYou can also use \nPROC CLUSTER\n\n\nPROC CLUSTER data=mydata METHOD=WARD out=out ccc pseudo print=15;\n\nto find the number of clusters with \npseudo F\n, \npseudo-$t^2$\n and \nCCC\n, and also look at junp in \nSemipartial R-Square\n.\n\n\nSometimes these indications do not agree to each other. which indicator is more reliable?\nIf you are doubting between 2 k-values, you can use Beale's F-type statistic to determine the final number of clusters. It will tell you whether the larger solution is significantly better or not (in the latter case the solution with fewer clusters is preferable).\nThis technique is discussed in the \"Applied Clustering Techniques\" course notes.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n*\n \nDefine\n \nthe\n \nvariables\n \nfor\n \nclustering\n;\n\n\n*------------------------------------\n;\n\n\n%let\n \nvarlist\n=\nvar1\n \nvar2\n \nvar3\n \nvar4\n \nvar5\n;\n\n\n\n*\n \nMacro\n \nwith\n \nthe\n \ncluster\n \nprocedure\n,\n \nto\n \ncall\n \nit\n \nwith\n \ndifferent\n \nnumber\n \nof\n \nclusters\n;\n\n\n*-------------------------------------------------------------------------------\n;\n\n\n%\nMACRO\n \nCLUSTERSIZE\n(\n\n         \ndatain\n=\n\n        \n,\ndataout\n=\n\n        \n,\nmaxclusters\n=\n\n        \n,\nmaxiter\n=\n\n        \n);\n\n    \nproc\n \nfastclus\n \ndata\n=\ndatain\n.\n \nout\n=\ndataout\n.\nmaxclusters\n.\n \noutstat\n=\nstatdata\nmaxclusters\n.\n \nmaxclusters\n=\nmaxclusters\n.\n \nmaxiter\n=\nmaxiter\n.\n \nnoprint\n;\n\n        \nid\n \npatient\n;\n\n        \nvar\n \nvarlist\n.;\n\n    \nrun\n;\n\n    \ntitle\n;\n\n\n%\nMEND\n;\n\n\n\n*\n \nCalcultion\n \nof\n \nthe\n \ncluster\n \nanalysis\n \nstatistics\n \nfor\n \n1\n-\n20\n \nclusters\n \nand\n \ndata\n \nset\n \ncreation\n \nfor\n \nelbow\n \nplot\n \nwith\n \nRSQ\n \nvalues\n;\n\n\n*---------------------------------------------------------------------------------------------------------------------\n;\n\n\n%macro\n \nstatCLUSTER\n;\n\n     \n%do\n \nk\n=\n \n1\n \n%to\n \n20\n;\n\n        \n%\nCLUSTERSIZE\n(\ndatain\n=\nSAS\n-\ndata\n-\nset\n,\n \ndataout\n=\nclusterdata\n,\n \nmaxclusters\n=\nk\n.,\n \nmaxiter\n=\n1000\n);\n\n        \ndata\n \nclusrsq\nk\n.;\n\n            \nset\n \nstatdata\nk\n.;\n\n            \nnclust\n=\nk\n.;\n\n            \nif\n \n_type_\n=\nRSQ\n;\n\n            \nkeep\n \nnclust\n \nover_all\n;\n\n        \nrun\n;\n\n     \n%end\n;\n\n\n%mend\n;\n\n\n%stat\nCLUSTER\n;\n\n\n\ndata\n \nclus_rsq\n;\n\n    \nset\n \nclusrsq1\n \nclusrsq2\n \nclusrsq3\n \nclusrsq4\n \nclusrsq5\n \nclusrsq6\n \nclusrsq7\n \nclusrsq8\n \nclusrsq9\n \nclusrsq10\n \nclusrsq11\n \nclusrsq12\n \nclusrsq13\n \nclusrsq14\n \nclusrsq15\n \nclusrsq16\n \nclusrsq17\n \nclusrsq18\n \nclusrsq19\n \nclusrsq20\n;\n\n\nrun\n;\n\n\n\n*\n \nRemove\n \nuseless\n \ndata\n \nsets\n;\n\n\n*-------------------------\n;\n\n\nproc\n \ndatasets\n \nlib\n=\nwork\n \nnowarn\n \nnolist\n \nnodetails\n;\n \n    \ndelete\n \nclusrsq\n:\n \nstatdata\n:\n \nclusterdata\n:\n \n;\n\n\nrun\n;\n \n\nquit\n;\n\n\n\n*\n \nPlot\n \nelbow\n \ncurve\n \nusing\n \nr\n-\nsquare\n \nvalues\n \nhighlighting\n \nthe\n \nbest\n \ncandidates\n \nto\n \noptimum\n \nnumber\n \nof\n \nclusters\n;\n\n\n*------------------------------------------------------------------------------------------------------\n;\n\n\nsymbol1\n \ncolor\n=\nblue\n \ninterpol\n=\njoin\n;\n\n\naxis1\n \nlabel\n=\n(\nNumber\n \nof\n \nclusters\n \nin\n \nthe\n \nanalysis\n)\n \norder\n=\n(\n0\n \nto\n \n15\n \nby\n \n1\n)\n \nreflabel\n=\n(\nj\n=\nc\n \nh\n=\n9\npt\n \nCandidate\n \n1\n \nCandidate\n \n2\n);\n\n\naxis2\n \nlabel\n=\n(\nR\n^\n2\n \nvalues\n \nj\n=\nc\n);\n\n\nproc\n \ngplot\n \ndata\n=\nclus_rsq\n;\n\n    \nplot\n \nover_all\n*\nnclust\n \n/\n \nhaxis\n=\naxis1\n \nvaxis\n=\naxis2\n \nhref\n=\n3\n \n5\n;\n\n\nrun\n;\n\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nThe Number of Clusters\n\n\n\n\n\n\nFurther Examination of the Cluster Analysis Solution\n\n\nVisualizing the Results\n\n\nTo interpret a cluster analysis solution, the first thing you want to try is to graph the cluster in a scatter plot to see whether or not they overlap with each other in terms of their location in the $p-$dimensional space. If the vectors have a high dimensionality,  we use \nCanonical Discriminant Analysis (CDA)\n. It is a \ndimension-reduction technique related to principal component analysis and canonical correlation\n. It creates a smaller number of variables that are linear combinations of the original $p$ clustering variables. The new variables, called canonical variables, are ordered in terms of the proportion of variance in the clustering variables that is accounted for by each of the canonical variables. Usually, the majority of the variants in the clustering variable will be accounted for by the first couple of canonical varaibles and those are the variables we can plot.\n\n\nIn SAS we can use the \nCANDISC\nprocedure to create the canonical variables for our cluster analysis output data set that has the cluster assignment variable that we created when we ran the cluster analysis. \n\n\n1\n2\n3\n4\nPROC CANDISC DATA=clusterdata OUT=candata;\n    CLASS cluster-assignment-variable;\n    VAR clustering1 clustering2;\nRUN;\n\n\n\n\n\n\n\n\nThe \nOUT=\n is the output data set that includes the canonical variables that are estimated by the canonical discriminate analysis.\n\n\nThe \nCLASS\n variable (mandatory) is the cluster assignment variable which is a categorical variable.\n\n\nIn the \nVAR\n statement we list the clustering variables\n\n\n\n\nYou can then plot the first two canonical variables using the \nSGPLOT\n procedure:\n\n\n1\n2\n3\nPROC SGPLOT DATA=candata;\n    SCATTER Y=can2 X=can1 / GROUP=cluster-assignment-variable;\nRUN;\n\n\n\n\n\n\nLet's analyze the following example result for a 4-cluster analysis.\n\n\n\n\nWhat this shows is that the observations in \nclusters 1 and 4\n are densely packed, meaning they are pretty highly correlated with each other, and \nwithin cluster variance is relatively low\n. In addition, they \ndo not overlap\n very much with the other clusters. The observations from \ncluster 2\n are a little more spread out, indicating less correlation among the observations and higher within cluster variance. But generally, the cluster is relatively distinct with the exception of some observations which are closer to clusters 1 and 4 indicating \nsome overlap\n with these clusters. However, \ncluster 3\n is all over the place. There is come indication of a cluster but the observations are spread out more than the other clusters. This means that the \nwithin cluster variance is high\n as there is less correlation between the observations in this cluster, so we do not really know what is going to happen with that cluster. What this suggests is that the \nbest cluster solution may have fewer than 4 clusters\n, meaning that it would be especially important to further evaluate the cluster solutions with fewer than four clusters.\n\n\n\n\nCheck these websites\n\n\n\n\nDiscriminant Function Analysis in SAS (UCLA)\n\n\nIntroduction to Discriminant Procedures\n\n\n\n\n\n\nCluster Means and Standard Deviations\n\n\nYou can also check these values per cluster to detect possible similarities between groups and detect the most different ones.", 
            "title": "Clustering"
        }, 
        {
            "location": "/other-analysis/clustering/#introduction-to-cluster-analysis", 
            "text": "When subjects are sampled, randomized or allocated by clusters, several statistical problems arise. If observations within a cluster are correlated, one of the assumptions of estimation and hypothesis testing is violated. Because of this correlation, the analyses must be modified to take into account the cluster design effect. When cluster designs are used, there are two sources of variations in the observations. The first is the  one between subjects within a cluster , and the second is the  variability among clusters . These two sources of variation cause the variance to inflate and must be taken into account in the analysis.", 
            "title": "Introduction to Cluster Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#gettin-ready-for-a-cluster-analysis", 
            "text": "", 
            "title": "Gettin' Ready for a Cluster Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#check-for-missing-data", 
            "text": "Variables with missing data should be excluded from the calculation unless they can be imputed.  1\n2\n3\n4 DATA SAS-data-set-without-missing;\n    SET SAS-data-set-with-missing;\n    IF CMISS(OF _ALL_) THEN DELETE;\nRUN;", 
            "title": "Check for Missing Data"
        }, 
        {
            "location": "/other-analysis/clustering/#dealing-with-categorical-variables", 
            "text": "", 
            "title": "Dealing with Categorical Variables"
        }, 
        {
            "location": "/other-analysis/clustering/#composite-variables", 
            "text": "Other questionnaire data like binary (yes/no questions) or a spectrum of responses can be transformed into  composite variables  to capture multiple questions into a  ranked ordinal scale . A  composite variable  is a variable created by combining two or more individual variables, called indicators, into a single variable. Each indicator alone doesn't provide sufficient information, but altogether they can represent the more complex concept.  A lot of work goes into creating composite variables. The indicators of the multidimensional concept must be specified. It's important that each indicator contribute unique information to the final score. The formula for combining the indicators into a single score, called aggregating data, must be established. The computation involved will depend on the type of data that is being aggregated. To aggregate the data, raw scores might be summed, averaged, transformed, and/or weighted.", 
            "title": "Composite variables"
        }, 
        {
            "location": "/other-analysis/clustering/#hot-encoding", 
            "text": "Check  this website  for a macro to generate dummy variables.", 
            "title": "Hot encoding"
        }, 
        {
            "location": "/other-analysis/clustering/#ordinal-categorical-variables", 
            "text": "If your categorical variables have an ordinal meaning you can create an auxiliary numeric variable with indexes representing the ordinal scale and use this new variable, which can be standardized, for the analysis.", 
            "title": "Ordinal Categorical Variables"
        }, 
        {
            "location": "/other-analysis/clustering/#methods-for-data-reduction", 
            "text": "You may need to reduce the number of variables to include in the analysis. There are several methods for this:   Principal Component Analysis with  PROC FACTOR  Variable Reduction for Modeling using  PROC VARCLUS", 
            "title": "Methods for data reduction"
        }, 
        {
            "location": "/other-analysis/clustering/#standardize-your-data", 
            "text": "When performing multivariate analysis, having variables that are measured at different scales can influence the numerical stability and precision of the estimators. Standardizing the data prior to performing statistical analysis can often prevent this problem.  1\n2\n3\n4\n5\n6\n7\n8 DATA SAS-data-set;\n    SET SAS-data-set-unformatted;\n    format variable1 variable2 variable3 10.4;\nRUN;\n\nPROC STANDARD DATA=SAS-data-set out=SAS-output-data-set MEAN=0 STD=1;\n    VAR variable1 variable2 variable3;\nRUN;    Warning  Do not forget to change the format of your numerical data and increase the number of decimal places before performing the standardization. Otherwise you may lose a lot of details on this process that can be crucial for the data analysis.    Check these websites   Standardization Procedures  Standardization of Variables in Cluster Analysis", 
            "title": "Standardize your Data"
        }, 
        {
            "location": "/other-analysis/clustering/#sas-procedures-to-perform-cluster-analysis", 
            "text": "Ward's minimum-variance hierarchical clustering method using agglomerative (bottom-up) approach and Ward's linkage.   Check these websites   Introduction to Clustering Procedures", 
            "title": "SAS Procedures to Perform Cluster Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#proc-cluster-hierarchical-cluster-analysis", 
            "text": "The  CLUSTER  procedure  hierarchically clusters the observations  in a SAS data set by using one of 11 methods. The data can be coordinates or distances.   All methods are based on the usual agglomerative hierarchical clustering procedure. Each observation begins in a cluster by itself. The two closest clusters are merged to form a new cluster that replaces the two old clusters. Merging of the two closest clusters is repeated until only one cluster is left. The various clustering methods differ in how the distance between two clusters is computed.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 data t;\n    input cid $ 1-2 income educ;\ncards;\nc1 5 5\nc2 6 6\nc3 15 14\nc4 16 15\nc5 25 20\nc6 30 19\nrun;\n\nproc cluster simple noeigen method=centroid rmsstd rsquare nonorm out=tree;\nid cid;\nvar income educ;\nrun;    The  SIMPLE  option displays simple, descriptive statistics.   The  NOEIGEN  option suppresses computation of eigenvalues. Specifying the  NOEIGEN  option saves time if the number of variables is large, but it should be used only if the variables are nearly uncorrelated or if you are not interested in the cubic clustering criterion.   The  METHOD=  specification determines the clustering method used by the procedure. Here, we are using  CENTROID  method. The  Centroid Distance  that appears in the output is simply the Euclidian distance between the centroid of the two clusters that are to be joined or merged. It is a measure of the homogeneity of merged clusters and the value should be small.  The  RMSSTD  option displays the root-mean-square standard deviation of each cluster.  RMSSTD  is the pooled standard deviation of all the variables forming the cluster. Since the objective of cluster analysis is to form homogeneous groups, the  RMSSTD  of a cluster should be as small as possible.  The  RSQUARE  option displays the $R^2$ ( RSQ ) and semipartial $R^2$ ( SPRSQ ) to evaluate cluster solution.  RSQ  measures the extent to which groups or clusters are different from each other (so, when you have just one cluster  RSQ  value is, intuitively, zero). Thus, the  RSQ  value should be high. SPRSQ  is a measure of the homogeneity of merged clusters, i.e. the loss of homogeneity due to combining two groups or clusters to form a new group or cluster.  Thus, its value should be small to imply that we are merging two homogeneous groups.   The  NONORM  option prevents the distances from being normalized to unit mean or unit root mean square with most methods.   The values of the  ID  variable identify observations in the displayed cluster history and in the  OUTTREE=  data set. If the  ID  statement is omitted, each observation is denoted by  OBn , where n is the observation number.  The  VAR  statement lists numeric variables to be used in the cluster analysis. If you omit the  VAR  statement, all numeric variables not listed in other statements are used.", 
            "title": "PROC CLUSTER: Hierarchical Cluster Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#proc-fastclus-disjoint-cluster-analysis", 
            "text": "The  FASTCLUS  procedure performs a  disjoint cluster analysis  on the basis of distances computed from one or more quantitative variables. The observations are  divided into clusters such that every observation belongs to one and only one cluster ; the clusters  do not form a tree structure  as they do in the  CLUSTER  procedure. If you want separate analyses for different numbers of clusters, you can run  PROC FASTCLUS  once for each analysis. The  FASTCLUS  procedure requires time proportional to the number of observations and thus can be used with much larger data sets than  PROC CLUSTER .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 data t2;\n    input cid $ 1-2 income educ;\ncards;\nc1 5 5\nc2 6 6\nc3 15 14\nc4 16 15\nc5 25 20\nc6 30 19\nrun;\n\nproc fastclus radius=0 replace=full maxclusters=3 maxiter=20 list distance;\nid cid;\nvar income educ;\nrun;   You must specify either the  MAXCLUSTERS=  or the  RADIUS=  argument in the  PROC FASTCLUS  statement.   The  RADIUS=  option establishes the minimum distance criterion for selecting new seeds. No observation is considered as a new seed unless its minimum distance to previous seeds exceeds the value given by the  RADIUS=  option. The default value is 0.   The  MAXCLUSTERS=  option specifies the maximum number of clusters allowed. If you omit the  MAXCLUSTERS=  option, a value of 100 is assumed.    The  REPLACE=  option specifies how seed replacement is performed.    FULL  requests default seed replacement.   PART  requests seed replacement only when the distance between the observation and the closest seed is greater than the minimum distance between seeds.   NONE  suppresses seed replacement.   RANDOM  selects a simple pseudo-random sample of complete observations as initial cluster seeds.      The  MAXITER=  option specifies the maximum number of iterations for recomputing cluster seeds. When the value of the  MAXITER=  option is greater than 0, each observation is assigned to the nearest seed, and the seeds are recomputed as the means of the clusters.    The  LIST  option lists all observations, giving the value of the  ID  variable (if any), the number of the cluster to which the observation is assigned, and the distance between the observation and the final cluster seed.   The  DISTANCE  option computes distances between the cluster means.   The  ID  variable, which can be character or numeric, identifies observations on the output when you specify the  LIST  option.  The  VAR  statement lists the numeric variables to be used in the cluster analysis. If you omit the  VAR  statement, all numeric variables not listed in other statements are used.   The cluster analysis may converge to a solution at the $n^{th}$ iteration because the change in cluster seeds at this iteration is less than the convergence criterion.  Note that a zero change in the centroid of the cluster seeds for the $n^{th}$ iteration implies that the reallocation did not result in any reassignment of observations.  The statistics used for the evaluation of the cluster solution are the same as in the hierarchical cluster analysis.  The cluster solution can also be  evaluated with respect to each clustering variable . If the measurement scales are not the same, then for each variable one should obtain the  ratio  of the respective  Within STD  to the  Total STD , and compare this ratio across the variables.   Interesting Examples   Multivariate Statistical Analysis in SAS: Segmentation and Classification of Behavioral Data", 
            "title": "PROC FASTCLUS: Disjoint Cluster Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#mixed-clustering", 
            "text": "On large data sets a useful methodology consists first in summarizing the observations in a large enough number of clusters (100 may be a standard value) and then applying a hierarchical clustering technique for aggregating these groups (Example  here ).  This procedure has the advantages of the hierarchical method for showing an optimal number of clusters and solves the difficulty of the too high initial number of observations by first clustering them, using a non hierarchical method, in a smaller number of clusters. This number is a parameter of the procedure; it must be high enough in order not to impose a prior partitionning of the data.", 
            "title": "Mixed Clustering"
        }, 
        {
            "location": "/other-analysis/clustering/#proc-varclus-variable-clustering", 
            "text": "The  VARCLUS  procedure divides a set of numeric variables into disjoint or hierarchical clusters.  PROC VARCLUS  tries to maximize the variance that is explained by the cluster components, summed over all the clusters.  In an ordinary principal component analysis, all components are computed from the same variables, and the first principal component is orthogonal to the second principal component and to every other principal component. In  PROC VARCLUS , each cluster component is computed from a set of variables that is different from all the other cluster components. The first principal component of one cluster might be correlated with the first principal component of another cluster. Hence, the  PROC VARCLUS  algorithm is a type of oblique component analysis.  PROC VARCLUS  can be used as a  variable-reduction method . A large set of variables can often be replaced by the set of cluster components with little loss of information. A given number of cluster components does not generally explain as much variance as the same number of principal components on the full set of variables, but the cluster components are usually easier to interpret than the principal components, even if the latter are rotated.  1\n2\n3 PROC VARCLUS DATA=SAS-data-set MAXEIGEN=0.7 OUTTREE=fortree short noprint;  \n    VAR variable1 variable2 variable3;\nRUN;", 
            "title": "PROC VARCLUS: Variable Clustering"
        }, 
        {
            "location": "/other-analysis/clustering/#proc-tree", 
            "text": "The  TREE  procedure produces a tree diagram from a  data set created by the  CLUSTER  or  VARCLUS  procedure  that contains the results of  hierarchical clustering  as a tree structure.  1\n2\n3\n4 PROC TREE DATA=tree OUT=clus3 NCLUSTERS=3;\n    ID id-variable;\n    COPY variable1 variable2 variable3;\nRUN;   The  TREE  procedure produces a tree diagram, also known as a dendrogram or phenogram, using a data set created by the  CLUSTER  procedure. The  CLUSTER  procedure creates output data sets that contain the results of  hierarchical clustering as a tree structure . The  TREE  procedure uses the output data set to produce a diagram of the tree structure.   The  NCLUSTERS=  option specifies the number of clusters desired in the  OUT=  data set.  The  ID  variable is used to identify the objects (leaves) in the tree on the output. The  ID  variable can be a character or numeric variable of any length.   The  COPY  statement specifies one or more character or numeric variables to be copied to the  OUT=  data set.", 
            "title": "PROC TREE"
        }, 
        {
            "location": "/other-analysis/clustering/#choosing-the-optimal-number-of-clusters-for-the-analysis", 
            "text": "In most cases, before using a clustering technique you have no prior idea of the number of clusters which will give the better differenciation of the data. The main objective is to summarize the data in the best way possible, i.e. getting a compromise between a good degree of differentiation and a not too high number of clusters.  For hierarchical clustering try the Sarle's Cubic Clustering Criterion in PROC CLUSTER:\nplot  CCC  versus the number of clusters and look for peaks where  ccc    3 or look for local peaks of pseudo-F statistic ( PSF ) combined with a small value of the pseudo-t^2 statistic ( PST2 ) and a larger pseudo t^2 for the next cluster fusion.\nFor K-Means clustering use this approach on a sample of your data to determine the max limit for k and assign it to the maxc= option in PROC FASTCLUS on the complete data.   For K-means cluster analysis, one can use  PROC FASTCLUS  like PROC FASTCLUS DATA=SAS-data-set OUT=out MAXC=4 MAXITER=20; \nand change the number defined by  MAXC= , and run a number of times, then compare the  Pseduo F  and  CCC  values, to see which number of clusters gives peaks.  You can also use  PROC CLUSTER  PROC CLUSTER data=mydata METHOD=WARD out=out ccc pseudo print=15; \nto find the number of clusters with  pseudo F ,  pseudo-$t^2$  and  CCC , and also look at junp in  Semipartial R-Square .  Sometimes these indications do not agree to each other. which indicator is more reliable?\nIf you are doubting between 2 k-values, you can use Beale's F-type statistic to determine the final number of clusters. It will tell you whether the larger solution is significantly better or not (in the latter case the solution with fewer clusters is preferable).\nThis technique is discussed in the \"Applied Clustering Techniques\" course notes.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53 *   Define   the   variables   for   clustering ;  *------------------------------------ ;  %let   varlist = var1   var2   var3   var4   var5 ;  *   Macro   with   the   cluster   procedure ,   to   call   it   with   different   number   of   clusters ;  *------------------------------------------------------------------------------- ;  % MACRO   CLUSTERSIZE ( \n          datain = \n         , dataout = \n         , maxclusters = \n         , maxiter = \n         ); \n     proc   fastclus   data = datain .   out = dataout . maxclusters .   outstat = statdata maxclusters .   maxclusters = maxclusters .   maxiter = maxiter .   noprint ; \n         id   patient ; \n         var   varlist .; \n     run ; \n     title ;  % MEND ;  *   Calcultion   of   the   cluster   analysis   statistics   for   1 - 20   clusters   and   data   set   creation   for   elbow   plot   with   RSQ   values ;  *--------------------------------------------------------------------------------------------------------------------- ;  %macro   statCLUSTER ; \n      %do   k =   1   %to   20 ; \n         % CLUSTERSIZE ( datain = SAS - data - set ,   dataout = clusterdata ,   maxclusters = k .,   maxiter = 1000 ); \n         data   clusrsq k .; \n             set   statdata k .; \n             nclust = k .; \n             if   _type_ = RSQ ; \n             keep   nclust   over_all ; \n         run ; \n      %end ;  %mend ;  %stat CLUSTER ;  data   clus_rsq ; \n     set   clusrsq1   clusrsq2   clusrsq3   clusrsq4   clusrsq5   clusrsq6   clusrsq7   clusrsq8   clusrsq9   clusrsq10   clusrsq11   clusrsq12   clusrsq13   clusrsq14   clusrsq15   clusrsq16   clusrsq17   clusrsq18   clusrsq19   clusrsq20 ;  run ;  *   Remove   useless   data   sets ;  *------------------------- ;  proc   datasets   lib = work   nowarn   nolist   nodetails ;  \n     delete   clusrsq :   statdata :   clusterdata :   ;  run ;   quit ;  *   Plot   elbow   curve   using   r - square   values   highlighting   the   best   candidates   to   optimum   number   of   clusters ;  *------------------------------------------------------------------------------------------------------ ;  symbol1   color = blue   interpol = join ;  axis1   label = ( Number   of   clusters   in   the   analysis )   order = ( 0   to   15   by   1 )   reflabel = ( j = c   h = 9 pt   Candidate   1   Candidate   2 );  axis2   label = ( R ^ 2   values   j = c );  proc   gplot   data = clus_rsq ; \n     plot   over_all * nclust   /   haxis = axis1   vaxis = axis2   href = 3   5 ;  run ;     Check these websites   The Number of Clusters", 
            "title": "Choosing the Optimal Number of Clusters for the Analysis"
        }, 
        {
            "location": "/other-analysis/clustering/#further-examination-of-the-cluster-analysis-solution", 
            "text": "", 
            "title": "Further Examination of the Cluster Analysis Solution"
        }, 
        {
            "location": "/other-analysis/clustering/#visualizing-the-results", 
            "text": "To interpret a cluster analysis solution, the first thing you want to try is to graph the cluster in a scatter plot to see whether or not they overlap with each other in terms of their location in the $p-$dimensional space. If the vectors have a high dimensionality,  we use  Canonical Discriminant Analysis (CDA) . It is a  dimension-reduction technique related to principal component analysis and canonical correlation . It creates a smaller number of variables that are linear combinations of the original $p$ clustering variables. The new variables, called canonical variables, are ordered in terms of the proportion of variance in the clustering variables that is accounted for by each of the canonical variables. Usually, the majority of the variants in the clustering variable will be accounted for by the first couple of canonical varaibles and those are the variables we can plot.  In SAS we can use the  CANDISC procedure to create the canonical variables for our cluster analysis output data set that has the cluster assignment variable that we created when we ran the cluster analysis.   1\n2\n3\n4 PROC CANDISC DATA=clusterdata OUT=candata;\n    CLASS cluster-assignment-variable;\n    VAR clustering1 clustering2;\nRUN;    The  OUT=  is the output data set that includes the canonical variables that are estimated by the canonical discriminate analysis.  The  CLASS  variable (mandatory) is the cluster assignment variable which is a categorical variable.  In the  VAR  statement we list the clustering variables   You can then plot the first two canonical variables using the  SGPLOT  procedure:  1\n2\n3 PROC SGPLOT DATA=candata;\n    SCATTER Y=can2 X=can1 / GROUP=cluster-assignment-variable;\nRUN;   Let's analyze the following example result for a 4-cluster analysis.   What this shows is that the observations in  clusters 1 and 4  are densely packed, meaning they are pretty highly correlated with each other, and  within cluster variance is relatively low . In addition, they  do not overlap  very much with the other clusters. The observations from  cluster 2  are a little more spread out, indicating less correlation among the observations and higher within cluster variance. But generally, the cluster is relatively distinct with the exception of some observations which are closer to clusters 1 and 4 indicating  some overlap  with these clusters. However,  cluster 3  is all over the place. There is come indication of a cluster but the observations are spread out more than the other clusters. This means that the  within cluster variance is high  as there is less correlation between the observations in this cluster, so we do not really know what is going to happen with that cluster. What this suggests is that the  best cluster solution may have fewer than 4 clusters , meaning that it would be especially important to further evaluate the cluster solutions with fewer than four clusters.   Check these websites   Discriminant Function Analysis in SAS (UCLA)  Introduction to Discriminant Procedures", 
            "title": "Visualizing the Results"
        }, 
        {
            "location": "/other-analysis/clustering/#cluster-means-and-standard-deviations", 
            "text": "You can also check these values per cluster to detect possible similarities between groups and detect the most different ones.", 
            "title": "Cluster Means and Standard Deviations"
        }, 
        {
            "location": "/procedures/regression-models/", 
            "text": "Interpreting the results of the \nSOLUTION\n option in the \nMODEL\n for categorical variables\n\n\nIn procedures that use the \nGLM\n parameterization for \nCLASS\n variables such as \nPROC GLM\n, \nPROC MIXED\n and \nPROC GLIMMIX\n, a predictor variable specified in the \nCLASS\n statement is represented in the model by a set of design variables created using \nGLM\n parameterization. \n\n\nThis is a less than full-rank parameterization in which a \nCLASS\n variable with k levels is represented in the design matrix by a set of k 0,1-coded indicator (or \ndummy\n) variables. If the \nSOLUTION\n option in the \nMODEL\n statement is also specified, the following note is included in the displayed results below the parameter estimates table:\n\n\n1\nNOTE\n:\n \nThe\n \nX\nX matrix has been found to be singular, and a generalized inverse was used to solve the normal equations. Terms whose estimates are followed by the letter \nB\n \nare\n \nnot\n \nuniquely\n \nestimable\n.\n\n\n\n\n\n\n\nNote that there are many possible parameterizations, each of which imposes a \ndifferent interpretation on the model parameters\n.\n\n\n\n\nSeealso\n\n\n\n\nCheck \nthis\n for a full explanation of the interpretation of the results.\n\n\nRead more\n about the \nGLM\n parametrization.", 
            "title": "General notes on regression procedures"
        }, 
        {
            "location": "/procedures/regression-models/#interpreting-the-results-of-the-solution-option-in-the-model-for-categorical-variables", 
            "text": "In procedures that use the  GLM  parameterization for  CLASS  variables such as  PROC GLM ,  PROC MIXED  and  PROC GLIMMIX , a predictor variable specified in the  CLASS  statement is represented in the model by a set of design variables created using  GLM  parameterization.   This is a less than full-rank parameterization in which a  CLASS  variable with k levels is represented in the design matrix by a set of k 0,1-coded indicator (or  dummy ) variables. If the  SOLUTION  option in the  MODEL  statement is also specified, the following note is included in the displayed results below the parameter estimates table:  1 NOTE :   The   X X matrix has been found to be singular, and a generalized inverse was used to solve the normal equations. Terms whose estimates are followed by the letter  B   are   not   uniquely   estimable .    Note that there are many possible parameterizations, each of which imposes a  different interpretation on the model parameters .   Seealso   Check  this  for a full explanation of the interpretation of the results.  Read more  about the  GLM  parametrization.", 
            "title": "Interpreting the results of the SOLUTION option in the MODEL for categorical variables"
        }, 
        {
            "location": "/procedures/glimmix/", 
            "text": "Modelling a binary reponse variable:\n\n\n1\n2\n3\n4\n5\n6\nPROC GLIMMIX DATA=SAS-data-set;\n  CLASS categorical1 categorical2;\n  MODEL response = continuous2 continuous2 categorical1 / DIST=BINARY LINK=LOGIT ODDSRATIO SOLUTION CL;\n  RANDOM intercept / SUBJECT=categorical2 SOLUTION CL;\n  COVTEST / WALD;\nRUN;\n\n\n\n\n\n\nModelling a multinomial reponse variable:\n\n\n1\n2\n3\n4\n5\n6\nPROC GLIMMIX DATA=SAS-data-set;\n  CLASS categorical1 categorical2;\n  MODEL response = continuous2 continuous2 categorical1 / DIST=MULTINOMIAL LINK=CLOGIT ODDSRATIO SOLUTION CL;\n  RANDOM intercept / SUBJECT=categorical2 SOLUTION CL;\n  COVTEST / WALD;\nRUN;\n\n\n\n\n\n\n\n\nLINK\n specifies the link function in the generalized linear mixed model (\nlink functions available\n)\n\n\nLINK=LOGIT\n to use the \nlogit\n link\n\n\nLINK=GENLOGIT | GLOGIT\n to use the \ngeneralized logit\n link\n\n\nLINK=CUMLOGIT | CLOGIT\n to use the \ncumulative logit\n link\n\n\n\n\n\n\nDIST\n specifies the built-in (conditional) probability distribution of the data (\ndistrubutions available and their corresponfing default link functions\n)\n\n\n\n\nEstimating an Odds Ratio for a Variable Involved in an Interaction\n\n\nIn models with \nLINK=LOGIT | GLOGIT | CLOGIT\n, you can obtain estimates of odds ratios through the \nODDSRATIO\n options in the \nPROC GLIMMIX\n, \nLSMEANS\n, and \nMODEL\n statements. Note that for these link functions the \nEXP\n option in the \nESTIMATE\n and \nLSMESTIMATE\n statements also produces odds or odds ratios. The \nODDSRATIO\n option on the \nPROC GLIMMIX\n statement, requests odds ratio calculations for main effects. \n\n\nEXP\n requests exponentiation of the estimate (\nESTIMATE\n statement) or least squares means estimate (\nLSMESTIMATE\n statement). If you specify the \nCL\n or \nALPHA=\n option, the (adjusted) confidence bounds are also exponentiated.\n\n\nBy default \nLSMEANS\n produces estimates on the \nlogit scale\n. The \nILINK\n option on the \nLSMEANS\n statement requests that the estimates be transformed back to the scale of the original data. The \nLSMEANS\n output will include estimates of the probability of each combination of the predictors interactions included in your model. The \nCL\n option requests confidence intervals for the estimates. For non-normal data, the \nEXP\n and \nILINK\n options give you a way to obtain the quantity of interest on the scale of the mean (inverse link). Results presented in this fashion can be much easier to interpret than data on the link scale. \n\n\n\n\nIs it correct to assume, if you're using a different link function than \nLINK=LOGIT | GLOGIT | CLOGIT\n, that the exponentiated estimate can still be interpreted as the Odds Ratio?\n\n\nNo, that is not correct. The odds ratio only make sense when you are comparing the predicted PROBABILITIES for two or more level of classification variables. If you use \nDIST=GAUSSIAN\n and \nLINK=IDENTITY\n, you are merely fitting a linear model to a response that has values 0 and 1. \nCheck \nthis\n dicussion for more information.\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\ntreatarm = {1, 2}\nvisit = {1, 2, 3, 4, 5}\n\nPROC GLIMMIX DATA=SAS-data-set;\n  CLASS categorical-variable(s);\n  MODEL response = predictor(s) / DIST=BINARY LINK=LOGIT ODDSRATIO SOLUTION CL;\n  RANDOM intercept / SUBJECT=repeated-variable SOLUTION CL;\n  COVTEST / WALD;\n  LSMEANS treatarm*visit / SLICEDIFF=visit ODDSRATIO ILINK CL;\n  LSMESTIMATES treatarm*visit \nOR Visit 1\n 1 0 0 0 0 -1 0 0 0 0,\n               treatarm*visit \nOR Visit 2\n 0 1 0 0 0 0 -1 0 0 0 / EXP ILINK CL;\nRUN;\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nFor more details check the \nSAS documentation\n\n\nAn example different procedures (\nPROC LOGISTIC\n and \nPROC GLIMMIX\n) can be found \nhere\n\n\nSome other options are also discussed \nhere\n and \nhere", 
            "title": "PROC GLIMMIX"
        }, 
        {
            "location": "/procedures/glimmix/#estimating-an-odds-ratio-for-a-variable-involved-in-an-interaction", 
            "text": "In models with  LINK=LOGIT | GLOGIT | CLOGIT , you can obtain estimates of odds ratios through the  ODDSRATIO  options in the  PROC GLIMMIX ,  LSMEANS , and  MODEL  statements. Note that for these link functions the  EXP  option in the  ESTIMATE  and  LSMESTIMATE  statements also produces odds or odds ratios. The  ODDSRATIO  option on the  PROC GLIMMIX  statement, requests odds ratio calculations for main effects.   EXP  requests exponentiation of the estimate ( ESTIMATE  statement) or least squares means estimate ( LSMESTIMATE  statement). If you specify the  CL  or  ALPHA=  option, the (adjusted) confidence bounds are also exponentiated.  By default  LSMEANS  produces estimates on the  logit scale . The  ILINK  option on the  LSMEANS  statement requests that the estimates be transformed back to the scale of the original data. The  LSMEANS  output will include estimates of the probability of each combination of the predictors interactions included in your model. The  CL  option requests confidence intervals for the estimates. For non-normal data, the  EXP  and  ILINK  options give you a way to obtain the quantity of interest on the scale of the mean (inverse link). Results presented in this fashion can be much easier to interpret than data on the link scale.    Is it correct to assume, if you're using a different link function than  LINK=LOGIT | GLOGIT | CLOGIT , that the exponentiated estimate can still be interpreted as the Odds Ratio?  No, that is not correct. The odds ratio only make sense when you are comparing the predicted PROBABILITIES for two or more level of classification variables. If you use  DIST=GAUSSIAN  and  LINK=IDENTITY , you are merely fitting a linear model to a response that has values 0 and 1. \nCheck  this  dicussion for more information.    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 treatarm = {1, 2}\nvisit = {1, 2, 3, 4, 5}\n\nPROC GLIMMIX DATA=SAS-data-set;\n  CLASS categorical-variable(s);\n  MODEL response = predictor(s) / DIST=BINARY LINK=LOGIT ODDSRATIO SOLUTION CL;\n  RANDOM intercept / SUBJECT=repeated-variable SOLUTION CL;\n  COVTEST / WALD;\n  LSMEANS treatarm*visit / SLICEDIFF=visit ODDSRATIO ILINK CL;\n  LSMESTIMATES treatarm*visit  OR Visit 1  1 0 0 0 0 -1 0 0 0 0,\n               treatarm*visit  OR Visit 2  0 1 0 0 0 0 -1 0 0 0 / EXP ILINK CL;\nRUN;    Check these websites   For more details check the  SAS documentation  An example different procedures ( PROC LOGISTIC  and  PROC GLIMMIX ) can be found  here  Some other options are also discussed  here  and  here", 
            "title": "Estimating an Odds Ratio for a Variable Involved in an Interaction"
        }, 
        {
            "location": "/procedures/mixed/", 
            "text": "Linear Mixed Model General Concepts\n\n\nThe linear mixed model is an extension of the general linear model, in which factors and covariates are assumed to have a linear relationship to the dependent variable.\n\n\nFactors.\n\u00a0Categorical\u00a0predictors should be selected as\u00a0factors\u00a0in the model. Each\u00a0level\u00a0of a factor can have a different linear effect on the value of the dependent variable.\n\n\n\n\nFixed-effects factors\n\u00a0are generally thought of as variables whose values of interest are all represented in the data file.\n\n\nRandom-effects factors\n\u00a0are variables whose values in the data file can be considered a random sample from a larger population of values. They are useful for explaining excess variability in the dependent variable.\n\n\n\n\n\n\nPractical Example\n\n\nA grocery store chain is interested in the effects of five different types of coupons on customer spending. At several store locations, these coupons are handed out to customers who frequent that location; one coupon \nselected at random\n is distributed to each customer.\n  The type of coupon is a \nfixed effect\n because the company is interested in those particular coupons. The store location is a \nrandom effect\n because the locations used are a sample from the larger population of interest, and while there is likely to be store-to-store variation in customer spending, the company is not directly interested in that variation in the context of this problem.\n\n\n\n\nCovariates.\n\u00a0Scale predictors\u00a0should be selected as\u00a0covariates\u00a0in the model. Within combinations of factor levels (or\u00a0cells), values of covariates are assumed to be linearly correlated with values of the dependent variables.\n\n\nInteractions.\n\u00a0The Linear Mixed Models procedure allows you to specify factorial interactions, which means that each combination of factor levels can have a different linear effect on the dependent variable. Additionally, you may specify factor-covariate interactions, if you believe that the linear relationship between a covariate and the dependent variable changes for different levels of a factor.\n\n\nRandom effects covariance structure.\n The Linear Mixed Models procedure allows you to specify the relationship between the levels of random effects. By default, levels of random effects are uncorrelated and have the same variance. \n\n\nRepeated effects.\n\u00a0Factors and covariates are features of the general linear model. In the Linear Mixed Models procedure, repeated effects variables are added, allowing you to relax the assumption of independence of the error terms. In order to model the covariance structure of the error terms, you need to specify the following:\n\n\n\n\nRepeated effects variables\n\u00a0are variables whose values in the data file can be considered as markers of multiple observations of a single subject.\n\n\nSubject variables\n\u00a0define the individual subjects of the repeated measurements. The error terms for each individual are independent of those of other individuals.\n\n\nThe\u00a0\ncovariance structure\n\u00a0specifies the relationship between the levels of the repeated effects. The types of covariance structures available allow for residual terms with a wide variety of variances and covariances.\n\n\n\n\n\n\nPractical Example\n\n\nIf the grocery store recorded the purchasing habits of their customers for four consecutive weeks, then the variable\u00a0Week\u00a0would be a \nrepeated effects variable\n. Specifying a subject variable denoting the\u00a0Customer ID\u00a0differentiates the repeated observations of separate customers. Specifying a first-order autoregressive covariance structure reflects your belief that a higher-than-average volume of purchases in one week will correspond to a higher (or lower)-than-average volume in the following week.\n\n\n\n\nSAS Formulation\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC MIXED DATA=SAS-data-set;\n  CLASS categorical1 categorical2;\n  MODEL response = continuous1 categorical1 continuous1*categorical1 / solution;\n  RANDOM categorical2;\n  LSMEANS continuous1*categorical1 / CL PDIFF DIFFS E;\nRUN;\nQUIT;", 
            "title": "PROC MIXED"
        }, 
        {
            "location": "/procedures/mixed/#linear-mixed-model-general-concepts", 
            "text": "The linear mixed model is an extension of the general linear model, in which factors and covariates are assumed to have a linear relationship to the dependent variable.  Factors. \u00a0Categorical\u00a0predictors should be selected as\u00a0factors\u00a0in the model. Each\u00a0level\u00a0of a factor can have a different linear effect on the value of the dependent variable.   Fixed-effects factors \u00a0are generally thought of as variables whose values of interest are all represented in the data file.  Random-effects factors \u00a0are variables whose values in the data file can be considered a random sample from a larger population of values. They are useful for explaining excess variability in the dependent variable.    Practical Example  A grocery store chain is interested in the effects of five different types of coupons on customer spending. At several store locations, these coupons are handed out to customers who frequent that location; one coupon  selected at random  is distributed to each customer.\n  The type of coupon is a  fixed effect  because the company is interested in those particular coupons. The store location is a  random effect  because the locations used are a sample from the larger population of interest, and while there is likely to be store-to-store variation in customer spending, the company is not directly interested in that variation in the context of this problem.   Covariates. \u00a0Scale predictors\u00a0should be selected as\u00a0covariates\u00a0in the model. Within combinations of factor levels (or\u00a0cells), values of covariates are assumed to be linearly correlated with values of the dependent variables.  Interactions. \u00a0The Linear Mixed Models procedure allows you to specify factorial interactions, which means that each combination of factor levels can have a different linear effect on the dependent variable. Additionally, you may specify factor-covariate interactions, if you believe that the linear relationship between a covariate and the dependent variable changes for different levels of a factor.  Random effects covariance structure.  The Linear Mixed Models procedure allows you to specify the relationship between the levels of random effects. By default, levels of random effects are uncorrelated and have the same variance.   Repeated effects. \u00a0Factors and covariates are features of the general linear model. In the Linear Mixed Models procedure, repeated effects variables are added, allowing you to relax the assumption of independence of the error terms. In order to model the covariance structure of the error terms, you need to specify the following:   Repeated effects variables \u00a0are variables whose values in the data file can be considered as markers of multiple observations of a single subject.  Subject variables \u00a0define the individual subjects of the repeated measurements. The error terms for each individual are independent of those of other individuals.  The\u00a0 covariance structure \u00a0specifies the relationship between the levels of the repeated effects. The types of covariance structures available allow for residual terms with a wide variety of variances and covariances.    Practical Example  If the grocery store recorded the purchasing habits of their customers for four consecutive weeks, then the variable\u00a0Week\u00a0would be a  repeated effects variable . Specifying a subject variable denoting the\u00a0Customer ID\u00a0differentiates the repeated observations of separate customers. Specifying a first-order autoregressive covariance structure reflects your belief that a higher-than-average volume of purchases in one week will correspond to a higher (or lower)-than-average volume in the following week.", 
            "title": "Linear Mixed Model General Concepts"
        }, 
        {
            "location": "/procedures/mixed/#sas-formulation", 
            "text": "1\n2\n3\n4\n5\n6\n7 PROC MIXED DATA=SAS-data-set;\n  CLASS categorical1 categorical2;\n  MODEL response = continuous1 categorical1 continuous1*categorical1 / solution;\n  RANDOM categorical2;\n  LSMEANS continuous1*categorical1 / CL PDIFF DIFFS E;\nRUN;\nQUIT;", 
            "title": "SAS Formulation"
        }, 
        {
            "location": "/sas-outputs/reports/", 
            "text": "ODS\n\n\nIn order to produce outputs from SAS, the three more common \nODS\n techniques, that produces different output files, are HTML, RTF, and PDF. Each \nODS\n statement uses options that are specific to that destination. The \nODS\n options (other than the \nFILE=\n option) used in the program are shown in the table below. \n\n\n\n\n\n\n\n\nRTF\n\n\nPDF\n\n\nHTML\n\n\n\n\n\n\n\n\n\n\nBODYTITLE\n \n \nSTARTPAGE=NO\n \n \nKEEPN\n \n \nNOTOC_DATA\n / \nTOC_DATA\n \n \nCONTENTS\n \n \nCOLUMNS=\n \n \nTEXT=\n\n\nBOOKMARKGEN=NO\n \n \nSTARTPAGE=NO\n \n \nCOMPRESS=9\n \n \nTEXT=\n\n\nSTYLE=SASWEB\n \n \nRS=NONE\n\n\n\n\n\n\n\n\nFor an explanation of the options, refer to \nthis page\n or to the \nODS\n User's Guide\n.\n\n\nSet the SAS system options.\n The \nNODATE\n option suppresses the display of the date and time in the output. \nPAGENO=\n specifies the starting page number. \nLINESIZE=\n specifies the output line length, and \nPAGESIZE=\n specifies the number of lines on an output page.\n\n\n1\nOPTIONS NODATE PAGENO=1 LINESIZE=80 PAGESIZE=40;\n\n\n\n\n\n\nPROC PRINT\n\n\nPROC TABULATE\n\n\nPROC TABULATE\n is a procedure that displays descriptive statistics in tabular format. It computes many statistics that other procedures compute, such as \nMEANS\n, \nFREQ\n, and \nREPORT\n and displays these statistics in a table format. \nTABULATE\n will produce tables in up to three dimensions and allows, within each dimension, multiple variables to be reported one after another hierarchically. There are also some very nice mechanisms that can be used to label and format the results. \n\n\n1\n2\n3\n4\n5\n6\nPROC TABULATE \noptions\n;\n    CLASS variables \n/ options\n;\n    VAR variables \n/ options\n;\n    TABLE \npage\n, \nrow\n, column \n/ options\n;\n    ... other statements...;\nRUN;\n\n\n\n\n\n\n\n\nVAR\n is used to list the variables you intend to use to create summary statistics on. They \nmust be numeric\n.\n\n\nCLASS\n variables allow you to get statistics by category. You will get one column/row for each value of the classification variable. You can also specify the universal CLASS variable \nALL\n which allows you to \nget totals\n. They can be \neither numeric or character\n and you can only request counts and percents as statistics. This is almost like using a \nBY\nstatement within the \nTABLE\n.\n\n\nTABLE\n consists of up to three dimension expressions and the table options. You can have multiple table statements in one \nPROC TABULATE\n. This will generate one table for each statement.\n\n\nA \ncomma\n specifies to add a new \ndimension\n. The order of the dimensions is page, row and column. If you only specify one dimension, then it is assumed to be column. If two are specified, row, then column.\n\n\nThe \nasterisk\n is used to produce a \ncross tabulation\n of one variable with another (within the same dimension however, different from \nPROC FREQ\n).\n\n\nA \nblank\n is used to represent \nconcatenation\n (i.e. place this output element after the preceding variable listed).\n\n\nParenthesis\n will \ngroup elements\n and associate an operator with each element in the group.\n\n\nAngle brackets\n specify a \ndenominator definition\n for use in percentage calculations. \n\n\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nPROC TABULATE\n and the Neat Things You Can Do With It\n\n\n\n\n\n\n\n\nVARDEF=divisor\n specifies the divisor to be used in the calculation of the variances. If divisor is \nDF\n (default), the degrees of freedom (N-1) are used as the divisor.\n\n\nORDER=\n specifies the order of appearance in the table of the \nCLASS\n variable levels\n\n\nFORMATTED\n: ordered by the formatted values\n\n\nDATA\n: the order that the observations are read from the data set\n\n\nFREQ\n: order the values so the one that occurs most frequently in the data set appears first\n\n\nINTERNAL\n: ordered by the \nSORT\n procedure (defaults)\n\n\n\n\n\n\n/CONDENSE\n prints multiple logical pages on a single physical page\n\n\n/PRINTMISS\n species that row and column headings are the same for all logical pages of the table\n\n\n/ROW = spacing\n specifies whether all title elements in a row crossing are allotted space even when they are blank. When \nROW=CONSTANT\n (or \nCONST\n), the default, all row title elements have space allotted to them; when \nROW=FLOAT\n, the row title space is divided equally among the nonblank title elements in the crossing\n\n\n\n\nAvailable Statistics\n\n\nIf you do not provide a statistic name, the default statistic produced will be \nN\n for the \nCLASS\n variables and \nSUM\n for the \nVAR\n variables.\n\n\n\n\nSingle Dimensional Table\n\n\n1\n2\n3\n4\n5\nPROC TABULATE DATA=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Height * (N MEAN) Height * MEAN * Sex Weight * MEAN * Sex;\nRUN; \n\n\n\n\n\n\n\n\nTwo Dimensional Table\n\n\nYou can get very different table structures by changing where the statistic definitions are placed. They can be attached to either the \nVAR\n or the \nCLASS\n variable, but the numbers in the cells will \nalways\n be calculated using the \nVAR\n variable(s). \n\n\nThe statistic specification can be \nattached to the columns\n,\n\n\n1\n2\n3\n4\n5\nPROC TABULATE data=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Sex, Height * (N MEAN MAX) Weight * (N MEAN MAX) ;\nRUN; \n\n\n\n\n\n\n\n\nor they can be \nattached to the rows\n.\n\n\n1\n2\n3\n4\n5\nPROC TABULATE data=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Sex * (N MEAN MAX), Height  Weight;\nRUN; \n\n\n\n\n\n\n\n\nYou can specify \nmultiple classification variables\n. They can be used in any of the dimensions and can be nested. When you have multiple \nCLASS\n variables, it is recommended to use the option \nMISSING\n to keep the observations that have any missing values and consider them as valid levels for the \nCLASS\n variable(s) instead of dropping those observations from all the tables.\n\n\n1\n2\n3\n4\n5\nPROC TABULATE DATA=sashelp.cars;\n    CLASS DriveTrain Origin Type / MISSING;\n    VAR Weight Length;\n    TABLE Origin * Type, DriveTrain * Weight * MEAN DriveTrain * Length * MEAN;\nRUN; \n\n\n\n\n\n\n\n\nIn order to get \nmarginal statistics\n in your table, you use the \nALL\n keyword. You can use the keyword in multiple places and, depending on where you put the keyword, there will be different subtotals produced. \n\n\n1\n2\n3\n4\nPROC TABULATE DATA=sashelp.cars;\n    CLASS DriveTrain Origin Type;\n    TABLE (Origin All=\nTotal\n) * (DriveTrain ALL=\nSubtotal DriveTrain\n), (Type ALL=\nSubtotal Type\n* N);\nRUN; \n\n\n\n\n\n\n\n\nThree Dimensional Table\n\n\nThree dimensional tables have a nice way to fill in the upper left area. Instead of the label of the page dimension appearing above the table, you can use the \nBOX=_page_\n option to place that label inside the big white box.\n\n\n1\n2\n3\n4\n5\nPROC TABULATE data=sashelp.cars;\n    CLASS DriveTrain Origin Type;\n    VAR Weight Length;\n    TABLE Origin = \nMade in\n, Type = \nCategory\n, DriveTrain * Weight * MEAN / BOX=_PAGE_;\nRUN; \n\n\n\n\n\n\n\n\nFormatting tables\n\n\nThere are two ways to \nadd labels for your variables\n:\n\n\n\n\nAdd the text after the variable name: \nvariable =\u2018label\u2019\n. This will work for both variables and\nstatistics.\n\n\nAdd a \nLABEL\n statement for variables and/or a \nKEYLABEL\n statement for statistics to your code:\n\n\n\n\n1\n2\nLABEL var=\u2018label\u2019;\nKEYLABEL stat=\u2018label\u2019;\n\n\n\n\n\n\nIn order to hide variable or statistic labels, you leave the label specification blank (\nvariable =\u2018 \u2019\n). \n\n\nTo \nchange the font color\n of each variable you can define their style as in the following example:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nPROC TABULATE DATA=sashelp.class;\n   CLASS Age Sex;\n   TABLES Age,\n          Sex=\nGender\n *(N*{STYLE=[COLOR=Black]}\n                  PCTN=\nPercent\n   *{STYLE=[COLOR=Green]}\n                  ROWPCTN=\nRow Percent\n*{STYLE=[COLOR=Purple]}\n                  COLPCTN=\nColumn Percent\n *{STYLE=[COLOR=Red]});\nRUN;\n\n\n\n\n\n\n\n\n\n\nYou can also \nspecify formats\n for numbers in the cells of the table using the \nvariable-or-statistic*F=fmt.\n expression.\n\n\nThe \nCLASSLEV\n statement is used to assign some style attributes to the variable values only (not to the column header)\n\n\nThe \nNOSEPS\n option \nremoves the horizontal dividers\n between the row values from your table\n\n\n\n\n1\nPROC TABULATE DATA=SAS-data-set NOSEPS;\n\n\n\n\n\n\n\n\nINDENT=\n is used for subsetting row subheaders\n\n\nRTS=\n specifies how wide you want the row header field to be\n\n\nUse the \nBOX=\n option to \nfill in the big white box in the upper left\n\n\n\n\n1\nTABLE (...) / BOX={LABEL=\nCustom label for upper left box\n} INDENT=3 RTS=12;\n\n\n\n\n\n\nDepending on where you place the style options, many different results can be achieved. If you place the style options on the \nPROC TABULATE\n statement, for example, you will affect all the table cells. Note that for the \nCLASS\n, \nCLASSLEV\n, \nVAR\n, and \nKEYWORD\n statements, the style options can also be specified in the dimension expression in the Table statement. See below for a list of some of the different places where you can put the style options and what portion of the table they will affect.\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nPROC TABULATE DATA=SAS-data-set F=10.2 S=[custom style attributes]; \n    CLASS variable1 / S=[custom style attributes];\n    CLASSLEV variable1 / S=[custom style attributes];\n    VAR variable2;\n    TABLE variable1=\n all={label=\nTotal\n S=[custom style attributes], \n          MEAN={S=[custom style attributes]} * variable2 \n          / BOX={LABEL=\ncustom label\n S=[custom style attributes]};\nRUN;\n\n\n\n\n\n\n\n\nPossible style attributes\n\n\n\n\nBackground color: \nBACKGROUND=yellow\n \n\n\nForeground color: \nFOREGROUND=black\n\n\nFont color: \nCOLOR=red\n\n\nChange font characteristics: \nFONTt_WEIGHT\n|\nFONT_FACE\n|\nFONT_SIZE\n\n\nVertical justification: \nVJUST=B|C|T\n\n\nHorizontal justification: \nJUST=R|C|L\n \n\n\nSpecify thickness of borders: \nBORDERWIDTH=\n\n\nChange size of table cells: \nCELLWIDTH=200\n|\nCELLHEIGHT=50\n\n\nSpecify vertical and horizontal rule dividers: \nRULES=none\n (removes all ines from the table)\n\n\nSpecify white space around cell: \nCELLSPACING=0\n \n\n\nSpecify thickness of spacing around cell: \nCELLPADDING=10\n\n\nSpecify width of table: \nOUTPUTWIDTH=\n \n\n\n\n\n\n\nExamples\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC TABULATE DATA=SAS-data-set ORDER=FREQ;\n    VAR var1 var2;\n    CLASS AEencoding;\n    CLASS grade / ORDER=FORMATTED;\n    CLASS treatment / ORDER=FORMATTED;\n    TABLE AEencoding=\n, treatment=\nTreatment/Grade\n*grade=\n*(N=\nN\n var1=\n%\n*SUM=\n) ALL=\nTotal (N=# cases)\n*(N=\nN\n var2=\n%\n*SUM=\n) / BOX=\nPreferred MeDDRA Term\n;\nRUN;\n\n\n\n\n\n\nPROC REPORT\n\n\nHow to write a header/footer in your tables:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nODS ESCAPECHAR=\n^\n;\n\nPROC REPORT DATA=sashelp.cars;\n    WHERE Make = \nJaguar\n;\n    COLUMN (\n1) Label 1\n model Invoice)\n            (\n2) Label 2\n Horsepower Weight Length);\n    COMPUTE BEFORE _PAGE_ / STYLE=HEADER{JUST=L FONTWEIGHT=BOLD COLOR=PURPLE};\n        LINE \nTest of custom header\n;\n    ENDCOMP;\n    COMPUTE AFTER / STYLE={TEXTDECORATION=UNDERLINE JUST=C COLOR=RED};\n        LINE \nTest of a custom footer\n;\n        LINE \n^S={color=green} Test of a custom footer with a different style\n;\n    ENDCOMP;\nRUN;\n\n\n\n\n\n\nSpecify the \nSTYLE\n of a cell based on other cell's value:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nPROC REPORT DATA=SAS-data-set NOWD;\n    COLUMN timeinterval date1 date2;\n    DEFINE timeinterval / DISPLAY NOPRINT; \n    DEFINE date1 / DISPLAY;\n    DEFINE date2 / DISPLAY;\n    COMPUTE date2;\n        IF timeinterval lt 0 and timeinterval ne . then call define(_col_,\nstyle\n,\nstyle={foreground=red font_weight=bold}\n);\n        ELSE call define(_col_,\nstyle\n,\nstyle={foreground=green font_weight=bold}\n);\n    ENDCOMP;\nRUN;\n\n\n\n\n\n\n\n\nDEFINE\n the variables involved in your conditional structure before the variable to which you want to apply the new format \n\n\nDEFINE\n your variables as \nDISPLAY NOPRINT\n if you want to use them for the conditional structure but you don't want them to appear in your table\n\n\n\n\nSpecify the \nSTYLE\n of your global header:\n\n\n1\n2\n3\n4\n5\nPROC REPORT DATA=SAS-data-set HEADSKIP HEADLINE NOWINDOWS STYLE(header)={ASIS=on BACKGROUND=very light grey FONTWEIGHT=BOLD};\n    COLUMN (\nStyle of this global header\n var1 var2);\n    DEFINE var1 / DISPLAY \nParameters\n LEFT STYLE=[FONTWEIGHT=BOLD];\n    DEFINE var2 / DISPLAY \nValues\n CENTER;\nRUN;\n\n\n\n\n\n\n\n\nCheck these websites\n\n\n\n\nBeyond the Basics: Advanced \nPROC REPORT\n Tips and Tricks\n\n\nCreating a Plan for Your Reports and Avoiding Common Pitfalls in \nREPORT\n Procedure Coding\n\n\nTurn Your Plain Report into a Painted Report Using ODS Styles", 
            "title": "Reports"
        }, 
        {
            "location": "/sas-outputs/reports/#ods", 
            "text": "In order to produce outputs from SAS, the three more common  ODS  techniques, that produces different output files, are HTML, RTF, and PDF. Each  ODS  statement uses options that are specific to that destination. The  ODS  options (other than the  FILE=  option) used in the program are shown in the table below.      RTF  PDF  HTML      BODYTITLE     STARTPAGE=NO     KEEPN     NOTOC_DATA  /  TOC_DATA     CONTENTS     COLUMNS=     TEXT=  BOOKMARKGEN=NO     STARTPAGE=NO     COMPRESS=9     TEXT=  STYLE=SASWEB     RS=NONE     For an explanation of the options, refer to  this page  or to the  ODS  User's Guide .  Set the SAS system options.  The  NODATE  option suppresses the display of the date and time in the output.  PAGENO=  specifies the starting page number.  LINESIZE=  specifies the output line length, and  PAGESIZE=  specifies the number of lines on an output page.  1 OPTIONS NODATE PAGENO=1 LINESIZE=80 PAGESIZE=40;", 
            "title": "ODS"
        }, 
        {
            "location": "/sas-outputs/reports/#proc-print", 
            "text": "", 
            "title": "PROC PRINT"
        }, 
        {
            "location": "/sas-outputs/reports/#proc-tabulate", 
            "text": "PROC TABULATE  is a procedure that displays descriptive statistics in tabular format. It computes many statistics that other procedures compute, such as  MEANS ,  FREQ , and  REPORT  and displays these statistics in a table format.  TABULATE  will produce tables in up to three dimensions and allows, within each dimension, multiple variables to be reported one after another hierarchically. There are also some very nice mechanisms that can be used to label and format the results.   1\n2\n3\n4\n5\n6 PROC TABULATE  options ;\n    CLASS variables  / options ;\n    VAR variables  / options ;\n    TABLE  page ,  row , column  / options ;\n    ... other statements...;\nRUN;    VAR  is used to list the variables you intend to use to create summary statistics on. They  must be numeric .  CLASS  variables allow you to get statistics by category. You will get one column/row for each value of the classification variable. You can also specify the universal CLASS variable  ALL  which allows you to  get totals . They can be  either numeric or character  and you can only request counts and percents as statistics. This is almost like using a  BY statement within the  TABLE .  TABLE  consists of up to three dimension expressions and the table options. You can have multiple table statements in one  PROC TABULATE . This will generate one table for each statement.  A  comma  specifies to add a new  dimension . The order of the dimensions is page, row and column. If you only specify one dimension, then it is assumed to be column. If two are specified, row, then column.  The  asterisk  is used to produce a  cross tabulation  of one variable with another (within the same dimension however, different from  PROC FREQ ).  A  blank  is used to represent  concatenation  (i.e. place this output element after the preceding variable listed).  Parenthesis  will  group elements  and associate an operator with each element in the group.  Angle brackets  specify a  denominator definition  for use in percentage calculations.       Check these websites   PROC TABULATE  and the Neat Things You Can Do With It     VARDEF=divisor  specifies the divisor to be used in the calculation of the variances. If divisor is  DF  (default), the degrees of freedom (N-1) are used as the divisor.  ORDER=  specifies the order of appearance in the table of the  CLASS  variable levels  FORMATTED : ordered by the formatted values  DATA : the order that the observations are read from the data set  FREQ : order the values so the one that occurs most frequently in the data set appears first  INTERNAL : ordered by the  SORT  procedure (defaults)    /CONDENSE  prints multiple logical pages on a single physical page  /PRINTMISS  species that row and column headings are the same for all logical pages of the table  /ROW = spacing  specifies whether all title elements in a row crossing are allotted space even when they are blank. When  ROW=CONSTANT  (or  CONST ), the default, all row title elements have space allotted to them; when  ROW=FLOAT , the row title space is divided equally among the nonblank title elements in the crossing", 
            "title": "PROC TABULATE"
        }, 
        {
            "location": "/sas-outputs/reports/#available-statistics", 
            "text": "If you do not provide a statistic name, the default statistic produced will be  N  for the  CLASS  variables and  SUM  for the  VAR  variables.", 
            "title": "Available Statistics"
        }, 
        {
            "location": "/sas-outputs/reports/#single-dimensional-table", 
            "text": "1\n2\n3\n4\n5 PROC TABULATE DATA=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Height * (N MEAN) Height * MEAN * Sex Weight * MEAN * Sex;\nRUN;", 
            "title": "Single Dimensional Table"
        }, 
        {
            "location": "/sas-outputs/reports/#two-dimensional-table", 
            "text": "You can get very different table structures by changing where the statistic definitions are placed. They can be attached to either the  VAR  or the  CLASS  variable, but the numbers in the cells will  always  be calculated using the  VAR  variable(s).   The statistic specification can be  attached to the columns ,  1\n2\n3\n4\n5 PROC TABULATE data=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Sex, Height * (N MEAN MAX) Weight * (N MEAN MAX) ;\nRUN;     or they can be  attached to the rows .  1\n2\n3\n4\n5 PROC TABULATE data=sashelp.class;\n    CLASS Sex;\n    VAR Height Weight;\n    TABLE Sex * (N MEAN MAX), Height  Weight;\nRUN;     You can specify  multiple classification variables . They can be used in any of the dimensions and can be nested. When you have multiple  CLASS  variables, it is recommended to use the option  MISSING  to keep the observations that have any missing values and consider them as valid levels for the  CLASS  variable(s) instead of dropping those observations from all the tables.  1\n2\n3\n4\n5 PROC TABULATE DATA=sashelp.cars;\n    CLASS DriveTrain Origin Type / MISSING;\n    VAR Weight Length;\n    TABLE Origin * Type, DriveTrain * Weight * MEAN DriveTrain * Length * MEAN;\nRUN;     In order to get  marginal statistics  in your table, you use the  ALL  keyword. You can use the keyword in multiple places and, depending on where you put the keyword, there will be different subtotals produced.   1\n2\n3\n4 PROC TABULATE DATA=sashelp.cars;\n    CLASS DriveTrain Origin Type;\n    TABLE (Origin All= Total ) * (DriveTrain ALL= Subtotal DriveTrain ), (Type ALL= Subtotal Type * N);\nRUN;", 
            "title": "Two Dimensional Table"
        }, 
        {
            "location": "/sas-outputs/reports/#three-dimensional-table", 
            "text": "Three dimensional tables have a nice way to fill in the upper left area. Instead of the label of the page dimension appearing above the table, you can use the  BOX=_page_  option to place that label inside the big white box.  1\n2\n3\n4\n5 PROC TABULATE data=sashelp.cars;\n    CLASS DriveTrain Origin Type;\n    VAR Weight Length;\n    TABLE Origin =  Made in , Type =  Category , DriveTrain * Weight * MEAN / BOX=_PAGE_;\nRUN;", 
            "title": "Three Dimensional Table"
        }, 
        {
            "location": "/sas-outputs/reports/#formatting-tables", 
            "text": "There are two ways to  add labels for your variables :   Add the text after the variable name:  variable =\u2018label\u2019 . This will work for both variables and\nstatistics.  Add a  LABEL  statement for variables and/or a  KEYLABEL  statement for statistics to your code:   1\n2 LABEL var=\u2018label\u2019;\nKEYLABEL stat=\u2018label\u2019;   In order to hide variable or statistic labels, you leave the label specification blank ( variable =\u2018 \u2019 ).   To  change the font color  of each variable you can define their style as in the following example:  1\n2\n3\n4\n5\n6\n7\n8 PROC TABULATE DATA=sashelp.class;\n   CLASS Age Sex;\n   TABLES Age,\n          Sex= Gender  *(N*{STYLE=[COLOR=Black]}\n                  PCTN= Percent    *{STYLE=[COLOR=Green]}\n                  ROWPCTN= Row Percent *{STYLE=[COLOR=Purple]}\n                  COLPCTN= Column Percent  *{STYLE=[COLOR=Red]});\nRUN;     You can also  specify formats  for numbers in the cells of the table using the  variable-or-statistic*F=fmt.  expression.  The  CLASSLEV  statement is used to assign some style attributes to the variable values only (not to the column header)  The  NOSEPS  option  removes the horizontal dividers  between the row values from your table   1 PROC TABULATE DATA=SAS-data-set NOSEPS;    INDENT=  is used for subsetting row subheaders  RTS=  specifies how wide you want the row header field to be  Use the  BOX=  option to  fill in the big white box in the upper left   1 TABLE (...) / BOX={LABEL= Custom label for upper left box } INDENT=3 RTS=12;   Depending on where you place the style options, many different results can be achieved. If you place the style options on the  PROC TABULATE  statement, for example, you will affect all the table cells. Note that for the  CLASS ,  CLASSLEV ,  VAR , and  KEYWORD  statements, the style options can also be specified in the dimension expression in the Table statement. See below for a list of some of the different places where you can put the style options and what portion of the table they will affect.   1\n2\n3\n4\n5\n6\n7\n8 PROC TABULATE DATA=SAS-data-set F=10.2 S=[custom style attributes]; \n    CLASS variable1 / S=[custom style attributes];\n    CLASSLEV variable1 / S=[custom style attributes];\n    VAR variable2;\n    TABLE variable1=  all={label= Total  S=[custom style attributes], \n          MEAN={S=[custom style attributes]} * variable2 \n          / BOX={LABEL= custom label  S=[custom style attributes]};\nRUN;    Possible style attributes   Background color:  BACKGROUND=yellow    Foreground color:  FOREGROUND=black  Font color:  COLOR=red  Change font characteristics:  FONTt_WEIGHT | FONT_FACE | FONT_SIZE  Vertical justification:  VJUST=B|C|T  Horizontal justification:  JUST=R|C|L    Specify thickness of borders:  BORDERWIDTH=  Change size of table cells:  CELLWIDTH=200 | CELLHEIGHT=50  Specify vertical and horizontal rule dividers:  RULES=none  (removes all ines from the table)  Specify white space around cell:  CELLSPACING=0    Specify thickness of spacing around cell:  CELLPADDING=10  Specify width of table:  OUTPUTWIDTH=", 
            "title": "Formatting tables"
        }, 
        {
            "location": "/sas-outputs/reports/#examples", 
            "text": "1\n2\n3\n4\n5\n6\n7 PROC TABULATE DATA=SAS-data-set ORDER=FREQ;\n    VAR var1 var2;\n    CLASS AEencoding;\n    CLASS grade / ORDER=FORMATTED;\n    CLASS treatment / ORDER=FORMATTED;\n    TABLE AEencoding= , treatment= Treatment/Grade *grade= *(N= N  var1= % *SUM= ) ALL= Total (N=# cases) *(N= N  var2= % *SUM= ) / BOX= Preferred MeDDRA Term ;\nRUN;", 
            "title": "Examples"
        }, 
        {
            "location": "/sas-outputs/reports/#proc-report", 
            "text": "How to write a header/footer in your tables:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 ODS ESCAPECHAR= ^ ;\n\nPROC REPORT DATA=sashelp.cars;\n    WHERE Make =  Jaguar ;\n    COLUMN ( 1) Label 1  model Invoice)\n            ( 2) Label 2  Horsepower Weight Length);\n    COMPUTE BEFORE _PAGE_ / STYLE=HEADER{JUST=L FONTWEIGHT=BOLD COLOR=PURPLE};\n        LINE  Test of custom header ;\n    ENDCOMP;\n    COMPUTE AFTER / STYLE={TEXTDECORATION=UNDERLINE JUST=C COLOR=RED};\n        LINE  Test of a custom footer ;\n        LINE  ^S={color=green} Test of a custom footer with a different style ;\n    ENDCOMP;\nRUN;   Specify the  STYLE  of a cell based on other cell's value:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 PROC REPORT DATA=SAS-data-set NOWD;\n    COLUMN timeinterval date1 date2;\n    DEFINE timeinterval / DISPLAY NOPRINT; \n    DEFINE date1 / DISPLAY;\n    DEFINE date2 / DISPLAY;\n    COMPUTE date2;\n        IF timeinterval lt 0 and timeinterval ne . then call define(_col_, style , style={foreground=red font_weight=bold} );\n        ELSE call define(_col_, style , style={foreground=green font_weight=bold} );\n    ENDCOMP;\nRUN;    DEFINE  the variables involved in your conditional structure before the variable to which you want to apply the new format   DEFINE  your variables as  DISPLAY NOPRINT  if you want to use them for the conditional structure but you don't want them to appear in your table   Specify the  STYLE  of your global header:  1\n2\n3\n4\n5 PROC REPORT DATA=SAS-data-set HEADSKIP HEADLINE NOWINDOWS STYLE(header)={ASIS=on BACKGROUND=very light grey FONTWEIGHT=BOLD};\n    COLUMN ( Style of this global header  var1 var2);\n    DEFINE var1 / DISPLAY  Parameters  LEFT STYLE=[FONTWEIGHT=BOLD];\n    DEFINE var2 / DISPLAY  Values  CENTER;\nRUN;    Check these websites   Beyond the Basics: Advanced  PROC REPORT  Tips and Tricks  Creating a Plan for Your Reports and Avoiding Common Pitfalls in  REPORT  Procedure Coding  Turn Your Plain Report into a Painted Report Using ODS Styles", 
            "title": "PROC REPORT"
        }, 
        {
            "location": "/sas-outputs/graphs/", 
            "text": "Check these websites\n\n\n\n\nHere\n are some examples of complex graphs.\n\n\nHere\n there are instructions to play with the axis' attributes.\n\n\nGraphically speaking\n blog with useful tips for graphics.\n\n\n\n\n\n\nBasic \nODS\n Options\n\n\nYou need to add this command to get the plots displayed in the output:\n\n\n1\n2\n3\nODS GRAPHICS ON;\n[your code here]\nODS GRAPHICS OFF;\n\n\n\n\n\n\nWhen you add the \nODS TRACE\n statement, SAS writes a trace record to the log that includes information about each output object (name, label, template, path):\n\n\n1\n2\n3\nODS TRACE ON;\n[your code here]\nODS TRACE OFF;\n\n\n\n\n\n\nYou produce a list of the possible output elements in the log that you may specify in the \nODS SELECT/EXCLUDE\n statement:\n\n\n1\n2\n3\nODS SELECT output-name1 output-name2 output-name3;\n[your code here]\nODS SELECT ALL;  /* Reset this option to the default */\n\n\n\n\n\n\nYo can keeps some of the outputs in SAS-data-sets:\n\n\n1\nODS OUTPUT output-name1=generated-data-set1 output-name1=generated-data-set2 output-name1=generated-data-set3;\n\n\n\n\n\n\n\n\n\n\nRemove date and pagination from the automatic output header:\n\n\n\n\n1\nOPTIONS NODATE NONUMBER;\n\n\n\n\n\n\n\n\nRemove graph's external borders:\n\n\n\n\n1\nODS GRAPHICS / NOBORDER;\n\n\n\n\n\n\nPlots\n\n\nGPLOT\n\n\n\n\nReference lines:\n\n\n\n\n1\n2\n3\n4\n5\n6\nSYMBOL1 COLOR=blue INTERPOL=join;\nAXIS1 LABEL=(\nX axis label\n) order=(0 to 15 by 1) reflabel=(j=c h=9pt \nReference line label 1\n \nReference line label 2\n \nReference line label 3\n);\nAXIS2 LABEL=(\nY axis label\n j=c);\nPROC GPLOT DATA=SAS-data-set;\n    PLOT variabley*variablex / HAXIS=AXIS1 VAXIS=AXIS2 HREF=6 9 13 /*location of ref lines*/;\nRUN;\n\n\n\n\n\n\nSGPLOT\n\n\n\n\nHighlight a certain boxplot and get the plot narrower: \n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nPROC SGPLOT DATA=sashelp.heart;\n    /* The order matters: first thing defined goes to the back */\n    REFLINE \nCoronary Heart Disease\n / AXIS=x \n        LINEATTRS=(THICKNESS=70 COLOR=yellow) TRANSPARENCY=0.5 ;\n    VBOX cholesterol / CATEGORY=deathcause;\n    XAXIS OFFSETMIN=0.25 OFFSETMAX=0.25 DISCRETEORDER=data;\n    YAXIS GRID;\nRUN;\n\n\n\n\n\n\n\n\nSpecify the colors of groups in SAS statistical graphics\n\n\n\n\nMiscellanea\n\n\nAvailable Colors at the SAS Registry\n\n\nYou can check the \nlist of SAS predefined colors\n and even list it using the SAS registry:\n\n\n1\n2\nPROC REGISTRY LIST STARTAT=\n\\COLORNAMES\\HTML\n; \nRUN; \n\n\n\n\n\n\n\n\nCheck this website\n\n\n\n\nUsing the SAS Registry to Control Color", 
            "title": "Graphs and Plots"
        }, 
        {
            "location": "/sas-outputs/graphs/#basic-ods-options", 
            "text": "You need to add this command to get the plots displayed in the output:  1\n2\n3 ODS GRAPHICS ON;\n[your code here]\nODS GRAPHICS OFF;   When you add the  ODS TRACE  statement, SAS writes a trace record to the log that includes information about each output object (name, label, template, path):  1\n2\n3 ODS TRACE ON;\n[your code here]\nODS TRACE OFF;   You produce a list of the possible output elements in the log that you may specify in the  ODS SELECT/EXCLUDE  statement:  1\n2\n3 ODS SELECT output-name1 output-name2 output-name3;\n[your code here]\nODS SELECT ALL;  /* Reset this option to the default */   Yo can keeps some of the outputs in SAS-data-sets:  1 ODS OUTPUT output-name1=generated-data-set1 output-name1=generated-data-set2 output-name1=generated-data-set3;     Remove date and pagination from the automatic output header:   1 OPTIONS NODATE NONUMBER;    Remove graph's external borders:   1 ODS GRAPHICS / NOBORDER;", 
            "title": "Basic ODS Options"
        }, 
        {
            "location": "/sas-outputs/graphs/#plots", 
            "text": "", 
            "title": "Plots"
        }, 
        {
            "location": "/sas-outputs/graphs/#gplot", 
            "text": "Reference lines:   1\n2\n3\n4\n5\n6 SYMBOL1 COLOR=blue INTERPOL=join;\nAXIS1 LABEL=( X axis label ) order=(0 to 15 by 1) reflabel=(j=c h=9pt  Reference line label 1   Reference line label 2   Reference line label 3 );\nAXIS2 LABEL=( Y axis label  j=c);\nPROC GPLOT DATA=SAS-data-set;\n    PLOT variabley*variablex / HAXIS=AXIS1 VAXIS=AXIS2 HREF=6 9 13 /*location of ref lines*/;\nRUN;", 
            "title": "GPLOT"
        }, 
        {
            "location": "/sas-outputs/graphs/#sgplot", 
            "text": "Highlight a certain boxplot and get the plot narrower:    1\n2\n3\n4\n5\n6\n7\n8 PROC SGPLOT DATA=sashelp.heart;\n    /* The order matters: first thing defined goes to the back */\n    REFLINE  Coronary Heart Disease  / AXIS=x \n        LINEATTRS=(THICKNESS=70 COLOR=yellow) TRANSPARENCY=0.5 ;\n    VBOX cholesterol / CATEGORY=deathcause;\n    XAXIS OFFSETMIN=0.25 OFFSETMAX=0.25 DISCRETEORDER=data;\n    YAXIS GRID;\nRUN;    Specify the colors of groups in SAS statistical graphics", 
            "title": "SGPLOT"
        }, 
        {
            "location": "/sas-outputs/graphs/#miscellanea", 
            "text": "", 
            "title": "Miscellanea"
        }, 
        {
            "location": "/sas-outputs/graphs/#available-colors-at-the-sas-registry", 
            "text": "You can check the  list of SAS predefined colors  and even list it using the SAS registry:  1\n2 PROC REGISTRY LIST STARTAT= \\COLORNAMES\\HTML ; \nRUN;     Check this website   Using the SAS Registry to Control Color", 
            "title": "Available Colors at the SAS Registry"
        }, 
        {
            "location": "/sas-outputs/templates/", 
            "text": "Summary\n\n\n\n\n'Check these websites'\n      * \nHere\n you can find some notes on \nGraph Template Language\n (categories of statements)\n      * \nBob Rodr\u00edguez\n is has written a lot about templates, check his papers for more information\n      * \nHere\n you can find the official documentation on ODS Graphics Template Modification\n      * \nPROC TEMPLATE style tips\n\n\nStyle Templates vs Graph Templates\n\n\nModifying Style Templates\n\n\n\n\nObtain the source code\n\n\n\n\n1\n2\n3\nPROC TEMPLATE;\nSOURCE styles.default;\nRUN;\n\n\n\n\n\n\n\n\nModify the code\n\n\n\n\n1\n2\n3\n4\n5\n6\nPROC TEMPLATE;\n    DEFINE STYLE MyListingStyle;\n    PARENT=styles.listing;\n        [make desired changes in code]\n    END;\nRUN;\n\n\n\n\n\n\n\n\nGenerate the plot\n\n\n\n\n1\n2\nODS\n \nLISTING\n \nSTYLE\n=\nmylistingstyle\n;\n\n\n[\nSGPLOT\n \nStatements\n]\n\n\n\n\n\n\n\nModifying Graph Templates\n\n\n\n\n\n\nObtain the source code\n    \nPROC TEMPLATE;\n            SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival;\n    RUN;\n\n\n\n\n\n\nModify the code\n    \nPROC TEMPLATE;\n            DEFINE Stat.Lifetest.Graphics.ProductLimitSurvival;\n            SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival;\n            [make desired changes in code]\n        END;\n    RUN;\n\n\n\n\n\n\nGenerate the plot\n    \nPROC LIFETEST DATA=db PLOTS=S;\n            [statements]\n    RUN;\n\n\n\n\n\n\nRevert to default template\n    \nPROC TEMPLATE;\n        DELETE Stat.Lifetest.Graphics.ProductLimitSurvival;\n    RUN;\n\n\n\n\n\n\nBasic Graph Template Functionalities\n\n\nObtaining the Default Templates\n\n\nFirst you need to know the name of the template. For this you can either look for its name listing all the available default templates that are kept in \nsashelp.tmplmst\n...\n\n\n1\n2\n3\n4\nPROC TEMPLATE;\n  PATH sashelp.tmplmst;\n  LIST Base.Freq / SORT=path DESCENDING;\nRUN; \n\n\n\n\n\n\n... or use the \nODS TRACE ON\n to obtain the name of an specific template.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nODS TRACE ON;\nODS GRAPHICS ON;\nPROC FREQ DATA=sashelp.baseball;\n    TABLE League*Division / AGREE NOCOL NOROW; \n    TEST KAPPA;\nRUN;\nODS GRAPHICS OFF;\nODS TRACE OFF;\n\n\n\n\n\n\nYou will obtain the following log output for the \nAgreement Plot\n where you can obtain the name of the template you are interested in:\n\n\n1\n2\n3\n4\n5\n6\n7\nOutput Added:\n\n\n-------------\n\nName:       AgreePlot\nLabel:      Agreement Plot\nTemplate:   Base.Freq.Graphics.AgreePlot\n\nPath:       Freq.Table1.AgreePlot\n\n\n-------------\n\n\n\n\n\n\n\nThen you use the \nSOURCE\n option from the \nTEMPLATE\n procedure to show in the log the full object template.\n\n\n1\n2\n3\n4\n%let\n \npath\n=\nC\n:\n\\\nyour\n-\npath\n-\nhere\n;\n\n\nPROC\n \nTEMPLATE\n;\n\n    \nSOURCE\n \nBase\n.\nFreq\n.\nGraphics\n.\nAgreePlot\n \n/\n \nfile\n=\npath.\n\\a\ngreeplot.sas\n;\n\n\nRUN\n;\n\n\n\n\n\n\n\n\n\nNote\n\n\nRemember that you must add a \nPROC TEMPLATE;\n statement before the generated source statements and optionally a \nRUN;\n statement after the \nEND;\n statement before you submit your modified definition.\n\n\n\n\nEditing Templates\n\n\nGraph definitions are self-contained and do not support inheritance (via the \nPARENT=\n option) as do table definitions. Consequently, the \nEDIT\n statement in \nPROC TEMPLATE\n is not supported for graph definitions.\n\n\nHere are some important points about what you can and cannot change in a template:\n\n\n\n\nDo not change the template name\n. A statistical procedure can access only a predefined list of templates. If you change the name, the procedure cannot find your template. You must make sure that it is in a template store that is read before \nSashelp.Tmplmst\n through the \nODS PATH\n statement.\n\n\nDo not change the names of columns\n. The underlying data object contains predefined column names that you must use. Be very careful if you change how a column is used in a template. Usually, columns are not interchangeable.\n\n\nDo not change the names of \nDYNAMIC\n variables\n. Changing dynamic variable names can lead to runtime errors. Do not add dynamic variables, because the procedure cannot set their values.\n\n\nDo not change the names of statements\n (for example, from a \nSCATTERPLOT\n to a \nNEEDLEPLOT\n or other type of plot).\n\n\n\n\nYou can change any of the following:\n\n\n\n\nYou can add macro variables that behave like dynamic variables\n. They are resolved at the time that the statistical procedure is run, and not at the time that the template is compiled. They are defined with an \nMVAR\n or \nNMVAR\n statement at the beginning the template. You can also move a variable from a \nDYNAMIC\n statement to an \nMVAR\n or \nNMVAR\n statement if you want to set it yourself rather than letting the procedure set it.\n\n\nYou can change the graph size\n.\n\n\nYou can change graph titles, footnotes, axis labels, and any other text that appears in the graph\n.\n\n\nYou can change which plot features are displayed\n.\n\n\nYou can change axis features, such as grid lines, offsets, view ports, tick value formatting, and so on\n.\n\n\nYou can change the content and arrangement of insets\n (small tables of statistics embedded in some graphs).\n\n\nYou can change the legend location, contents, border, background, title, and so on\n.\n\n\n\n\nUsing Customized Templates\n\n\nThe \nODS PATH\n statement specifies the template stores to search, as well as the order in which to search them. You can change the default template search path by using the \nODS PATH\n statement.\n\n\n1\nODS PATH work.mystore(update) sashelp.tmplmst(read);\n\n\n\n\n\n\nYou can display the current template search path with the following statement:\n\n\n1\nODS PATH SHOW;\n\n\n\n\n\n\nThe log messages for the default template search path are as follows:\n\n\n1\n2\n3\n4\nCurrent ODS PATH list is:\n\n1. WORK.MYSTORE(UPDATE)\n2. SASHELP.TMPLMST(READ)\n\n\n\n\n\n\nWhen you are done, you can reset the default template search path as follows:\n\n\n1\nODS PATH RESET;\n\n\n\n\n\n\n1\n2\n3\n4\nCurrent ODS PATH list is:\n\n1. SASUSER.TEMPLAT(UPDATE)\n2. SASHELP.TMPLMST(READ)\n\n\n\n\n\n\nReverting to the Default Templates\n\n\nThe following statements delete the modified template from \nSASUSER.TEMPLAT\n and revert to the default template in\n\nSASHELP.TMPLMST\n, which is where the SAS templates are stored.\n\n\n1\n2\n3\nPROC TEMPLATE;\n    DELETE Base.Freq.Graphics.AgreePlot;\nRUN;\n\n\n\n\n\n\nThe following note is printed in the SAS log:\n\n\n1\nNOTE: \nBase.Freq.Graphics.AgreePlot\n has been deleted from: SASUSER.TEMPLAT\n\n\n\n\n\n\nYou can run the following step to delete the entire \nSASUSER.TEMPLAT\n store of customized templates:\n\n\n1\n2\n3\n4\n5\nODS PATH sashelp.tmplmst(read);\nPROC DATASETS LIBRARY=sasuser NOLIST;\n   DELETE TEMPLAT(MEMTYPE=ITEMSTOR);\nRUN;\nODS PATH sasuser.templat(update) sashelp.tmplmst(read);\n\n\n\n\n\n\nOther Related Topics\n\n\n\n\nSolve the error \n\"\nunable to write to the template store\n\"\n:\n\n\n\n\n1\n2\n3\n4\nERROR\n:\n \nTemplate\n \nxxxxx\n \nwas\n \nunable\n \nto\n \nwrite\n \nto\n \nthe\n \ntemplate\n \nstore\n!\n\n\n\nods\n \npath\n \nshow\n;\n\n\nods\n \npath\n(\nprepend\n)\n \nwork\n.\ntemplat\n(\nupdate\n);", 
            "title": "Templates"
        }, 
        {
            "location": "/sas-outputs/templates/#style-templates-vs-graph-templates", 
            "text": "", 
            "title": "Style Templates vs Graph Templates"
        }, 
        {
            "location": "/sas-outputs/templates/#modifying-style-templates", 
            "text": "Obtain the source code   1\n2\n3 PROC TEMPLATE;\nSOURCE styles.default;\nRUN;    Modify the code   1\n2\n3\n4\n5\n6 PROC TEMPLATE;\n    DEFINE STYLE MyListingStyle;\n    PARENT=styles.listing;\n        [make desired changes in code]\n    END;\nRUN;    Generate the plot   1\n2 ODS   LISTING   STYLE = mylistingstyle ;  [ SGPLOT   Statements ]", 
            "title": "Modifying Style Templates"
        }, 
        {
            "location": "/sas-outputs/templates/#modifying-graph-templates", 
            "text": "Obtain the source code\n     PROC TEMPLATE;\n            SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival;\n    RUN;    Modify the code\n     PROC TEMPLATE;\n            DEFINE Stat.Lifetest.Graphics.ProductLimitSurvival;\n            SOURCE Stat.Lifetest.Graphics.ProductLimitSurvival;\n            [make desired changes in code]\n        END;\n    RUN;    Generate the plot\n     PROC LIFETEST DATA=db PLOTS=S;\n            [statements]\n    RUN;    Revert to default template\n     PROC TEMPLATE;\n        DELETE Stat.Lifetest.Graphics.ProductLimitSurvival;\n    RUN;", 
            "title": "Modifying Graph Templates"
        }, 
        {
            "location": "/sas-outputs/templates/#basic-graph-template-functionalities", 
            "text": "", 
            "title": "Basic Graph Template Functionalities"
        }, 
        {
            "location": "/sas-outputs/templates/#obtaining-the-default-templates", 
            "text": "First you need to know the name of the template. For this you can either look for its name listing all the available default templates that are kept in  sashelp.tmplmst ...  1\n2\n3\n4 PROC TEMPLATE;\n  PATH sashelp.tmplmst;\n  LIST Base.Freq / SORT=path DESCENDING;\nRUN;    ... or use the  ODS TRACE ON  to obtain the name of an specific template.  1\n2\n3\n4\n5\n6\n7\n8 ODS TRACE ON;\nODS GRAPHICS ON;\nPROC FREQ DATA=sashelp.baseball;\n    TABLE League*Division / AGREE NOCOL NOROW; \n    TEST KAPPA;\nRUN;\nODS GRAPHICS OFF;\nODS TRACE OFF;   You will obtain the following log output for the  Agreement Plot  where you can obtain the name of the template you are interested in:  1\n2\n3\n4\n5\n6\n7 Output Added:  ------------- \nName:       AgreePlot\nLabel:      Agreement Plot\nTemplate:   Base.Freq.Graphics.AgreePlot Path:       Freq.Table1.AgreePlot  -------------    Then you use the  SOURCE  option from the  TEMPLATE  procedure to show in the log the full object template.  1\n2\n3\n4 %let   path = C : \\ your - path - here ;  PROC   TEMPLATE ; \n     SOURCE   Base . Freq . Graphics . AgreePlot   /   file = path. \\a greeplot.sas ;  RUN ;     Note  Remember that you must add a  PROC TEMPLATE;  statement before the generated source statements and optionally a  RUN;  statement after the  END;  statement before you submit your modified definition.", 
            "title": "Obtaining the Default Templates"
        }, 
        {
            "location": "/sas-outputs/templates/#editing-templates", 
            "text": "Graph definitions are self-contained and do not support inheritance (via the  PARENT=  option) as do table definitions. Consequently, the  EDIT  statement in  PROC TEMPLATE  is not supported for graph definitions.  Here are some important points about what you can and cannot change in a template:   Do not change the template name . A statistical procedure can access only a predefined list of templates. If you change the name, the procedure cannot find your template. You must make sure that it is in a template store that is read before  Sashelp.Tmplmst  through the  ODS PATH  statement.  Do not change the names of columns . The underlying data object contains predefined column names that you must use. Be very careful if you change how a column is used in a template. Usually, columns are not interchangeable.  Do not change the names of  DYNAMIC  variables . Changing dynamic variable names can lead to runtime errors. Do not add dynamic variables, because the procedure cannot set their values.  Do not change the names of statements  (for example, from a  SCATTERPLOT  to a  NEEDLEPLOT  or other type of plot).   You can change any of the following:   You can add macro variables that behave like dynamic variables . They are resolved at the time that the statistical procedure is run, and not at the time that the template is compiled. They are defined with an  MVAR  or  NMVAR  statement at the beginning the template. You can also move a variable from a  DYNAMIC  statement to an  MVAR  or  NMVAR  statement if you want to set it yourself rather than letting the procedure set it.  You can change the graph size .  You can change graph titles, footnotes, axis labels, and any other text that appears in the graph .  You can change which plot features are displayed .  You can change axis features, such as grid lines, offsets, view ports, tick value formatting, and so on .  You can change the content and arrangement of insets  (small tables of statistics embedded in some graphs).  You can change the legend location, contents, border, background, title, and so on .", 
            "title": "Editing Templates"
        }, 
        {
            "location": "/sas-outputs/templates/#using-customized-templates", 
            "text": "The  ODS PATH  statement specifies the template stores to search, as well as the order in which to search them. You can change the default template search path by using the  ODS PATH  statement.  1 ODS PATH work.mystore(update) sashelp.tmplmst(read);   You can display the current template search path with the following statement:  1 ODS PATH SHOW;   The log messages for the default template search path are as follows:  1\n2\n3\n4 Current ODS PATH list is:\n\n1. WORK.MYSTORE(UPDATE)\n2. SASHELP.TMPLMST(READ)   When you are done, you can reset the default template search path as follows:  1 ODS PATH RESET;   1\n2\n3\n4 Current ODS PATH list is:\n\n1. SASUSER.TEMPLAT(UPDATE)\n2. SASHELP.TMPLMST(READ)", 
            "title": "Using Customized Templates"
        }, 
        {
            "location": "/sas-outputs/templates/#reverting-to-the-default-templates", 
            "text": "The following statements delete the modified template from  SASUSER.TEMPLAT  and revert to the default template in SASHELP.TMPLMST , which is where the SAS templates are stored.  1\n2\n3 PROC TEMPLATE;\n    DELETE Base.Freq.Graphics.AgreePlot;\nRUN;   The following note is printed in the SAS log:  1 NOTE:  Base.Freq.Graphics.AgreePlot  has been deleted from: SASUSER.TEMPLAT   You can run the following step to delete the entire  SASUSER.TEMPLAT  store of customized templates:  1\n2\n3\n4\n5 ODS PATH sashelp.tmplmst(read);\nPROC DATASETS LIBRARY=sasuser NOLIST;\n   DELETE TEMPLAT(MEMTYPE=ITEMSTOR);\nRUN;\nODS PATH sasuser.templat(update) sashelp.tmplmst(read);", 
            "title": "Reverting to the Default Templates"
        }, 
        {
            "location": "/sas-outputs/templates/#other-related-topics", 
            "text": "Solve the error  \" unable to write to the template store \" :   1\n2\n3\n4 ERROR :   Template   xxxxx   was   unable   to   write   to   the   template   store !  ods   path   show ;  ods   path ( prepend )   work . templat ( update );", 
            "title": "Other Related Topics"
        }, 
        {
            "location": "/macros/", 
            "text": "You can learn about macros in the \nSAS Macro Language 1: Essentials course\n.\n\n\nHere\n you have some macro repositories.\n\n\nHow to define \noptional macro arguments\n.\n\n\nRemove element/string from macro variable\n\n\n1\n2\n3\n4\n5\n6\n%put\n \n=\nlist\n;\n     \n/* Check list contents before */\n\n\n\n%let\n \nremovefromlist\n \n=\n \nstring_to_remove\n;\n\n\n%let\n \nlist\n \n=\n \n%sysfunc\n(\ntranwrd\n(\nlist\n.,\n \nremovefromlist\n.,\n \n%str\n()));;\n\n\n\n%put\n \n=\nlist\n;\n     \n/* Check list contents after */\n\n\n\n\n\n\n\nCall a Macro for a List of Variable Names\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n%macro\n \nrunall\n(\nparamlist\n);\n\n \n%let\n \nnum\n \n=\n \n%sysfunc\n(\ncountw\n(\nparamlist\n));\n\n    \n%local\n \ni\n;\n\n    \n%do\n \ni\n \n=\n1\n \n%to\n \nnum\n;\n\n        \n%let\n \nparameter\ni\n \n=\n \n%scan\n(\nparamlist\n,\n \ni\n);\n\n        \n%macro_analysis\n(\nvariablename\n=\nparameter\ni\n);\n\n    \n%end\n;\n\n\n%mend\n;\n\n\n\n%runall\n(\nitem1\n \nitem2\n \nitem3\n \nitem4\n \nitem5\n);\n\n\n\n\n\n\n\nCreate Macrovariable from Data Set Values\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\nDATA\n \n_NULL_\n;\n\n    \nSET\n \nOddsRatios\n;\n\n    \nCALL\n \nSYMPUT\n \n(\nvar1\n,\nOddsRatioEst\n);\n\n    \nCALL\n \nSYMPUT\n \n(\nvar2\n,\nLowerCl\n);\n\n    \nCALL\n \nSYMPUT\n \n(\nvar3\n,\nUpperCL\n);\n\n    \n*\n \nThe\n \nvariables\n \nhave\n \na\n \nlot\n \nof\n \nextra\n \nspaces\n;\n\n    \n%\nLET\n \nOR2report\n=\nOR\n:\n \nvar1\n.\n \n(\nvar2\n.,\nvar3\n.);\n\n    \n%\nPUT\n \nOR2report\n;\n\n\nRUN\n;\n\n\n\nDATA\n \n_NULL_\n;\n\n    \nSET\n \nOddsRatios\n;\n\n    \nlength\n \nOddsRatioEst\n \nLowerCL\n \nUpperCL\n \n7\n;\n\n    \nEstaux\n \n=\n \nint\n(\n1000\n*\nOddsRatioEst\n)/\n1000\n;\n\n    \nLoweraux\n \n=\n \nint\n(\n1000\n*\nLowerCl\n)/\n1000\n;\n\n    \nUpperaux\n \n=\n \nint\n(\n1000\n*\nUpperCL\n)/\n1000\n;\n\n    \n*\n \nThe\n \nextra\n \nblancks\n \nhas\n \nreduced\n \nwith\n \nthe\n \nCATX\n \nfunction\n;\n\n    \nfullOR\n=\nCATX\n(\n \n,\nOR:\n,\nEstaux\n,\n(\n,\nLoweraux\n,\n,\n,\nUpperaux\n,\n)\n);\n\n    \nCALL\n \nSYMPUT\n \n(\nOR2report\n,\nfullOR\n);\n\n    \n%\nPUT\n \nOR2report\n;\n\n\nRUN\n;\n\n\n\n\n\n\n\nMacros Available in SAS\n\n\nCheck this \npowerpoint\n presentation for more tips.\n\n\nColor Utility Macros\n\n\nTo initiate these macros in your current session you call the \n%COLORMAC\n macro.\n\n\nIf you submit the following line:\n\n\n1\n%HELPCLR(HELP);\n\n\n\n\n\n\n\nYou will get a guide of the color utility macros available:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nColor Utility Macros Help              \n\nHELP is currently available for the following macros \n\n        CMY        CMYK       CNS        HLS          \n        HVS        RGB        HLS2RGB    RGB2HLS      \n\nEnter %HELPCLR(macroname) for details on each macro, \nor %HELPCLR(ALL) for details on all macros.   \n\n\n\n\n\n\nSG\n Annotation Macros\n\n\nThey can be used within a \nDATA\n to simplify the process of creating annotation observations.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n  \n%SGARROW\n\n  \n%SGPOLYGON\n\n  \n%SGIMAGE\n\n  \n%SGPOLYLINE\n\n  \n%SGLINE\n\n  \n%SGRECTANGLE\n\n  \n%SGOVAL\n\n  \n%SGTEXT\n\n  \n%SGPOLYCONT\n\n  \n%SGTEXTCONT\n\n\n\n\n\n\n\nTemplate Modification Macros\n\n\n\n\n%MODSTYLE\n macro allows you to easily make changes to style templates without accessing the code\n\n\n%MODTMPLT\n macro allows you to easily make limited changes to graph templates without accessing the code\n\n\n\n\nGraphical Macros\n\n\n\n\n%CompactMatrixMacro\n (Author: Sanjay Matange): it help you modify graphs based on panels\n\n\n%NEWSURV\n macro (Author: Jeff Meyers): it helps you tune the properties of survival plots\n\n\n%FORESTPLOT\n macro (Author: Jeff Meyers): it allows another way of presenting results\n\n\n%EULER_MACRO\n: useful to present proportion Euler diagrams\n\n\n%VENN\n macro: useful to plot intersection between different events\n\n\n%GTLPieChartMacro\n: useful for pie charts\n\n\n\n\nExport Macros\n\n\n\n\n%DS2CSV\n: exports a dataset to *.csv format.\n\n\n\n\nWhere to Find these Macros?\n\n\n\n\nColor utility macros, SGAnnotation macros, \n%MODSTYLE\n and \n%MODTMPLT\n are SAS autocall macros\n\n\n%AXISBREAK\n\n\n%COMPACTMATRIXMACRO\n\n\n%ORTHO3D_MACRO\n\n\n%NEWSURV\n\n\n%FORESTPLOT\n\n\n%EULER_MACRO\n\n\n%VENN\n\n\n%GTLPIECHARTMACRO\n\n\n\n\nMacro examples\n\n\nMacro Program for Creating Box Plots for All of Predictor Variables\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n%let\n \ncategorical\n=\nHouse_Style2\n \nOverall_Qual2\n \nOverall_Cond2\n \nFireplaces\n \n         \nSeason_Sold\n \nGarage_Type_2\n \nFoundation_2\n \nHeating_QC\n \n         \nMasonry_Veneer\n \nLot_Shape_2\n \nCentral_Air\n;\n\n\n/* Macro Usage: %box(DSN = , Response = , CharVar = ) */\n\n\n%macro\n \nbox\n(\ndsn\n      \n=\n \n,\n\n           \nresponse\n \n=\n \n,\n\n           \nCharvar\n  \n=\n \n);\n\n\n%let\n \ni\n \n=\n \n1\n \n;\n\n\n%do\n \n%while\n(\n%scan\n(\ncharvar\n,\ni\n,\n%str\n(\n \n))\n \n^=\n \n%str\n())\n \n;\n\n    \n%let\n \nvar\n \n=\n \n%scan\n(\ncharvar\n,\ni\n,\n%str\n(\n \n));\n\n    \nproc\n \nsgplot\n \ndata\n=\ndsn\n;\n\n        \nvbox\n \nresponse\n \n/\n \ncategory\n=\nvar\n \n                         \ngrouporder\n=\nascending\n \n                         \nconnect\n=\nmean\n;\n\n        \ntitle\n \nresponse across Levels of \nvar\n;\n\n    \nrun\n;\n\n    \n%let\n \ni\n \n=\n \n%eval\n(\ni\n \n+\n \n1\n \n)\n \n;\n\n\n%end\n \n;\n\n\n%mend\n \nbox\n;\n\n\n%box\n(\ndsn\n      \n=\n \nstatdata\n.\nameshousing3\n,\n\n     \nresponse\n \n=\n \nSalePrice\n,\n\n     \ncharvar\n  \n=\n \ncategorical\n);\n\n\ntitle\n;\n\n\noptions\n \nlabel\n;\n\n\n\n\n\n\n\nMacro's Sources\n\n\n\n\nLes macros SAS de Dominique Ladiray", 
            "title": "Using Macros"
        }, 
        {
            "location": "/macros/#remove-elementstring-from-macro-variable", 
            "text": "1\n2\n3\n4\n5\n6 %put   = list ;       /* Check list contents before */  %let   removefromlist   =   string_to_remove ;  %let   list   =   %sysfunc ( tranwrd ( list .,   removefromlist .,   %str ()));;  %put   = list ;       /* Check list contents after */", 
            "title": "Remove element/string from macro variable"
        }, 
        {
            "location": "/macros/#call-a-macro-for-a-list-of-variable-names", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 %macro   runall ( paramlist ); \n  %let   num   =   %sysfunc ( countw ( paramlist )); \n     %local   i ; \n     %do   i   = 1   %to   num ; \n         %let   parameter i   =   %scan ( paramlist ,   i ); \n         %macro_analysis ( variablename = parameter i ); \n     %end ;  %mend ;  %runall ( item1   item2   item3   item4   item5 );", 
            "title": "Call a Macro for a List of Variable Names"
        }, 
        {
            "location": "/macros/#create-macrovariable-from-data-set-values", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21 DATA   _NULL_ ; \n     SET   OddsRatios ; \n     CALL   SYMPUT   ( var1 , OddsRatioEst ); \n     CALL   SYMPUT   ( var2 , LowerCl ); \n     CALL   SYMPUT   ( var3 , UpperCL ); \n     *   The   variables   have   a   lot   of   extra   spaces ; \n     % LET   OR2report = OR :   var1 .   ( var2 ., var3 .); \n     % PUT   OR2report ;  RUN ;  DATA   _NULL_ ; \n     SET   OddsRatios ; \n     length   OddsRatioEst   LowerCL   UpperCL   7 ; \n     Estaux   =   int ( 1000 * OddsRatioEst )/ 1000 ; \n     Loweraux   =   int ( 1000 * LowerCl )/ 1000 ; \n     Upperaux   =   int ( 1000 * UpperCL )/ 1000 ; \n     *   The   extra   blancks   has   reduced   with   the   CATX   function ; \n     fullOR = CATX (   , OR: , Estaux , ( , Loweraux , , , Upperaux , ) ); \n     CALL   SYMPUT   ( OR2report , fullOR ); \n     % PUT   OR2report ;  RUN ;", 
            "title": "Create Macrovariable from Data Set Values"
        }, 
        {
            "location": "/macros/#macros-available-in-sas", 
            "text": "Check this  powerpoint  presentation for more tips.", 
            "title": "Macros Available in SAS"
        }, 
        {
            "location": "/macros/#color-utility-macros", 
            "text": "To initiate these macros in your current session you call the  %COLORMAC  macro.  If you submit the following line:  1 %HELPCLR(HELP);    You will get a guide of the color utility macros available:  1\n2\n3\n4\n5\n6\n7\n8\n9 Color Utility Macros Help              \n\nHELP is currently available for the following macros \n\n        CMY        CMYK       CNS        HLS          \n        HVS        RGB        HLS2RGB    RGB2HLS      \n\nEnter %HELPCLR(macroname) for details on each macro, \nor %HELPCLR(ALL) for details on all macros.", 
            "title": "Color Utility Macros"
        }, 
        {
            "location": "/macros/#sg-annotation-macros", 
            "text": "They can be used within a  DATA  to simplify the process of creating annotation observations.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10    %SGARROW \n   %SGPOLYGON \n   %SGIMAGE \n   %SGPOLYLINE \n   %SGLINE \n   %SGRECTANGLE \n   %SGOVAL \n   %SGTEXT \n   %SGPOLYCONT \n   %SGTEXTCONT", 
            "title": "SG Annotation Macros"
        }, 
        {
            "location": "/macros/#template-modification-macros", 
            "text": "%MODSTYLE  macro allows you to easily make changes to style templates without accessing the code  %MODTMPLT  macro allows you to easily make limited changes to graph templates without accessing the code", 
            "title": "Template Modification Macros"
        }, 
        {
            "location": "/macros/#graphical-macros", 
            "text": "%CompactMatrixMacro  (Author: Sanjay Matange): it help you modify graphs based on panels  %NEWSURV  macro (Author: Jeff Meyers): it helps you tune the properties of survival plots  %FORESTPLOT  macro (Author: Jeff Meyers): it allows another way of presenting results  %EULER_MACRO : useful to present proportion Euler diagrams  %VENN  macro: useful to plot intersection between different events  %GTLPieChartMacro : useful for pie charts", 
            "title": "Graphical Macros"
        }, 
        {
            "location": "/macros/#export-macros", 
            "text": "%DS2CSV : exports a dataset to *.csv format.", 
            "title": "Export Macros"
        }, 
        {
            "location": "/macros/#where-to-find-these-macros", 
            "text": "Color utility macros, SGAnnotation macros,  %MODSTYLE  and  %MODTMPLT  are SAS autocall macros  %AXISBREAK  %COMPACTMATRIXMACRO  %ORTHO3D_MACRO  %NEWSURV  %FORESTPLOT  %EULER_MACRO  %VENN  %GTLPIECHARTMACRO", 
            "title": "Where to Find these Macros?"
        }, 
        {
            "location": "/macros/#macro-examples", 
            "text": "", 
            "title": "Macro examples"
        }, 
        {
            "location": "/macros/#macro-program-for-creating-box-plots-for-all-of-predictor-variables", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 %let   categorical = House_Style2   Overall_Qual2   Overall_Cond2   Fireplaces  \n          Season_Sold   Garage_Type_2   Foundation_2   Heating_QC  \n          Masonry_Veneer   Lot_Shape_2   Central_Air ;  /* Macro Usage: %box(DSN = , Response = , CharVar = ) */  %macro   box ( dsn        =   , \n            response   =   , \n            Charvar    =   );  %let   i   =   1   ;  %do   %while ( %scan ( charvar , i , %str (   ))   ^=   %str ())   ; \n     %let   var   =   %scan ( charvar , i , %str (   )); \n     proc   sgplot   data = dsn ; \n         vbox   response   /   category = var  \n                          grouporder = ascending  \n                          connect = mean ; \n         title   response across Levels of  var ; \n     run ; \n     %let   i   =   %eval ( i   +   1   )   ;  %end   ;  %mend   box ;  %box ( dsn        =   statdata . ameshousing3 , \n      response   =   SalePrice , \n      charvar    =   categorical );  title ;  options   label ;", 
            "title": "Macro Program for Creating Box Plots for All of Predictor Variables"
        }, 
        {
            "location": "/macros/#macros-sources", 
            "text": "Les macros SAS de Dominique Ladiray", 
            "title": "Macro's Sources"
        }, 
        {
            "location": "/wizard/", 
            "text": "Despu\u00e9s de crear un c\u00f3digo de recodificaci\u00f3n con el wizard\nclick derecho en el workflow, create as code, template, save code", 
            "title": "Using Wizard Menus"
        }, 
        {
            "location": "/sasvi/", 
            "text": "Check these websites\n\n\n\n\nVI Tutorials\n\n\n\n\n\n\nHome Page\n\n\nAt the Home Page you can:\n\n\n\n\nClick the icon beside \"Home\" in the banner to access your applications using a side panel\n\n\nAdd application \nshortcuts\n to your Home page (you can customize the color and the name)\n\n\nCreate customized \ncollections\n of documents/projects that you can then share\n\n\nCheck recent projects in the \nRecent\n tile\n\n\nCheck your favorite projects at the \nFavorites\n list (you can Edit $\\rightarrow$ Add a new favorite to the list or include it when you inspect the project by activating the star symbol)\n\n\nCheck your customize list of links in the \nLinks\n tile (you can Edit $\\rightarrow$ Add to include a new one)\n\n\nChange your application settings at the top right corner\n\n\n\n\nData Explorer\n\n\nReport Designer\n\n\nThese are the tipical step that a report author might walk through while creating a report in the designer:\n\n\n\n\nChoose your data source\n\n\nModify data item properties\n\n\nDrag one or more report objects onto the canvas\n\n\nAssign data items to report object roles\n\n\nApply any needed or desired filters\n\n\nUpdate properties for the report\n\n\nUpdate properties and styles for report objects\n\n\nIf desired, add new sections to the report\n\n\nIf required, add interactions between the report objects\n\n\nSave your report\n\n\n\n\nThe minimun steps to create a basic report are 1, 3, 4 and 10.\n\n\nData Preparation\n\n\nReport Viewer", 
            "title": "SAS Visual Analytics"
        }, 
        {
            "location": "/sasvi/#home-page", 
            "text": "At the Home Page you can:   Click the icon beside \"Home\" in the banner to access your applications using a side panel  Add application  shortcuts  to your Home page (you can customize the color and the name)  Create customized  collections  of documents/projects that you can then share  Check recent projects in the  Recent  tile  Check your favorite projects at the  Favorites  list (you can Edit $\\rightarrow$ Add a new favorite to the list or include it when you inspect the project by activating the star symbol)  Check your customize list of links in the  Links  tile (you can Edit $\\rightarrow$ Add to include a new one)  Change your application settings at the top right corner", 
            "title": "Home Page"
        }, 
        {
            "location": "/sasvi/#data-explorer", 
            "text": "", 
            "title": "Data Explorer"
        }, 
        {
            "location": "/sasvi/#report-designer", 
            "text": "These are the tipical step that a report author might walk through while creating a report in the designer:   Choose your data source  Modify data item properties  Drag one or more report objects onto the canvas  Assign data items to report object roles  Apply any needed or desired filters  Update properties for the report  Update properties and styles for report objects  If desired, add new sections to the report  If required, add interactions between the report objects  Save your report   The minimun steps to create a basic report are 1, 3, 4 and 10.", 
            "title": "Report Designer"
        }, 
        {
            "location": "/sasvi/#data-preparation", 
            "text": "", 
            "title": "Data Preparation"
        }, 
        {
            "location": "/sasvi/#report-viewer", 
            "text": "", 
            "title": "Report Viewer"
        }, 
        {
            "location": "/miscellanea/arrays/", 
            "text": "Declaring arrays\n\n\nThe dimension has to be known in advance (???)\nThere's no way to write an implicit loop through all the elements of the array (???)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\ndata _null_;\n\n    ARRAY arrayname[2,3] \n$\n v11-3 (0 0 0)\n                         \n$\n v21-3 (0 0 0);\n\n    DO i=1 TO DIM(arrayname);\n        arrayname[i] = arrayname[i] + 1;\n    END;\n\n    result=CATX(\n,\n,OF v11-3);\n    PUT result=;\n\nRUN;", 
            "title": "Working with Arrays"
        }, 
        {
            "location": "/miscellanea/arrays/#declaring-arrays", 
            "text": "The dimension has to be known in advance (???)\nThere's no way to write an implicit loop through all the elements of the array (???)   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13 data _null_;\n\n    ARRAY arrayname[2,3]  $  v11-3 (0 0 0)\n                          $  v21-3 (0 0 0);\n\n    DO i=1 TO DIM(arrayname);\n        arrayname[i] = arrayname[i] + 1;\n    END;\n\n    result=CATX( , ,OF v11-3);\n    PUT result=;\n\nRUN;", 
            "title": "Declaring arrays"
        }, 
        {
            "location": "/miscellanea/execution-time/", 
            "text": "Measure your code execution time\n\n\n1\n2\n3\n4\n5\n6\n7\n%let\n \ndatetime_start\n \n=\n \n%sysfunc\n(\nTIME\n())\n \n;\n\n\n%put\n \nSTART\n \nTIME\n:\n \n%sysfunc\n(\ndatetime\n(),\ndatetime14\n.);\n\n\n\n[\nYOUR\n \nCODE\n \nHERE\n]\n\n\n\n%put\n \nEND\n \nTIME\n:\n \n%sysfunc\n(\ndatetime\n(),\ndatetime14\n.);\n\n\n%put\n \nTOTAL\n \nTIME\n:\n  \n%sysfunc\n(\nputn\n(\n%sysevalf\n(\n%sysfunc\n(\nTIME\n())\n-\ndatetime_start\n.),\nmmss\n.))\n \n(\nmm\n:\nss\n)\n \n;", 
            "title": "Execution time"
        }, 
        {
            "location": "/miscellanea/execution-time/#measure-your-code-execution-time", 
            "text": "1\n2\n3\n4\n5\n6\n7 %let   datetime_start   =   %sysfunc ( TIME ())   ;  %put   START   TIME :   %sysfunc ( datetime (), datetime14 .);  [ YOUR   CODE   HERE ]  %put   END   TIME :   %sysfunc ( datetime (), datetime14 .);  %put   TOTAL   TIME :    %sysfunc ( putn ( %sysevalf ( %sysfunc ( TIME ()) - datetime_start .), mmss .))   ( mm : ss )   ;", 
            "title": "Measure your code execution time"
        }, 
        {
            "location": "/miscellanea/patient-identification/", 
            "text": "Dealing with Study Identification Numbers\n\n\n\n\nSite calculation from the two first numbers of the patient number:\n\n\n\n\n1\nsite = SUBSTR(PUT(patient,z4.),1,2);\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nPUT\n: turns the numeric variable \npatient\n into a string (\nz4.\n adds leading zeroes if needed)\n\n\nSUBSTR\n: takes the first \n2\n characters starting from position \n1\n\n\n\n\n\n\n\n\nSubtract the patient number (e.g. last 4 characters) from a string:\n\n\n\n\n1\npatient = substr(patient_code,max(1,length(patient_code)-3));\n\n\n\n\n\n\n\n\nJoin the site number and the patient number to get a more general ID number for each patient:\n\n\n\n\n1\npatient = PUT(nsite,z2.) || PUT(npatient,z2.);", 
            "title": "Patient identification"
        }, 
        {
            "location": "/miscellanea/patient-identification/#dealing-with-study-identification-numbers", 
            "text": "Site calculation from the two first numbers of the patient number:   1 site = SUBSTR(PUT(patient,z4.),1,2);    Note   PUT : turns the numeric variable  patient  into a string ( z4.  adds leading zeroes if needed)  SUBSTR : takes the first  2  characters starting from position  1     Subtract the patient number (e.g. last 4 characters) from a string:   1 patient = substr(patient_code,max(1,length(patient_code)-3));    Join the site number and the patient number to get a more general ID number for each patient:   1 patient = PUT(nsite,z2.) || PUT(npatient,z2.);", 
            "title": "Dealing with Study Identification Numbers"
        }
    ]
}
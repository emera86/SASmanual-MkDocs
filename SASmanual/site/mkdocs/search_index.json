{
    "docs": [
        {
            "location": "/", 
            "text": "Hope this site help you with your SAS programming.\n\n\nEnjoy! :)\n\n\n\n\nCheck\n\n\nContact me if you want any content to be added to this brief manual. \nI will be glad to include it.", 
            "title": "Gettin' Started!"
        }, 
        {
            "location": "/essentials/", 
            "text": "Getting Started with SAS Programming\n\n\nChapter summary in SAS\n\n\nWorking with SAS Programs\n\n\nChapter summary in SAS\n\n\nComments\n\n\n1\n2\n/* comment */\n\n\n*\n \ncomment\n \nstatement\n;\n\n\n\n\n\n\n\nAccessing Data\n\n\nChapter summary in SAS\n\n\nAccessing SAS libraries\n\n\n\n\nlibref\n: library reference name (shortcut to the physical location). There are three rules for valid librefs:\n\n\nA length of one to eight characters\n\n\nBegin with a letter or underscore\n\n\nThe remaining characters are letters, numbers, or underscores\n\n\nValid variable names begin with a letter or underscore, and continue with letters, numbers, or underscores. The \nVALIDVARNAME\n system option specifies the rules for valid SAS variable names that can be created and processed during a SAS session: \n\n\n\n\n1\nOPTIONS VALIDVARNAME=V7 (default) | UPCASE | ANY;\n\n\n\n\n\n\n\n\nlibref.data-set-name\n: data set reference two-level name\n\n\ndata-set-name\n: when the data set belongs to a temporary library, you can optionally use a one-level name (SAS assumes that it is contained in the \nwork\n library, which is the default)\n\n\nThe \nLIBNAME\n statement associates the \nlibref\n with the physical location of the library/data for the current SAS session\n\n\n\n\n1\nLIBNAME libref-name \nSAS-library-folder-path\n \noptions\n;\n\n\n\n\n\n\nExample\n\n\n1\n2\n%let\n \npath\n=/\nfolders\n/\nmyfolders\n/\necprg193\n;\n \n\nlibname\n \norion\n \npath\n;\n\n\n\n\n\n\n\n\n\nTo erase the association between SAS and a custom library\n\n\n\n\n1\nLIBNAME libref-name CLEAR;\n\n\n\n\n\n\n\n\nTo check the \ncontents of a library\n programatically\n\n\n\n\n1\n2\nPROC CONTENTS DATA=libref._ALL_;\nRUN;\n\n\n\n\n\n\n\n\nTo hide the descriptors of all data sets in the library (it could generate a very long report) you can add the option \nnods\n (only compatible with the keybord \n_all_\n)\n\n\n\n\n1\n2\nPROC CONTENTS DATA=libref._ALL_ NODS;\nRUN;\n\n\n\n\n\n\n\n\nTo access a data set you can use a \nproc print\n step\n\n\n\n\n1\n2\nPROC PRINT DATA=SAS-data-set;\nRUN;\n\n\n\n\n\n\nExamining SAS Data Sets\n\n\nParts of a library (SAS notation):\n\n\n\n\nTable = \ndata set\n\n\nColumn = \nvariable\n\n\nRow = \nobservation\n\n\n\n\nThe \ndescriptor portion\n (PROC CONTENTS) contains information about the attributes of the data set (metadata), including the variable names. It is show in three tables:\n\n\n\n\nTable 1: general information about the data set (name, creation date/time, etc.)\n\n\nTable 2: operating environment information, file location, etc.\n\n\nTable 3: alphabetic list of variables in the data set and their attributes\n\n\n\n\nThe \ndata portion\n (PROC PRINT) contains the data values, stored in variables (numeric/character)\n\n\n\n\nNumeric values: right-aligned\n\n\nCharacter values: left-aligned\n\n\nMissing values\n: \nblank\n for character variables and \nperiod\n for numeric ones. To change this default behaviour use  \nMISSING='new-character'\n\n\nValid \ncharacter values\n: letters, numbers, special characters and blanks\n\n\nValid \nnumeric values\n: digits 0-9, minus sign, single decimal point, scientific notation (E)\n\n\nValues length: for character variables 1 byte = 1 character, numeric variables have 8 bytes of storage by default (16-17 significant digits)\n\n\nOther attributes: \nformat\n, \ninformat\n, \nlabel\n\n\n\n\nProducing Detailed Reports\n\n\nChapter summary in SAS\n\n\nSubsetting Report Data\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nPROC PRINT DATA=SAS-data-set(OBS=3) NOOBS;  /* OBS=3 prints only 3 elements | NOOBS hides the \nObs\n */\n    VAR variable1 variable2 variable3;      /* prints out only this variables in the report */\n    SUM variable1 variable2;                /* adds an extra line at the end with the total */\n    WHERE variable3\n1000; variable3\n1000;   /* operators: \n \n \n= \n= = ^= in + - / * ** \n | ~ ^ ? */\n    WHERE variable4 in (\nChild\n,\nElder\n);   /* only the last WHERE condition is applied */\n    WHERE variable1=20 AND variable4 CONTAINS \ncase-sensitive-substring\n;  /* CONTAINS = ? */\n    IDWHERE ANYALPHA(variable) NE 0         /* only values containing at least a letter */\n    ID variable1                            /* replaces the \nObs\n column by a selected variable values */\n    BY variable3variable3                   /* separate in different tables for different variable values (sort first) */\nRUN;\n\n\n\n\n\n\nSpecial \nWHERE operators\n:\n\n\n\n\nBETWEEN x AND y\n: an inclusive range\n\n\nWHERE SAME AND\n: augment a previous where expression (both applied)\n\n\nIS NULL\n: a missing value\n\n\nIS MISSING\n: a missing value\n\n\nLIKE\n: matches a pattern (% = any number of characters, _ = one character). E.g.: 'T_m%'\n\n\nThe \nSOUNDS-LIKE (=*)\n operator selects observations that contain a spelling variation of a specified word or words. This operator uses the \nSoundex\n algorithm to compare the variable value and the operand.\n\n\nANYVALUE\n is an interesting function that searches a character string for an alphabetic character, and returns the first position at which the character is found\n\n\n\n\nNote:\n To compare with a SAS date value you need to express is as a SAS date constant: \n'DDMM\n\\YY\nYY'D\n\n\nSorting and Grouping Report Data\n\n\n1\n2\n3\n4\n5\nPROC SORT DATA=SAS-data-set\n    OUT=new-SAS-data-set NODUPKEY;                                           /* optional */\n    DUPOUT=work.duplicates;                                                  /* optional */\n    BY ASCENDING variable1-to-be-sorted DESCENDING variable2-to-be-sorted;   /* optional (ASCENDING is the default order)*/\nRUN;\n\n\n\n\n\n\n\n\nThe \nNODUPKEY\n option deletes observations with duplicate \nBY\n values\n\n\nDUPOUT\n writes duplicate observations to a separate output data set\n\n\n\n\nEnhancing Reports\n\n\n1\n2\n3\n4\n5\n6\nTITLEline \ntext\n;       \nFOOTNOTEline \ntext\n;\n\nTITLE1 \ntext1\n;\nTITLE1 \ntext1_change\n;     /* Change title text and also cancels all footnotes with higher numbers */\nTITLE;                     /* Cancel (erase) all titles */\n\n\n\n\n\n\n\n\nThe \nlines\n specifies the line (1-10) on which the title/footnote will appear (line = 1 is the default value)\n\n\nThe title/footnote will remain until you \nchange\n it, \ncancel\n it or you \nend your SAS session\n\n\n\n\n\n\nAssigning \ntemporary labels\n to display in the report instead of the variable names:\n\n\n1\n2\n3\n4\n5\nPROC PRINT DATA=SAS-data-set LABEL;           /* you need to add the LABEL option to display the labels */ \n    LABEL variable1 = \nnew variable1 name\n \n          variable2 = \nnew variable2 name\n;\n    LABEL variable3 = \nnew variable3 name\n;\nRUN;\n\n\n\n\n\n\n\n\nThe \nLABEL\n lengths can go up to 256 characters long\n\n\nYou can specify several labels in one \nLABEL\n statement or use a separate \nLABEL\n statement for each variable\n\n\n\n\n1\n2\n3\nPROC PRINT DATA=SAS-data-set SPLIT=\n*\n;            /* you no longer need to add the LABEL option, SPLIT does the same work */ \n    LABEL variable1 = \nnew variable1*long name\n;   /* the variable name ocuppies 2 lines now */\nRUN;\n\n\n\n\n\n\nFormatting Data Values\n\n\nChapter summary in SAS\n\n\nUsing SAS Formats\n\n\n1\n2\n3\n4\nPROC PRINT DATA=SAS-data-base;\n    FORMAT FORMAT variable1 variable2 format;\n    FORMAT variable3 format3 variable4 format4;\nRUN;\n\n\n\n\n\n\nFormat definition: \n\\$\nformat\n\\w\n.\n\\d\n\n\n\n\n\\$\n = character format\n\n\nformat\n = format name\n\n\n\\w\n = total width (includes special characters, commas, decimal point and decimal places)\n\n\n.\n = required syntax (dot)\n\n\n\\d\n = decimal places (numeric format)\n\n\n\n\nSAS formats (\nDictionary of formats\n):\n\n\n\n\n\\$w.\n = writes standard character data\n\n\n\\$UPCASE.\n = writes a string in uppercase\n\n\n\\$QUOTE.\n = writes a string in quotation marks \n\n\nw.d\n = writes standard numeric data\n\n\nCOMMAw.d\n = writes numeric values with a comma that separates every three digits and a period that separates the decimal fraction\n\n\nDOLLARw.d\n = writes numeric values with a leading dollar sign, a comma that separates every three digits and a period that separates the decimal fraction\n\n\nCOMMAXw.d\n = writes numeric values with a period that separates every three digits and a coma that separates the decimal fraction\n\n\nEUROXw.d\n = writes numeric values with a leading euro symbol, a period that separates every three digits and a comma that separates the decimal fraction\n\n\n\n\nSAS date values: \nMMDDYY\n\\w\n.\n / \nDDMMYY\n\\w\n.\n / \nMONYY\n\\w\n.\n / \nDATE\n\\w\n.\n / \nWEEKDATE.\n\n\n w = 6: only date numbers\n\n w = 8: date numbers with \n/\n separators (just the last 2 digits of year)\n* w = 10: date numbers with \n/\n separators (full 4-digit year)\n\n\nNote:\n dates before 01/01/1960 (0 value) will appear as negative numbers\n\n\nCreating and Applying User-Defined Formats\n\n\n1\n2\n3\n4\nPROC FORMAT;\n    VALUE \n$\nformat-name value-or-range1=\nformatted-value1\n\n                         value-or-range2=\nformatted-value2\n;\nRUN;\n\n\n\n\n\n\n1\n2\n3\nPROC PRINT DATA=SAS-data-set;\n    FORMAT variable1 \n$\nformat-name.;\nRUN;\n\n\n\n\n\n\n\n\nA format name can have a maximum of \n32 characters\n\n\nThe name of a format that applies to \ncharacter values\n must begin with a \ndollar sign\n followed by a letter or underscore\n\n\nThe name of a format that applies to \nnumeric values\n must begin with a letter or underscore\n\n\nA format name cannot end in a number\n\n\nAll remaining characters can be letters, underscores or numbers\n\n\nA user defined format name cannot be the name of a SAS format\n\n\n\n\nEach \nvalue-range set\n has three parts:\n\n\n\n\nvalue-or-range\n: specifies one or more values to be formatted (it can be a value, a range or a list of values)\n\n\n=\n: equal sign\n\n\nformatted-value\n: the formatted value you want to display instead of the stored value/s (it is allways a character string no matter wheter the format applies to character values or numeric values)\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPROC FORMAT LIBRARY = my-format-library;   /* To save the custom formats */\n    VALUE string \nA\n-\nH\n=\nFirst\n\n                 \nI\n,\nJ\n,\nK\n=\nMiddle\n\n                  OTHER = \nEnd\n;           /* Non-specified values */\n    VALUE tiers low-\n50000=\nTier1\n         /* 50000 not included */\n                50000-\n100000=\nTier2\n      /* 100000 not included */\n                100000-high=\nTier3\n\n                .=\nMissing value\n;\nRUN;\n\n\n\n\n\n\nNote1:\n if you omit the \nLIBRARY\n option, then formats and informats are stored in the \nwork.formats\n catalog\n\n\nNote2:\n if you do not includ the keyword \nOTHER\n, then SAS applies the format only to values that match the value-range sets that you specify and the rest of values are displayed as they are stored in the data set\n\n\nNote3:\n you can only use the \n symbol to define a non-inclusive range.\n\n\n1\nOPTIONS FMTSEARCH = (libref1 libref2... librefn)\n\n\n\n\n\n\n\n\nThe \nFMTSEARCH\n system option controls the order in which format catalogs are searched until the desired member is found.\n\n\nThe \nWORK.FORMATS\n catalog is always searched first, unless it appears in the \nFMTSEARCH\n list. \n\n\n\n\nReading SAS Data Sets\n\n\nChapter summary in SAS\n\n\nTo create a new data set that is a subset of a previous data set:\n\n\n1\n2\n3\n4\n5\n6\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    WHERE where-expression;\n    variable_name   WHERE where-expression;\n    variable_name = expression;     /* new variable creation */\nRUN;\n\n\n\n\n\n\nNote1:\n if a missing value is involved in an arithmetic calculation the result will be a missing value too\n\n\nNote2:\n new variables being created in the DATA step and not contained in the original data set cannot be used in a WHERE statement\n\n\nCustomizing a SAS Data Set\n\n\nHow to select a subset of the variables/observations of the original data set:\n\n\n1\n2\n3\n4\n5\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    DROP variable-list;        /* original variables to exclude */\n    KEEP variable-list;        /* original variables to include + new variables */\nRUN;\n\n\n\n\n\n\nHow SAS processes the \nDATA\n step:\n\n\nCompilation phase\n\n\n\n\nSAS scan each DATA step statement for syntax errors and converts the program into machine code if everything's alright. \n\n\nSAS also creates the program data vector (\nPDV\n) in memory to hold the current observation.\n\n\n_N_\n: iteration number of the DATA step\n\n\n_ERROR_\n: its value is 0 is there are no errors (1 if there are some)\n\n\nSAS creates the descriptor portion of the new data set (takes the original one, adds the new variables and flags the variables to be dropped). \n\n\n\n\nExecution phase\n\n\n\n\nSAS initializes the PDV to missing\n\n\nSAS reads and processes the observations from the input data set \n\n\nSAS creates observations in the data portion of the output data set (an implicit output/implicit return loop over all the observations that continues until EOF)\n\n\n\n\n\n\nSubsetting \nIF\n statement: \n\n\n1\n2\n3\n4\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression;\nRUN;\n\n\n\n\n\n\n\n\nWhen the expression is false, SAS excludes the observation from the output data set and continues processing\n\n\nWhile original values can be managed with a \nWHERE\n statement as well as an \nIF\n statement, for \nnew variable\n conditionals only \nIF\n can be used\n\n\nYou should subset as early as possible in your program for more efficient processing (a \nWHERE\n before an \nIF\n can make the processing more efficient).\n\n\nIn a \nPROC\n step \nIF\n statements are \nNOT allowed\n\n\n\n\n\n\nSubsetting \nIF-THEN/DELETE\n statement: \n\n\n1\n2\n3\n4\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression1 or expression2 THEN DELETE;\nRUN;\n\n\n\n\n\n\n\n\nThe \nIF-THEN/DELETE\n statement eliminates the observations where the \nconditions are not met\n (on the contrary of what the \nIF\n does)\n\n\nThe \nDELETE\n statement stops processing the current observation. It is often used in a THEN clause of an IF-THEN statement or as part of a conditionally executed DO group.\n\n\n\n\n\n\nAddition of several variables: \nTotal=sum(var1, var2, var3)\n\nCount of nonmissing values: \nNonmissing=n(var1, var2, var3)\n\n\nAdding Permanent Attributes\n\n\nPermanent variable labels\n\n\n1\n2\n3\n4\n5\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    LABEL variable1=\nlabel1\n\n          variable2=\nlabel2\n;\nRUN;\n\n\n\n\n\n\n1\n2\nPROC PRINT DATA=output-SAS-data-set label;\nRUN;\n\n\n\n\n\n\n\n\nIf you use the \nLABEL\n statement in the \nPROC\n step the labels are \ntemporary\n while if you use it in the \nDATA\n step, SAS \npermanently\n associates the labels to the variables\n\n\nLabels and formats that you specify in \nPROC\n steps override the permanent labels in the current step. However, the permanent labels are not changed.\n\n\n\n\nPermanent variable formats\n\n\n1\n2\n3\n4\n5\nDATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    FORMAT variable1 format1\n           variable2 format2;\nRUN;\n\n\n\n\n\n\nReading Spreadsheet and Database Data\n\n\nChapter summary in SAS\n\n\nReading Spreadsheet Data\n\n\nTo determine the SAS products that are included in your SAS license, you can run the following PROC SETINIT step:\n\n\n1\n2\nPROC SETINIT;\nRUN;\n\n\n\n\n\n\n\n\nSAS/ACCESS LIBNAME statement (read/write/update data):\n\n\n1\nLIBNAME libref \nengine\n \nPATH=\nworkbook-name\n \noptions\n;\n\n\n\n\n\n\nE.g.:\n\n\nDefault engine:\n \nLIBNAME orionx excel \"\npath/sales.xls\"\n\n\nPC Files server engine:\n \nLIBNAME orionx pcfiles PATH=\"\npath/sales.xls\"\n\n\n\n\n\\engine\n: excel (if both SAS and Office are 32/64 bits), pcfiles (if the value is different)\n\n\nThe icon of the library will be different (a globe) indicating that the data is outside SAS\n\n\nThe members whose name ends with a \n\\$\n are the \nspreadsheets\n while the others are named \nranges\n. In case it has the \n\\$\n, you need to refer to that Excel worksheet in a special way to account for that special character (SAS name literal): \nlibref.'worksheetname\\$'n\n\n\nYou can use the \nVALIDVARNAME = v7\n option in SAS Enterprise Guide to cause it to behave the same as in the SAS window environment\n\n\nIs important to disassociate the library: the workbook cannot be opened in Excel meanwhile (SAS puts a lock on the Excel file when the libref is assigned): \nLIBNAME libref CLEAR;\n\n\n\n\n\n\nImport the xls data:\n\n\n1\n2\n3\n4\n5\nPROC IMPORT DATAFILE=\n/folders/myfolders/reading_test.xlsx\n\n            OUT=work.myexcel\n            DBMS=xlsx \n            REPLACE;\nRUN;\n\n\n\n\n\n\nReading Database Data\n\n\n1\nLIBNAME libref engine \nSAS/ACCESS options\n;\n\n\n\n\n\n\n\n\nengine\n: oracle or BD2\n\n\nSAS/ACCESS options\n: USER, PASSWORD/PW, PATH (specifies the Oracle driver, node and database), SCHEMA (enables you to read database objects such as tables and views)\n\n\n\n\nReading Raw Data Files\n\n\nChapter summary in SAS\n\n\nIntroduction to Reading Raw Data Files\n\n\n\n\nRaw data files\n are not software specific\n\n\nA \ndelimited raw data file\n is an external text file in which the values are separated by spaces or other special characters.\n\n\nA \nlist input\n will be used to work with delimited raw data files that contain standard and/or nonstandard data\n\n\nStandard data\n is data that SAS can read without any special instructions\n\n\nNonstandard data\n includes values like dates or numeric values that include special characters like dollar signs (extra instructions needed)\n\n\n\n\nReading Standard Delimited Data\n\n\n1\n2\n3\n4\n5\nDATA output-SAS-data-set-name;\n    LENGTH variable(s) \n$\n length;\n    INFILE \nraw-data-file-name\n DLM=\ndelimiter\n;  \n    INPUT variable1 \n$\n variable2 \n$\n ... variableN \n$\n;    /* $ = character variables */     \nRUN;\n\n\n\n\n\n\nE.g.:\n\n\n1\n2\n3\n4\n5\nDATA work.sales1;\n    LENGTH First_Name Last_Name $ 12 Gender $ 1;\n    INFILE \npath/sales.csv\n DLM=\n,\n;  \n    INPUT Employee_ID Gender $ Salary $ Job_Title $ Country $; \nRUN;\n\n\n\n\n\n\n\n\nWith \nlist input\n, the default length for all variables is 8 bytes\n\n\nSAS uses an \ninput buffer\n only if the input data is a raw data file\n\n\nThe variable names will appear in the report as stated in the \nLENGTH\n statement (watch out the uppercase/lowercase)\n\n\nThe \nLENGTH\n statement must precede the \nINPUT\n statement in order to correctly set the length of the variable\n\n\nThe variables not specified in the \nLENGTH\n statement will appear at the end of the table. If you want to keep the original order you should include all variables even if you want them to have the defaul length (8)\n\n\n\n\nReading Nonstandard Delimited Data\n\n\nYou can use a \nmodified list input\n to read all of the fields from a raw data file (including nonstandard variables)\n\n\n\n\nInformats are similar to formats except that \nformats\n provide instruction on how to \nwrite\n a value while \ninformats\n provide instruction on how to \nread\n a value\n\n\nThe \ncolon format modifier (:)\n causes SAS to read up to the delimiter\n\n\n\n\n1\nINPUT variable \n$\n variable \n:informat\n;\n\n\n\n\n\n\nE.g.:\n\n\n1\n2\n:date.\n:mmddyy.\n\n\n\n\n\n\n\n\nCOMMA./DOLLAR.\n: reads nonstandard numeric data and removes embedded commas, blanks, dollar sign, percent signs and dashes\n\n\nCOMMAX./DOLLARX.\n: reads nonstandard numeric data and removes embedded non-numeric characters; reverses the roles of the decima point and the comma\n\n\nEUROX.\n: reads nonstandard numeric data and removes embedded non-numeric characters in European currency\n\n\n\\$CHAR.\n: reads character values and preserves leading blanks\n\n\n\\$UPCASE.\n: reads character values and converts them to uppercase\n\n\n\n\n\n\n\n\nYou cannot use a \nWHERE\n statement when the input data is a raw data file instead of a SAS data set\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nDATA (...);\n    INFILE DATALINES DLM=\n,\n;   /* only if datalines are delimited */\n    INPUT (...);\n    DATALINES;\n    \ninstream data\n\n    ;\n    INPUT (...);\n    DATALINES;\n\ninstream data\n\n;\n\n\n\n\n\n\n\n\nThe null statement (\n;\n) indicates the end of the input data\n\n\nYou precede the instream data with the \nDATALINES\n statement and follow it with a null statement\n\n\nThe instream data should be the \nlast part of the DATA step\n except for a null statement\n\n\n\n\nE.g.:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nDATA work.managers;\n   infile datalines dlm=\n/\n;\n   input ID First :$12. Last :$12. Gender $ Salary :comma. \n            Title :$25. HireDate :date.;\n   datalines;\n120102/Tom/Zhou/M/108,255/Sales Manager/01Jun1993\n120103/Wilson/Dawes/M/87,975/Sales Manager/01Jan1978\n120261/Harry/Highpoint/M/243,190/Chief Sales Officer/01Aug1991\n121143/Louis/Favaron/M/95,090/Senior Sales Manager/01Jul2001\n121144/Renee/Capachietti/F/83,505/Sales Manager/01Nov1995\n121145/Dennis/Lansberry/M/84,260/Sales Manager/01Apr1980\n;\n\ntitle;\n\n\n\n\n\n\n1\n2\n3\n4\n5\ntitle \nOrion Star Management Team\n;\nproc print data=work.managers noobs;\n   format HireDate mmddyy10.;\nrun;\ntitle;\n\n\n\n\n\n\nValidating Data\n\n\nWhen SAS encounters a data error, it prints messages and a ruler in the log and assigns a missing value to the affected variable. Then SAS continues processing.\n\n\n\n\nMissing values between delimiters (consecutive delimiters)\n\n\n1\nINFILE \nraw-data-file-name\n \nDLM=\n DSD;\n\n\n\n\n\n\nThe \nDSD\n option sets the default delimiter to a comma, treats consecutive delimiters as missing values and enables SAS to read values with embedded delimiters if the value is surrounded by quotation marks\n\n\n\n\nMissing values at the end of a line\n\n\n1\nINFILE \nraw-data-file-name\n MISSOVER;\n\n\n\n\n\n\nWith the \nMISSOVER\n option, if SAS reaches the end of a record without finding values for all fields, variables without values are set to \nmissing\n.\n\n\nManipulating Data\n\n\nChapter summary in SAS\n\n\nUsing SAS Functions\n\n\nSUM function\n\n\n1\nSUM(argument1, argument2, ...)\n\n\n\n\n\n\n\n\nThe arguments must be numeric values\n\n\nThe \nSUM\n function ignores missing values, so if an argument has a missing value, the result of the SUM function is the sum of the nonmissing values\n\n\nIf you add two values by \n+\n, if one of them is missing, the result will be a missing value which makes the \nSUM\n function a better choice\n\n\n\n\n\n\nDATE funtion\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nYEAR(SAS-date)     \nQTR(SAS-date)\nMONTH(SAS-date)\nDAY(SAS-date)\nWEEKDAY(SAS-date)\nTODAY()                /* Obtain the current date and convert to SAS-date (no argument) */\nDATE()                 /* Obtain the current date and convert to SAS-date (no argument) */\nMDY(month, day, year)\n\n\n\n\n\n\n\n\nThe arguments must be numeric values (except from \nTODAY()\n and \nDATE()\n functions)\n\n\nYou can subtract dates: \nAgein2012=(Bday2012-Birth_Date)/365.25;\n\n\n\n\n\n\nConcatenation function\n\n\n1\nCATX(\n \n, First_Name, Last_Name)\n\n\n\n\n\n\nThe \nCATX\n function removes leading and trailing blanks, inserts delimiters, and returns a concatenated character string. In the code, you first specify a character string that is used as a delimiter between concatenated items.\n\n\n\n\nTime interval function\n\n\n1\nINTCK(\nyear\n, Hire_Date, \n01JAN2012\nd)\n\n\n\n\n\n\nThe \nINTCK\n function returns the number of interval boundaries of a given kind that lie between the two dates, times, or datetime values. In the code, you first specify the interval value.\n\n\n\n\nWhat happens if you use a variable to describe a new one that you are gonna DROP in that same DATA statement?\n\n\nThe \nDROP\n statement is a compile-time-only statement. SAS sets a drop flag for the dropped variables, but the variables are in the PDV and, therefore, are available for processing.\n\n\nConditional Processing\n\n\nIF-THEN-ELSE conditional structures\n\n\n1\n2\n3\nIF expression THEN statement;\nELSE IF expression THEN statement;\nELSE statement;\n\n\n\n\n\n\nIn the conditional expressions involving strings watch out for possible mixed case values where the condition may not be met:  \nCountry = upcase(Country);\n to avoid problems\n\n\n\n\nExecuting multiple statements in an IF-THEN-ELSE statement\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nIF expression THEN\n    DO;\n        executable statements;\n    END;\nELSE IF expression THEN\n    DO;\n        executable statements;\n    END;\n\n\n\n\n\n\n\n\nIn the \nDATA\n step, the first reference to a variable determines its length. The first reference to a new variable can be in a \nLENGTH\n statement, an \nassignment\n statement, or \nanother\n statement such as an INPUT statement. After a variable is created in the PDV, the length of the variable's first value doesn't matter. \n\n\nTo avoid truncation in a variable defined inside a conditional structure you can:\n\n\n\n\nDefine the longer string as the first condition\n\n\nAdd some blanks at the end of shorter strings to fit the longer one\n\n\nDefine the length explicitly before any other reference to the variable\n\n\n\n\n\n\nSELECT group\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nSELECT(Gender);\n      WHEN(\nF\n) DO;\n         Gift1=\nScarf\n;\n         Gift2=\nPedometer\n;\n      END;\n      WHEN(\nM\n) DO;\n         Gift1=\nGloves\n;\n         Gift2=\nMoney Clip\n;\n      END;\n      OTHERWISE DO;\n         Gift1=\nCoffee\n;\n         Gift2=\nCalendar\n;\n      END;\nEND;\n\n\n\n\n\n\n\n\nThe \nSELECT\n statement executes one of several statements or groups of statements\n\n\nThe \nSELECT\n statement begins a SELECT group. They contain \nWHEN\n statements that identify SAS statements that are executed when a particular condition is true\n\n\nUse at least one \nWHEN\n statement in a SELECT group\n\n\nAn optional \nOTHERWISE\n statement specifies a statement to be executed if no \nWHEN\n condition is met\n\n\nAn \nEND\n statement ends a \nSELECT\n group\n\n\n\n\nCombining SAS Data Sets\n\n\nChapter summary in SAS\n\n\nConcatenating Data Sets\n\n\nCombine files vertically by concatenating\n\n\n1\n2\n3\nDATA SAS-data-set;\n    SET SAS-data-set1 SAS-data-set2 ...;\nRUN;\n\n\n\n\n\n\nCombine two different variables that are actually the same one\n\n\n1\n2\n3\nDATA SAS-data-set;\n    SET SAS-data-set1 (RENAME=(old-name1 = new-name1 old-name2 = new-name2)) SAS-data-set2 ...;\nRUN;\n\n\n\n\n\n\n\n\nThe name change affects the PDV and the output data set, but has no effect on the input data set\n\n\nThe \nvariable attributes\n are assigned from the \nfirst data set\n in the SET statement\n\n\nYou will get an \nerror\n in the DATA step if a variable is defined with \ndifferent data types\n in the files that you are trying to concatenate\n\n\n\n\nMerging SAS Data Sets One-to-One\n\n\nCombine files horizontally by merging\n\n\n\n\nThe \nmatch-merging\n is a process based on the values of common variables\n\n\nData sets are merged in the order that they appear in the MERGE statement\n\n\nYou may need to \nSORT\n the files by the \nBY-variable(s)\n before merging the files\n\n\n\n\n1\n2\n3\n4\n5\nDATA SAS-data-set;\n    MERGE SAS-data-set1 (RENAME=(old-name1 = new-name1 ...)) SAS-data-set2 ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\n\n\nIn a \none-to-one\n relationship, a single observastion in one data set is related to one, and only one, observation in another data set based on the values of one or more common variables\n\n\nIn a \none-to-many\n relationship, a single observation in one data set is related to one or more observations in another data set\n\n\nIn a \nmany-to-one\n relationship, multiple observations in one data set are related to one observation in another data set\n\n\nIn a \nmany-to-many\n relationship, multiple observations in one data set are related to multiple observations in another data set\n\n\nSometimes the data sets have \nnon-matches\n: at least one observation in one of the data sets is unrelated to any observation in another data set based on the values of one or more common variables\n\n\n\n\nMerging SAS Data Sets One-to-Many\n\n\n1\n2\n3\n4\n5\nDATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\nIn a \none-to-many merge\n, does it matter which data set is listed first in the MERGE statement?\n\n\nWhen you reverse the order of the data sets in the MERGE statement, the results are the same, but the order of the variables is different. SAS performs a \nmany-to-one merge\n.\n\n\n\n\nMERGENOBY\n (= NOWARN (default) | WARN | ERROR) controls whether a message is issued when MERGE processing occurs without an associated BY statement\n\n\n\n\nPerforming a merge without a BY statement merges the observations based on their positions\n\n\nThis is almost never done intentionally and can lead to unexpected results\n\n\n\n\nMerging SAS Data Sets that Have Non-Matches\n\n\n1\n2\n3\n4\n5\nDATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\n\n\nAfter the merging, the output data set contains \nboth matches and non-matches\n\n\nYou want the new data set to contain only the observations that match across the input data sets, and not those ones that are missing in one of the data sets that you are merging\n\n\n\n\n1\n2\n3\n4\n5\n6\nDATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY \nDESCENDING\n BY-variable(s);\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\n\n\nWhen you spefify the \nIN\n option after an input data set in the MERGE statement, SAS creates a \ntemporary numeric variable\n that indicates whether the data set contributed data to the current observation (0 = it did not contribute to the current observation, 1 = it did contribute)\n\n\nThese variables are only available \nduring execution\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\nDATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY \nDESCENDING\n BY-variable(s);\n    IF variable1 = 1 and variable2 = 1;     /* write only matches */\n    \nadditional SAS statements\n\nRUN;\n\n\n\n\n\n\n\n\nMatches\n\n\n\n\n1\n2\nIF variable1 = 1 and variable2 = 1 \nIF variable1 and variable2\n\n\n\n\n\n\n\n\nNon-matches from either data set\n\n\n\n\n1\n2\nIF variable1 = 0 or not variable2 = 0\nIF not variable1 or not variable2`\n\n\n\n\n\n\nE.g.:\n\n\n1\n2\n3\n4\n5\n6\n7\nDATA SAS-new-data-set1 SAS-new-data-set2;\n    MERGE SAS-data-set1 (in=var1) SAS-data-set2 (in=var2);\n    BY BY-variable(s);\n    IF var2 THEN OUTPUT SAS-new-data-set1;\n    ELSE IF var1 and not var2 THEN OUTPUT SAS-new-data-set2;\n    KEEP variable1 variable2 variable5 variable8;\nrun;\n\n\n\n\n\n\nCreating Summary Reports\n\n\nChapter summary in SAS\n\n\nUsing PROC FREQ to Create Summary Reports\n\n\n\n\nWhen you're summarizing data, there's no need to show a frequency distribution for variables that have a large number of distinct values\n\n\nFrequency distributions work best with variables whose values meet two criteria: variable with \ncategorical values\n and values are \nbest summarized by counts instead of averages\n\n\nVariables that have continuous numerical values, such as dollar amounts and dates, will need to be \ngrouped into categories\n by \napplying formats\n inside the PROC FREQ step (substitute an specific range of those values by a tag)\n\n\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set \noption(s)\n;\n    TABLES variable(s) \nloption(s)\n;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\n\n\nPROC FREQ\n produces frequency tables that report the distribution of any or all variable values in a SAS data set\n\n\nIn the \nTABLE\n statement you specify the frequency tables to produce \n\n\nTo create \none-way\n frequency tables you specify one or more variable names separated by space\n\n\nWATCH OUT\n: if you omit the \nTABLE\n statement, SAS produces a one-way table for every variable in the data set\n\n\nThe \nPROC FREQ\n step automatically displays output in a report, so you don't need to add a PROC PRINT step \n\n\nEach unique variable's value displayed in the 1\nst\n column of the output is called a \nlevel of the variable\n\n\n\n\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set \noption(s)\n;\n    TABLES variable/NOCUM NOPERCENT;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\n\n\nNOCUM\n option supresses the display of  the cummulative frequency and cummulative percent values \n\n\nNOPERCENT\n option supresses the display of all percentages\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPROC SORT DATA=SAS-data-set\n    OUT=SAS-data-set-sorted;\n    BY variable_sorted;\nRUN;\n\nPROC FREQ DATA=SAS-data-set-sorted;\n    TABLES variable-freq;\n    BY variable_sorted;\nRUN;\n\n\n\n\n\n\n\n\nWhenever you use the \nBY\n statement, the data set must be sorted by the variable named in the statement\n\n\nUsing this we will get a frequency table on \nvariable_freq\n for each value of \nvariable_sorted\n\n\n\n\n\n\nCrosstabulation tables\n\n\n\n\nSometimes it is useful to view a single table with statistics for each distintic combination of values of the selected variables\n\n\nThe simplest crosstabulation table is a \ntwo-way table\n\n\n\n\n1\n2\n3\n4\n5\n6\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable1 * variable2 / NOFREQ NOPERCENT NOROW NOCOL;\nRUN;\n\nvariable1 = table rows\nvariable2 = table columns\n\n\n\n\n\n\nInformation contained in crosstabulation tables (legend):\n\n\n\n\nFrequency\n: indicates the number of observations with the unique combination of values represented in that cell\n\n\nPercent\n: indicates the cell's percentage of the total frequency\n\n\nRow Pct\n: cell's percentage of the total frequency for its row\n\n\nCol Pct\n: cell's percentage of the total frequency for its column \n\n\n\nLIST\n option format: the first two columns specify each possible combination of the two variables; it displays the same statistics as the default \none-way frequency\n table\n\n\nCROSSLIST\n option format: it displays the same statistics as the default \ncrosstabulation\n table\n\n\n\n\n\n\nThe \nFORMAT=\n option allows you to format the frequency value (to any SAS numeric format or a user-defined numeric format while its length is not more than 24) and to change the width of the column (e.g. to allow variable labels to fit in one line). \n\n\n1\n2\n3\n4\n5\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable1 * variable2 /\n    FORMAT = \nw\n.;\n    FORMAT variable1 $format-name.;    \nRUN;\n\n\n\n\n\n\nThe \nFORMAT=\n option applies only to crosstabulation tables displayed in the default format. It doesn't apply to crosstabulation tables produced with the \nLIST\n/\nCROSSLIST\n option\n\n\nUsing PROC FREQ for Data Validation\n\n\nYou can use a \nPROC FREQ\n step with the \nTABLES\n statement to detect invalud numeric and character data by looking at distinct values. The \nFREQ\n procedure \nlists all discrete values\n for a variable and \nreports its missing values\n.\n\n\n1\n2\n3\nPROC FREQ DATA=SAS-data-set \nORDER=FREQ\n;\n    TABLES variable;\nRUN;\n\n\n\n\n\n\n\n\nYou can check for non-expected variable's values\n\n\nYou can check for missing values\n\n\nYou can find duplicated values\n\n\n\n\n\n\nThe table showing the \nNumber of Variable Levels\n can indicate whether a variable contains duplicate/missing/non-expected values:\n\n\n1\n2\n3\nPROC FREQ DATA=SAS-data-set NLEVELS;\n    TABLES variable / NOPRINT;\nRUN;\n\n\n\n\n\n\n\n\nYou can use a \nWHERE\n statement to print out only the invalid values to be checked:\n\n\n1\n2\n3\n4\n5\n6\nPROC PRINT DATA=SAS-data-set;\n    WHERE gender NOT IN (\nF\n,\nM\n) OR\n          job_title IS NULL OR\n          salary NOT BETWEEN 24000 AND 500000 OR\n          employee IS MISSING;\nRUN;\n\n\n\n\n\n\n\n\nYou can output the tables to a new data set instead of displaying it:\n\n\n1\n2\n3\nPROC FREQ DATA=SAS-data-set NOPRINT;\n   TABLE variable / OUT=SAS-new-data-set;\nRUN;\n\n\n\n\n\n\nUsing the MEANS and UNIVARIATE Procedures\n\n\nPROC MEANS\n produces summary reports with descriptive statistics and you can create statistics for groups of observations\n\n\n\n\nIt automatically displays output in a report and you can also save the output in a SAS data set\n\n\nIt reports the \nnumber of nonmissing values\n of the analysis variable (N), and the \nmean\n, the \nstandard deviation\n and \nminimum/maximum values\n of every numeric variable in the data set\n\n\nThe variables in the \nCLASS\n statement are called \nclassification variables\n or \nclass variables\n (they typically have few discrete values)\n\n\nEach combination of class variable values is called a \nclass level\n\n\nThe data set \ndoesn't need to be sorted\n or indexed by the class variables\n\n\nN Obs\n reports the number of observations with each unique combination of class variables, whether or not there are missing values (if these \nN Obs\n are identical to \nN\n, there are no missing values in you data set)\n\n\n\n\n1\n2\n3\n4\nPROC MEANS DATA=SAS-data-set \nstatistic(s)\n;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;\n\n\n\n\n\n\nTo write the report in a new data set (including total addition):\n\n\n1\n2\n3\n4\n5\nPROC MEANS DATA=SAS-data-set NOPRINT NWAY;\n    OUTPUT OUT=SAS-new-data-set SUM=addition-new-variable;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;\n\n\n\n\n\n\nFormat options: \n\n\n\n\nMAXDEC=number\n (default format = BESTw.) \n\n\nNONOBS\n\n\nFW=number\n: specifies that the field width for all columns is \nnumber\n\n\nPRINTALLTYPES\n: displays statistics for all requested combination of class variables\n\n\n\n\n\n\n\n\n\n\nAlternative procedure to validate data: \n \nPROC MEANS\n\n\n\n\nThe \nMIN\n/\nMAX\n values can be useful to check if the data is within a range\n\n\nNMISS\n option displays the number of observations with missing values\n\n\n\n\n\n\nAlternative procedure to validate data: \n \nPROC UNIVARIATE\n\n\nPROC UNIVARIATE\n is a procedure that is useful for detecting data outliers that also produces summary reports of \ndescriptive statistics\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\n    ID variable_to_relate;\n    HISTOGRAM variables \n/options\n;\n    PROBPLOT variables \n/options\n;\n    INSET keywords \n/options\n;\nRUN;\n\n\n\n\n\n\n\n\nIf you omit the \nVAR\n statement, all numeric variables in the data set are analyzed\n\n\nThe \nExtreme Observations\n table contains useful information to locate outliers: it displays the 5 lowest/highest values by default along with the corresponding observation number. The \nID\n statement specifies that SAS will use this variable as a label in the table of extreme observations and as an identifier for any extreme.\n\n\nTo specify the number of listed observations you can use \nNEXTROBS=\n\n\nHISTOGRAM/PROBPLOT\n options: normal(mu=est sigma=est) creates a normal curve overlay to the histogram using the estimates of the population mean and standard deviation\n\n\nINSET\n writes a legend for the graph. \n/ position=ne\n moves the \nINSET\n to the north-east corner of the graph.\n\n\n\n\nTo include in the report only one of the automatically produced tables:\n\n\n1) Check the specific table name in the \nLOG information\n using \nODS TRACE\n:\n\n\n1\n2\n3\n4\n5\nODS TRACE ON;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;\nODS TRACE OFF;\n\n\n\n\n\n\n2) Select the wanted table with \nODS SELECT\n:\n\n\n1\n2\n3\n4\nODS SELECT ExtremeObs;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;\n\n\n\n\n\n\n\n\nSUMMARY of validation procedures\n\n\n\n\nUsing the SAS Output Delivery System\n\n\n1\n2\n3\nODS destination FILE=\nfilename\n \noptions\n;\n    \nSAS code to generate the report\n\nODS destination CLOSE;\n\n\n\n\n\n\n\n\nYou can have multiple destinations open and execute multiple procedures\n\n\nAll generated output will be sent to every open destination\n\n\nYou might not be able to view the file, or the most updated file, outside of SAS until you close the destination\n\n\n\n\nE.g.:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nODS\n \npdf\n \nFILE\n=\nC:/output/test.pdf\n;\n\n\n(...)\n\n\nODS\n \npdf\n \nCLOSE\n;\n\n\n\nODS\n \ncsvall\n \nFILE\n=\nC:/output/test.cvs\n;\n\n\nODS\n \nrtf\n \nFILE\n=\nC:/output/test.rtf\n;\n\n\n(...)\n\n\nODS\n \ncsvall\n \nCLOSE\n;\n\n\nODS\n \nrtf\n \nCLOSE\n;\n\n\n\n\n\n\n\nAllowed file formats and their corresponding destinations:", 
            "title": "The Essentials"
        }, 
        {
            "location": "/essentials/#getting-started-with-sas-programming", 
            "text": "Chapter summary in SAS", 
            "title": "Getting Started with SAS Programming"
        }, 
        {
            "location": "/essentials/#working-with-sas-programs", 
            "text": "Chapter summary in SAS  Comments  1\n2 /* comment */  *   comment   statement ;", 
            "title": "Working with SAS Programs"
        }, 
        {
            "location": "/essentials/#accessing-data", 
            "text": "Chapter summary in SAS", 
            "title": "Accessing Data"
        }, 
        {
            "location": "/essentials/#accessing-sas-libraries", 
            "text": "libref : library reference name (shortcut to the physical location). There are three rules for valid librefs:  A length of one to eight characters  Begin with a letter or underscore  The remaining characters are letters, numbers, or underscores  Valid variable names begin with a letter or underscore, and continue with letters, numbers, or underscores. The  VALIDVARNAME  system option specifies the rules for valid SAS variable names that can be created and processed during a SAS session:    1 OPTIONS VALIDVARNAME=V7 (default) | UPCASE | ANY;    libref.data-set-name : data set reference two-level name  data-set-name : when the data set belongs to a temporary library, you can optionally use a one-level name (SAS assumes that it is contained in the  work  library, which is the default)  The  LIBNAME  statement associates the  libref  with the physical location of the library/data for the current SAS session   1 LIBNAME libref-name  SAS-library-folder-path   options ;   Example  1\n2 %let   path =/ folders / myfolders / ecprg193 ;   libname   orion   path ;     To erase the association between SAS and a custom library   1 LIBNAME libref-name CLEAR;    To check the  contents of a library  programatically   1\n2 PROC CONTENTS DATA=libref._ALL_;\nRUN;    To hide the descriptors of all data sets in the library (it could generate a very long report) you can add the option  nods  (only compatible with the keybord  _all_ )   1\n2 PROC CONTENTS DATA=libref._ALL_ NODS;\nRUN;    To access a data set you can use a  proc print  step   1\n2 PROC PRINT DATA=SAS-data-set;\nRUN;", 
            "title": "Accessing SAS libraries"
        }, 
        {
            "location": "/essentials/#examining-sas-data-sets", 
            "text": "Parts of a library (SAS notation):   Table =  data set  Column =  variable  Row =  observation   The  descriptor portion  (PROC CONTENTS) contains information about the attributes of the data set (metadata), including the variable names. It is show in three tables:   Table 1: general information about the data set (name, creation date/time, etc.)  Table 2: operating environment information, file location, etc.  Table 3: alphabetic list of variables in the data set and their attributes   The  data portion  (PROC PRINT) contains the data values, stored in variables (numeric/character)   Numeric values: right-aligned  Character values: left-aligned  Missing values :  blank  for character variables and  period  for numeric ones. To change this default behaviour use   MISSING='new-character'  Valid  character values : letters, numbers, special characters and blanks  Valid  numeric values : digits 0-9, minus sign, single decimal point, scientific notation (E)  Values length: for character variables 1 byte = 1 character, numeric variables have 8 bytes of storage by default (16-17 significant digits)  Other attributes:  format ,  informat ,  label", 
            "title": "Examining SAS Data Sets"
        }, 
        {
            "location": "/essentials/#producing-detailed-reports", 
            "text": "Chapter summary in SAS", 
            "title": "Producing Detailed Reports"
        }, 
        {
            "location": "/essentials/#subsetting-report-data", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 PROC PRINT DATA=SAS-data-set(OBS=3) NOOBS;  /* OBS=3 prints only 3 elements | NOOBS hides the  Obs  */\n    VAR variable1 variable2 variable3;      /* prints out only this variables in the report */\n    SUM variable1 variable2;                /* adds an extra line at the end with the total */\n    WHERE variable3 1000; variable3 1000;   /* operators:      =  = = ^= in + - / * **   | ~ ^ ? */\n    WHERE variable4 in ( Child , Elder );   /* only the last WHERE condition is applied */\n    WHERE variable1=20 AND variable4 CONTAINS  case-sensitive-substring ;  /* CONTAINS = ? */\n    IDWHERE ANYALPHA(variable) NE 0         /* only values containing at least a letter */\n    ID variable1                            /* replaces the  Obs  column by a selected variable values */\n    BY variable3variable3                   /* separate in different tables for different variable values (sort first) */\nRUN;   Special  WHERE operators :   BETWEEN x AND y : an inclusive range  WHERE SAME AND : augment a previous where expression (both applied)  IS NULL : a missing value  IS MISSING : a missing value  LIKE : matches a pattern (% = any number of characters, _ = one character). E.g.: 'T_m%'  The  SOUNDS-LIKE (=*)  operator selects observations that contain a spelling variation of a specified word or words. This operator uses the  Soundex  algorithm to compare the variable value and the operand.  ANYVALUE  is an interesting function that searches a character string for an alphabetic character, and returns the first position at which the character is found   Note:  To compare with a SAS date value you need to express is as a SAS date constant:  'DDMM \\YY YY'D", 
            "title": "Subsetting Report Data"
        }, 
        {
            "location": "/essentials/#sorting-and-grouping-report-data", 
            "text": "1\n2\n3\n4\n5 PROC SORT DATA=SAS-data-set\n    OUT=new-SAS-data-set NODUPKEY;                                           /* optional */\n    DUPOUT=work.duplicates;                                                  /* optional */\n    BY ASCENDING variable1-to-be-sorted DESCENDING variable2-to-be-sorted;   /* optional (ASCENDING is the default order)*/\nRUN;    The  NODUPKEY  option deletes observations with duplicate  BY  values  DUPOUT  writes duplicate observations to a separate output data set", 
            "title": "Sorting and Grouping Report Data"
        }, 
        {
            "location": "/essentials/#enhancing-reports", 
            "text": "1\n2\n3\n4\n5\n6 TITLEline  text ;       \nFOOTNOTEline  text ;\n\nTITLE1  text1 ;\nTITLE1  text1_change ;     /* Change title text and also cancels all footnotes with higher numbers */\nTITLE;                     /* Cancel (erase) all titles */    The  lines  specifies the line (1-10) on which the title/footnote will appear (line = 1 is the default value)  The title/footnote will remain until you  change  it,  cancel  it or you  end your SAS session    Assigning  temporary labels  to display in the report instead of the variable names:  1\n2\n3\n4\n5 PROC PRINT DATA=SAS-data-set LABEL;           /* you need to add the LABEL option to display the labels */ \n    LABEL variable1 =  new variable1 name  \n          variable2 =  new variable2 name ;\n    LABEL variable3 =  new variable3 name ;\nRUN;    The  LABEL  lengths can go up to 256 characters long  You can specify several labels in one  LABEL  statement or use a separate  LABEL  statement for each variable   1\n2\n3 PROC PRINT DATA=SAS-data-set SPLIT= * ;            /* you no longer need to add the LABEL option, SPLIT does the same work */ \n    LABEL variable1 =  new variable1*long name ;   /* the variable name ocuppies 2 lines now */\nRUN;", 
            "title": "Enhancing Reports"
        }, 
        {
            "location": "/essentials/#formatting-data-values", 
            "text": "Chapter summary in SAS", 
            "title": "Formatting Data Values"
        }, 
        {
            "location": "/essentials/#using-sas-formats", 
            "text": "1\n2\n3\n4 PROC PRINT DATA=SAS-data-base;\n    FORMAT FORMAT variable1 variable2 format;\n    FORMAT variable3 format3 variable4 format4;\nRUN;   Format definition:  \\$ format \\w . \\d   \\$  = character format  format  = format name  \\w  = total width (includes special characters, commas, decimal point and decimal places)  .  = required syntax (dot)  \\d  = decimal places (numeric format)   SAS formats ( Dictionary of formats ):   \\$w.  = writes standard character data  \\$UPCASE.  = writes a string in uppercase  \\$QUOTE.  = writes a string in quotation marks   w.d  = writes standard numeric data  COMMAw.d  = writes numeric values with a comma that separates every three digits and a period that separates the decimal fraction  DOLLARw.d  = writes numeric values with a leading dollar sign, a comma that separates every three digits and a period that separates the decimal fraction  COMMAXw.d  = writes numeric values with a period that separates every three digits and a coma that separates the decimal fraction  EUROXw.d  = writes numeric values with a leading euro symbol, a period that separates every three digits and a comma that separates the decimal fraction   SAS date values:  MMDDYY \\w .  /  DDMMYY \\w .  /  MONYY \\w .  /  DATE \\w .  /  WEEKDATE.   w = 6: only date numbers  w = 8: date numbers with  /  separators (just the last 2 digits of year)\n* w = 10: date numbers with  /  separators (full 4-digit year)  Note:  dates before 01/01/1960 (0 value) will appear as negative numbers", 
            "title": "Using SAS Formats"
        }, 
        {
            "location": "/essentials/#creating-and-applying-user-defined-formats", 
            "text": "1\n2\n3\n4 PROC FORMAT;\n    VALUE  $ format-name value-or-range1= formatted-value1 \n                         value-or-range2= formatted-value2 ;\nRUN;   1\n2\n3 PROC PRINT DATA=SAS-data-set;\n    FORMAT variable1  $ format-name.;\nRUN;    A format name can have a maximum of  32 characters  The name of a format that applies to  character values  must begin with a  dollar sign  followed by a letter or underscore  The name of a format that applies to  numeric values  must begin with a letter or underscore  A format name cannot end in a number  All remaining characters can be letters, underscores or numbers  A user defined format name cannot be the name of a SAS format   Each  value-range set  has three parts:   value-or-range : specifies one or more values to be formatted (it can be a value, a range or a list of values)  = : equal sign  formatted-value : the formatted value you want to display instead of the stored value/s (it is allways a character string no matter wheter the format applies to character values or numeric values)   1\n2\n3\n4\n5\n6\n7\n8\n9 PROC FORMAT LIBRARY = my-format-library;   /* To save the custom formats */\n    VALUE string  A - H = First \n                  I , J , K = Middle \n                  OTHER =  End ;           /* Non-specified values */\n    VALUE tiers low- 50000= Tier1          /* 50000 not included */\n                50000- 100000= Tier2       /* 100000 not included */\n                100000-high= Tier3 \n                .= Missing value ;\nRUN;   Note1:  if you omit the  LIBRARY  option, then formats and informats are stored in the  work.formats  catalog  Note2:  if you do not includ the keyword  OTHER , then SAS applies the format only to values that match the value-range sets that you specify and the rest of values are displayed as they are stored in the data set  Note3:  you can only use the   symbol to define a non-inclusive range.  1 OPTIONS FMTSEARCH = (libref1 libref2... librefn)    The  FMTSEARCH  system option controls the order in which format catalogs are searched until the desired member is found.  The  WORK.FORMATS  catalog is always searched first, unless it appears in the  FMTSEARCH  list.", 
            "title": "Creating and Applying User-Defined Formats"
        }, 
        {
            "location": "/essentials/#reading-sas-data-sets", 
            "text": "Chapter summary in SAS  To create a new data set that is a subset of a previous data set:  1\n2\n3\n4\n5\n6 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    WHERE where-expression;\n    variable_name   WHERE where-expression;\n    variable_name = expression;     /* new variable creation */\nRUN;   Note1:  if a missing value is involved in an arithmetic calculation the result will be a missing value too  Note2:  new variables being created in the DATA step and not contained in the original data set cannot be used in a WHERE statement", 
            "title": "Reading SAS Data Sets"
        }, 
        {
            "location": "/essentials/#customizing-a-sas-data-set", 
            "text": "How to select a subset of the variables/observations of the original data set:  1\n2\n3\n4\n5 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    DROP variable-list;        /* original variables to exclude */\n    KEEP variable-list;        /* original variables to include + new variables */\nRUN;   How SAS processes the  DATA  step:  Compilation phase   SAS scan each DATA step statement for syntax errors and converts the program into machine code if everything's alright.   SAS also creates the program data vector ( PDV ) in memory to hold the current observation.  _N_ : iteration number of the DATA step  _ERROR_ : its value is 0 is there are no errors (1 if there are some)  SAS creates the descriptor portion of the new data set (takes the original one, adds the new variables and flags the variables to be dropped).    Execution phase   SAS initializes the PDV to missing  SAS reads and processes the observations from the input data set   SAS creates observations in the data portion of the output data set (an implicit output/implicit return loop over all the observations that continues until EOF)    Subsetting  IF  statement:   1\n2\n3\n4 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression;\nRUN;    When the expression is false, SAS excludes the observation from the output data set and continues processing  While original values can be managed with a  WHERE  statement as well as an  IF  statement, for  new variable  conditionals only  IF  can be used  You should subset as early as possible in your program for more efficient processing (a  WHERE  before an  IF  can make the processing more efficient).  In a  PROC  step  IF  statements are  NOT allowed    Subsetting  IF-THEN/DELETE  statement:   1\n2\n3\n4 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    IF expression1 or expression2 THEN DELETE;\nRUN;    The  IF-THEN/DELETE  statement eliminates the observations where the  conditions are not met  (on the contrary of what the  IF  does)  The  DELETE  statement stops processing the current observation. It is often used in a THEN clause of an IF-THEN statement or as part of a conditionally executed DO group.    Addition of several variables:  Total=sum(var1, var2, var3) \nCount of nonmissing values:  Nonmissing=n(var1, var2, var3)", 
            "title": "Customizing a SAS Data Set"
        }, 
        {
            "location": "/essentials/#adding-permanent-attributes", 
            "text": "Permanent variable labels  1\n2\n3\n4\n5 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    LABEL variable1= label1 \n          variable2= label2 ;\nRUN;   1\n2 PROC PRINT DATA=output-SAS-data-set label;\nRUN;    If you use the  LABEL  statement in the  PROC  step the labels are  temporary  while if you use it in the  DATA  step, SAS  permanently  associates the labels to the variables  Labels and formats that you specify in  PROC  steps override the permanent labels in the current step. However, the permanent labels are not changed.   Permanent variable formats  1\n2\n3\n4\n5 DATA output-SAS-data-set;\n    SET input-SAS-data-set;\n    FORMAT variable1 format1\n           variable2 format2;\nRUN;", 
            "title": "Adding Permanent Attributes"
        }, 
        {
            "location": "/essentials/#reading-spreadsheet-and-database-data", 
            "text": "Chapter summary in SAS", 
            "title": "Reading Spreadsheet and Database Data"
        }, 
        {
            "location": "/essentials/#reading-spreadsheet-data", 
            "text": "To determine the SAS products that are included in your SAS license, you can run the following PROC SETINIT step:  1\n2 PROC SETINIT;\nRUN;    SAS/ACCESS LIBNAME statement (read/write/update data):  1 LIBNAME libref  engine   PATH= workbook-name   options ;   E.g.:  Default engine:   LIBNAME orionx excel \" path/sales.xls\"  PC Files server engine:   LIBNAME orionx pcfiles PATH=\" path/sales.xls\"   \\engine : excel (if both SAS and Office are 32/64 bits), pcfiles (if the value is different)  The icon of the library will be different (a globe) indicating that the data is outside SAS  The members whose name ends with a  \\$  are the  spreadsheets  while the others are named  ranges . In case it has the  \\$ , you need to refer to that Excel worksheet in a special way to account for that special character (SAS name literal):  libref.'worksheetname\\$'n  You can use the  VALIDVARNAME = v7  option in SAS Enterprise Guide to cause it to behave the same as in the SAS window environment  Is important to disassociate the library: the workbook cannot be opened in Excel meanwhile (SAS puts a lock on the Excel file when the libref is assigned):  LIBNAME libref CLEAR;    Import the xls data:  1\n2\n3\n4\n5 PROC IMPORT DATAFILE= /folders/myfolders/reading_test.xlsx \n            OUT=work.myexcel\n            DBMS=xlsx \n            REPLACE;\nRUN;", 
            "title": "Reading Spreadsheet Data"
        }, 
        {
            "location": "/essentials/#reading-database-data", 
            "text": "1 LIBNAME libref engine  SAS/ACCESS options ;    engine : oracle or BD2  SAS/ACCESS options : USER, PASSWORD/PW, PATH (specifies the Oracle driver, node and database), SCHEMA (enables you to read database objects such as tables and views)", 
            "title": "Reading Database Data"
        }, 
        {
            "location": "/essentials/#reading-raw-data-files", 
            "text": "Chapter summary in SAS", 
            "title": "Reading Raw Data Files"
        }, 
        {
            "location": "/essentials/#introduction-to-reading-raw-data-files", 
            "text": "Raw data files  are not software specific  A  delimited raw data file  is an external text file in which the values are separated by spaces or other special characters.  A  list input  will be used to work with delimited raw data files that contain standard and/or nonstandard data  Standard data  is data that SAS can read without any special instructions  Nonstandard data  includes values like dates or numeric values that include special characters like dollar signs (extra instructions needed)", 
            "title": "Introduction to Reading Raw Data Files"
        }, 
        {
            "location": "/essentials/#reading-standard-delimited-data", 
            "text": "1\n2\n3\n4\n5 DATA output-SAS-data-set-name;\n    LENGTH variable(s)  $  length;\n    INFILE  raw-data-file-name  DLM= delimiter ;  \n    INPUT variable1  $  variable2  $  ... variableN  $ ;    /* $ = character variables */     \nRUN;   E.g.:  1\n2\n3\n4\n5 DATA work.sales1;\n    LENGTH First_Name Last_Name $ 12 Gender $ 1;\n    INFILE  path/sales.csv  DLM= , ;  \n    INPUT Employee_ID Gender $ Salary $ Job_Title $ Country $; \nRUN;    With  list input , the default length for all variables is 8 bytes  SAS uses an  input buffer  only if the input data is a raw data file  The variable names will appear in the report as stated in the  LENGTH  statement (watch out the uppercase/lowercase)  The  LENGTH  statement must precede the  INPUT  statement in order to correctly set the length of the variable  The variables not specified in the  LENGTH  statement will appear at the end of the table. If you want to keep the original order you should include all variables even if you want them to have the defaul length (8)", 
            "title": "Reading Standard Delimited Data"
        }, 
        {
            "location": "/essentials/#reading-nonstandard-delimited-data", 
            "text": "You can use a  modified list input  to read all of the fields from a raw data file (including nonstandard variables)   Informats are similar to formats except that  formats  provide instruction on how to  write  a value while  informats  provide instruction on how to  read  a value  The  colon format modifier (:)  causes SAS to read up to the delimiter   1 INPUT variable  $  variable  :informat ;   E.g.:  1\n2 :date.\n:mmddyy.    COMMA./DOLLAR. : reads nonstandard numeric data and removes embedded commas, blanks, dollar sign, percent signs and dashes  COMMAX./DOLLARX. : reads nonstandard numeric data and removes embedded non-numeric characters; reverses the roles of the decima point and the comma  EUROX. : reads nonstandard numeric data and removes embedded non-numeric characters in European currency  \\$CHAR. : reads character values and preserves leading blanks  \\$UPCASE. : reads character values and converts them to uppercase     You cannot use a  WHERE  statement when the input data is a raw data file instead of a SAS data set     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 DATA (...);\n    INFILE DATALINES DLM= , ;   /* only if datalines are delimited */\n    INPUT (...);\n    DATALINES;\n     instream data \n    ;\n    INPUT (...);\n    DATALINES; instream data \n;    The null statement ( ; ) indicates the end of the input data  You precede the instream data with the  DATALINES  statement and follow it with a null statement  The instream data should be the  last part of the DATA step  except for a null statement   E.g.:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 DATA work.managers;\n   infile datalines dlm= / ;\n   input ID First :$12. Last :$12. Gender $ Salary :comma. \n            Title :$25. HireDate :date.;\n   datalines;\n120102/Tom/Zhou/M/108,255/Sales Manager/01Jun1993\n120103/Wilson/Dawes/M/87,975/Sales Manager/01Jan1978\n120261/Harry/Highpoint/M/243,190/Chief Sales Officer/01Aug1991\n121143/Louis/Favaron/M/95,090/Senior Sales Manager/01Jul2001\n121144/Renee/Capachietti/F/83,505/Sales Manager/01Nov1995\n121145/Dennis/Lansberry/M/84,260/Sales Manager/01Apr1980\n;\n\ntitle;   1\n2\n3\n4\n5 title  Orion Star Management Team ;\nproc print data=work.managers noobs;\n   format HireDate mmddyy10.;\nrun;\ntitle;", 
            "title": "Reading Nonstandard Delimited Data"
        }, 
        {
            "location": "/essentials/#validating-data", 
            "text": "When SAS encounters a data error, it prints messages and a ruler in the log and assigns a missing value to the affected variable. Then SAS continues processing.   Missing values between delimiters (consecutive delimiters)  1 INFILE  raw-data-file-name   DLM=  DSD;   The  DSD  option sets the default delimiter to a comma, treats consecutive delimiters as missing values and enables SAS to read values with embedded delimiters if the value is surrounded by quotation marks   Missing values at the end of a line  1 INFILE  raw-data-file-name  MISSOVER;   With the  MISSOVER  option, if SAS reaches the end of a record without finding values for all fields, variables without values are set to  missing .", 
            "title": "Validating Data"
        }, 
        {
            "location": "/essentials/#manipulating-data", 
            "text": "Chapter summary in SAS", 
            "title": "Manipulating Data"
        }, 
        {
            "location": "/essentials/#using-sas-functions", 
            "text": "SUM function  1 SUM(argument1, argument2, ...)    The arguments must be numeric values  The  SUM  function ignores missing values, so if an argument has a missing value, the result of the SUM function is the sum of the nonmissing values  If you add two values by  + , if one of them is missing, the result will be a missing value which makes the  SUM  function a better choice    DATE funtion  1\n2\n3\n4\n5\n6\n7\n8 YEAR(SAS-date)     \nQTR(SAS-date)\nMONTH(SAS-date)\nDAY(SAS-date)\nWEEKDAY(SAS-date)\nTODAY()                /* Obtain the current date and convert to SAS-date (no argument) */\nDATE()                 /* Obtain the current date and convert to SAS-date (no argument) */\nMDY(month, day, year)    The arguments must be numeric values (except from  TODAY()  and  DATE()  functions)  You can subtract dates:  Agein2012=(Bday2012-Birth_Date)/365.25;    Concatenation function  1 CATX(   , First_Name, Last_Name)   The  CATX  function removes leading and trailing blanks, inserts delimiters, and returns a concatenated character string. In the code, you first specify a character string that is used as a delimiter between concatenated items.   Time interval function  1 INTCK( year , Hire_Date,  01JAN2012 d)   The  INTCK  function returns the number of interval boundaries of a given kind that lie between the two dates, times, or datetime values. In the code, you first specify the interval value.   What happens if you use a variable to describe a new one that you are gonna DROP in that same DATA statement?  The  DROP  statement is a compile-time-only statement. SAS sets a drop flag for the dropped variables, but the variables are in the PDV and, therefore, are available for processing.", 
            "title": "Using SAS Functions"
        }, 
        {
            "location": "/essentials/#conditional-processing", 
            "text": "IF-THEN-ELSE conditional structures  1\n2\n3 IF expression THEN statement;\nELSE IF expression THEN statement;\nELSE statement;   In the conditional expressions involving strings watch out for possible mixed case values where the condition may not be met:   Country = upcase(Country);  to avoid problems   Executing multiple statements in an IF-THEN-ELSE statement  1\n2\n3\n4\n5\n6\n7\n8 IF expression THEN\n    DO;\n        executable statements;\n    END;\nELSE IF expression THEN\n    DO;\n        executable statements;\n    END;    In the  DATA  step, the first reference to a variable determines its length. The first reference to a new variable can be in a  LENGTH  statement, an  assignment  statement, or  another  statement such as an INPUT statement. After a variable is created in the PDV, the length of the variable's first value doesn't matter.   To avoid truncation in a variable defined inside a conditional structure you can:   Define the longer string as the first condition  Add some blanks at the end of shorter strings to fit the longer one  Define the length explicitly before any other reference to the variable    SELECT group   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 SELECT(Gender);\n      WHEN( F ) DO;\n         Gift1= Scarf ;\n         Gift2= Pedometer ;\n      END;\n      WHEN( M ) DO;\n         Gift1= Gloves ;\n         Gift2= Money Clip ;\n      END;\n      OTHERWISE DO;\n         Gift1= Coffee ;\n         Gift2= Calendar ;\n      END;\nEND;    The  SELECT  statement executes one of several statements or groups of statements  The  SELECT  statement begins a SELECT group. They contain  WHEN  statements that identify SAS statements that are executed when a particular condition is true  Use at least one  WHEN  statement in a SELECT group  An optional  OTHERWISE  statement specifies a statement to be executed if no  WHEN  condition is met  An  END  statement ends a  SELECT  group", 
            "title": "Conditional Processing"
        }, 
        {
            "location": "/essentials/#combining-sas-data-sets", 
            "text": "Chapter summary in SAS", 
            "title": "Combining SAS Data Sets"
        }, 
        {
            "location": "/essentials/#concatenating-data-sets", 
            "text": "Combine files vertically by concatenating  1\n2\n3 DATA SAS-data-set;\n    SET SAS-data-set1 SAS-data-set2 ...;\nRUN;   Combine two different variables that are actually the same one  1\n2\n3 DATA SAS-data-set;\n    SET SAS-data-set1 (RENAME=(old-name1 = new-name1 old-name2 = new-name2)) SAS-data-set2 ...;\nRUN;    The name change affects the PDV and the output data set, but has no effect on the input data set  The  variable attributes  are assigned from the  first data set  in the SET statement  You will get an  error  in the DATA step if a variable is defined with  different data types  in the files that you are trying to concatenate", 
            "title": "Concatenating Data Sets"
        }, 
        {
            "location": "/essentials/#merging-sas-data-sets-one-to-one", 
            "text": "Combine files horizontally by merging   The  match-merging  is a process based on the values of common variables  Data sets are merged in the order that they appear in the MERGE statement  You may need to  SORT  the files by the  BY-variable(s)  before merging the files   1\n2\n3\n4\n5 DATA SAS-data-set;\n    MERGE SAS-data-set1 (RENAME=(old-name1 = new-name1 ...)) SAS-data-set2 ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;    In a  one-to-one  relationship, a single observastion in one data set is related to one, and only one, observation in another data set based on the values of one or more common variables  In a  one-to-many  relationship, a single observation in one data set is related to one or more observations in another data set  In a  many-to-one  relationship, multiple observations in one data set are related to one observation in another data set  In a  many-to-many  relationship, multiple observations in one data set are related to multiple observations in another data set  Sometimes the data sets have  non-matches : at least one observation in one of the data sets is unrelated to any observation in another data set based on the values of one or more common variables", 
            "title": "Merging SAS Data Sets One-to-One"
        }, 
        {
            "location": "/essentials/#merging-sas-data-sets-one-to-many", 
            "text": "1\n2\n3\n4\n5 DATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;   In a  one-to-many merge , does it matter which data set is listed first in the MERGE statement?  When you reverse the order of the data sets in the MERGE statement, the results are the same, but the order of the variables is different. SAS performs a  many-to-one merge .   MERGENOBY  (= NOWARN (default) | WARN | ERROR) controls whether a message is issued when MERGE processing occurs without an associated BY statement   Performing a merge without a BY statement merges the observations based on their positions  This is almost never done intentionally and can lead to unexpected results", 
            "title": "Merging SAS Data Sets One-to-Many"
        }, 
        {
            "location": "/essentials/#merging-sas-data-sets-that-have-non-matches", 
            "text": "1\n2\n3\n4\n5 DATA SAS-data-set;\n    MERGE SAS-data-set1 SAS-data-set2 ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;    After the merging, the output data set contains  both matches and non-matches  You want the new data set to contain only the observations that match across the input data sets, and not those ones that are missing in one of the data sets that you are merging   1\n2\n3\n4\n5\n6 DATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY  DESCENDING  BY-variable(s);\n     additional SAS statements \nRUN;    When you spefify the  IN  option after an input data set in the MERGE statement, SAS creates a  temporary numeric variable  that indicates whether the data set contributed data to the current observation (0 = it did not contribute to the current observation, 1 = it did contribute)  These variables are only available  during execution   1\n2\n3\n4\n5\n6\n7 DATA SAS-data-set;\n    MERGE SAS-data-set1 (IN=variable1) \n          SAS-data-set2 (IN=variable2) ...;\n    BY  DESCENDING  BY-variable(s);\n    IF variable1 = 1 and variable2 = 1;     /* write only matches */\n     additional SAS statements \nRUN;    Matches   1\n2 IF variable1 = 1 and variable2 = 1 \nIF variable1 and variable2    Non-matches from either data set   1\n2 IF variable1 = 0 or not variable2 = 0\nIF not variable1 or not variable2`   E.g.:  1\n2\n3\n4\n5\n6\n7 DATA SAS-new-data-set1 SAS-new-data-set2;\n    MERGE SAS-data-set1 (in=var1) SAS-data-set2 (in=var2);\n    BY BY-variable(s);\n    IF var2 THEN OUTPUT SAS-new-data-set1;\n    ELSE IF var1 and not var2 THEN OUTPUT SAS-new-data-set2;\n    KEEP variable1 variable2 variable5 variable8;\nrun;", 
            "title": "Merging SAS Data Sets that Have Non-Matches"
        }, 
        {
            "location": "/essentials/#creating-summary-reports", 
            "text": "Chapter summary in SAS", 
            "title": "Creating Summary Reports"
        }, 
        {
            "location": "/essentials/#using-proc-freq-to-create-summary-reports", 
            "text": "When you're summarizing data, there's no need to show a frequency distribution for variables that have a large number of distinct values  Frequency distributions work best with variables whose values meet two criteria: variable with  categorical values  and values are  best summarized by counts instead of averages  Variables that have continuous numerical values, such as dollar amounts and dates, will need to be  grouped into categories  by  applying formats  inside the PROC FREQ step (substitute an specific range of those values by a tag)   1\n2\n3\n4 PROC FREQ DATA=SAS-data-set  option(s) ;\n    TABLES variable(s)  loption(s) ;\n     additional statements \nRUN;    PROC FREQ  produces frequency tables that report the distribution of any or all variable values in a SAS data set  In the  TABLE  statement you specify the frequency tables to produce   To create  one-way  frequency tables you specify one or more variable names separated by space  WATCH OUT : if you omit the  TABLE  statement, SAS produces a one-way table for every variable in the data set  The  PROC FREQ  step automatically displays output in a report, so you don't need to add a PROC PRINT step   Each unique variable's value displayed in the 1 st  column of the output is called a  level of the variable    1\n2\n3\n4 PROC FREQ DATA=SAS-data-set  option(s) ;\n    TABLES variable/NOCUM NOPERCENT;\n     additional statements \nRUN;    NOCUM  option supresses the display of  the cummulative frequency and cummulative percent values   NOPERCENT  option supresses the display of all percentages    1\n2\n3\n4\n5\n6\n7\n8\n9 PROC SORT DATA=SAS-data-set\n    OUT=SAS-data-set-sorted;\n    BY variable_sorted;\nRUN;\n\nPROC FREQ DATA=SAS-data-set-sorted;\n    TABLES variable-freq;\n    BY variable_sorted;\nRUN;    Whenever you use the  BY  statement, the data set must be sorted by the variable named in the statement  Using this we will get a frequency table on  variable_freq  for each value of  variable_sorted    Crosstabulation tables   Sometimes it is useful to view a single table with statistics for each distintic combination of values of the selected variables  The simplest crosstabulation table is a  two-way table   1\n2\n3\n4\n5\n6 PROC FREQ DATA=SAS-data-set;\n    TABLES variable1 * variable2 / NOFREQ NOPERCENT NOROW NOCOL;\nRUN;\n\nvariable1 = table rows\nvariable2 = table columns   Information contained in crosstabulation tables (legend):   Frequency : indicates the number of observations with the unique combination of values represented in that cell  Percent : indicates the cell's percentage of the total frequency  Row Pct : cell's percentage of the total frequency for its row  Col Pct : cell's percentage of the total frequency for its column   LIST  option format: the first two columns specify each possible combination of the two variables; it displays the same statistics as the default  one-way frequency  table  CROSSLIST  option format: it displays the same statistics as the default  crosstabulation  table    The  FORMAT=  option allows you to format the frequency value (to any SAS numeric format or a user-defined numeric format while its length is not more than 24) and to change the width of the column (e.g. to allow variable labels to fit in one line).   1\n2\n3\n4\n5 PROC FREQ DATA=SAS-data-set;\n    TABLES variable1 * variable2 /\n    FORMAT =  w .;\n    FORMAT variable1 $format-name.;    \nRUN;   The  FORMAT=  option applies only to crosstabulation tables displayed in the default format. It doesn't apply to crosstabulation tables produced with the  LIST / CROSSLIST  option", 
            "title": "Using PROC FREQ to Create Summary Reports"
        }, 
        {
            "location": "/essentials/#using-proc-freq-for-data-validation", 
            "text": "You can use a  PROC FREQ  step with the  TABLES  statement to detect invalud numeric and character data by looking at distinct values. The  FREQ  procedure  lists all discrete values  for a variable and  reports its missing values .  1\n2\n3 PROC FREQ DATA=SAS-data-set  ORDER=FREQ ;\n    TABLES variable;\nRUN;    You can check for non-expected variable's values  You can check for missing values  You can find duplicated values    The table showing the  Number of Variable Levels  can indicate whether a variable contains duplicate/missing/non-expected values:  1\n2\n3 PROC FREQ DATA=SAS-data-set NLEVELS;\n    TABLES variable / NOPRINT;\nRUN;    You can use a  WHERE  statement to print out only the invalid values to be checked:  1\n2\n3\n4\n5\n6 PROC PRINT DATA=SAS-data-set;\n    WHERE gender NOT IN ( F , M ) OR\n          job_title IS NULL OR\n          salary NOT BETWEEN 24000 AND 500000 OR\n          employee IS MISSING;\nRUN;    You can output the tables to a new data set instead of displaying it:  1\n2\n3 PROC FREQ DATA=SAS-data-set NOPRINT;\n   TABLE variable / OUT=SAS-new-data-set;\nRUN;", 
            "title": "Using PROC FREQ for Data Validation"
        }, 
        {
            "location": "/essentials/#using-the-means-and-univariate-procedures", 
            "text": "PROC MEANS  produces summary reports with descriptive statistics and you can create statistics for groups of observations   It automatically displays output in a report and you can also save the output in a SAS data set  It reports the  number of nonmissing values  of the analysis variable (N), and the  mean , the  standard deviation  and  minimum/maximum values  of every numeric variable in the data set  The variables in the  CLASS  statement are called  classification variables  or  class variables  (they typically have few discrete values)  Each combination of class variable values is called a  class level  The data set  doesn't need to be sorted  or indexed by the class variables  N Obs  reports the number of observations with each unique combination of class variables, whether or not there are missing values (if these  N Obs  are identical to  N , there are no missing values in you data set)   1\n2\n3\n4 PROC MEANS DATA=SAS-data-set  statistic(s) ;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;   To write the report in a new data set (including total addition):  1\n2\n3\n4\n5 PROC MEANS DATA=SAS-data-set NOPRINT NWAY;\n    OUTPUT OUT=SAS-new-data-set SUM=addition-new-variable;\n    VAR analysis-variable(s);\n    CLASS classification-variable(s);\nRUN;   Format options:    MAXDEC=number  (default format = BESTw.)   NONOBS  FW=number : specifies that the field width for all columns is  number  PRINTALLTYPES : displays statistics for all requested combination of class variables      Alternative procedure to validate data:    PROC MEANS   The  MIN / MAX  values can be useful to check if the data is within a range  NMISS  option displays the number of observations with missing values    Alternative procedure to validate data:    PROC UNIVARIATE  PROC UNIVARIATE  is a procedure that is useful for detecting data outliers that also produces summary reports of  descriptive statistics  1\n2\n3\n4\n5\n6\n7 PROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\n    ID variable_to_relate;\n    HISTOGRAM variables  /options ;\n    PROBPLOT variables  /options ;\n    INSET keywords  /options ;\nRUN;    If you omit the  VAR  statement, all numeric variables in the data set are analyzed  The  Extreme Observations  table contains useful information to locate outliers: it displays the 5 lowest/highest values by default along with the corresponding observation number. The  ID  statement specifies that SAS will use this variable as a label in the table of extreme observations and as an identifier for any extreme.  To specify the number of listed observations you can use  NEXTROBS=  HISTOGRAM/PROBPLOT  options: normal(mu=est sigma=est) creates a normal curve overlay to the histogram using the estimates of the population mean and standard deviation  INSET  writes a legend for the graph.  / position=ne  moves the  INSET  to the north-east corner of the graph.   To include in the report only one of the automatically produced tables:  1) Check the specific table name in the  LOG information  using  ODS TRACE :  1\n2\n3\n4\n5 ODS TRACE ON;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;\nODS TRACE OFF;   2) Select the wanted table with  ODS SELECT :  1\n2\n3\n4 ODS SELECT ExtremeObs;\nPROC UNIVARIATE DATA=SAS-data-set;\n    VAR variable(s);\nRUN;    SUMMARY of validation procedures", 
            "title": "Using the MEANS and UNIVARIATE Procedures"
        }, 
        {
            "location": "/essentials/#using-the-sas-output-delivery-system", 
            "text": "1\n2\n3 ODS destination FILE= filename   options ;\n     SAS code to generate the report \nODS destination CLOSE;    You can have multiple destinations open and execute multiple procedures  All generated output will be sent to every open destination  You might not be able to view the file, or the most updated file, outside of SAS until you close the destination   E.g.:  1\n2\n3\n4\n5\n6\n7\n8\n9 ODS   pdf   FILE = C:/output/test.pdf ;  (...)  ODS   pdf   CLOSE ;  ODS   csvall   FILE = C:/output/test.cvs ;  ODS   rtf   FILE = C:/output/test.rtf ;  (...)  ODS   csvall   CLOSE ;  ODS   rtf   CLOSE ;    Allowed file formats and their corresponding destinations:", 
            "title": "Using the SAS Output Delivery System"
        }, 
        {
            "location": "/statistics/introduction/", 
            "text": "Chapter summary in SAS\n\n\nBasic Statistical Concepts\n\n\nDescriptive statistics (exploratory data analysis, EDA)\n\n\n\n\nExplore your data\n\n\n\n\nInferential statistics (explanatory modelling)\n\n\n\n\nHow is X related to Y?\n\n\nSample sizes are typically small and include few variables\n\n\nThe focus is on the parameters of the model\n\n\nTo assess the model, you use p-values and confidence intervals\n\n\n\n\nPredictive modelling\n\n\n\n\nIf you know X, can you predict Y?\n\n\nSample sizes are large and include many predictive (input) variables\n\n\nThe focus is on the predictions of observations rather than the parameters of the model\n\n\nTo assess a predictive model, you validate predictions using holdout sample data\n\n\n\n\n\n\n\n\nParameters\n: numerical values (typically unknown, you can't measure the entire population) that summarize characteristics of a population (greek letters)\n\n\nStatistics\n: summarizes characteristics of a sample (standard alphabet)\n\n\nYou use \nstatistics\n to estimate \nparameters\n\n\n\n\n\n\n\n\nIndependent variable\n: it can take different values, it affects or determines a \ndependent variable\n. It can be called predictor, explanatory, control or input variable.\n\n\nDependent variable\n: it can take different values in response to an \nindependent variable\n. Also known as response, outcome or target variable.\n\n\n\n\n\n\nScale of measurement\n: variable's classification\n\n\n\n\nQuantitative/numerical variables\n: counts or measurements, you can perform arithmetical operations with it\n\n\nDiscrete data\n: variables that can have only a countable number of values within a measurement range\n\n\nContinuous data\n: variables that are measured on a scale that has infinite number of values and has no breaks or jumps\n\n\nInterval scale data\n: it can be rank-ordered like ordinal data but also has a sensible spacing of observations such that differenes between measurements are meaningful but it lacks a true zero (ratios are meaningless)\n\n\nRatio scale data\n: it is rank-ordered with meaningful spacing and also includes a true zero point and can therefore accurately indicate the ratio difference between two spaces on the measurement scale\n\n\n\n\n\n\n\n\n\n\nCategorical/attribute variables\n: variables that denote groupings or labels\n\n\nNominal data (qualitative/classification variable)\n: exhibits no ordering within its observed levels, groups or categories\n\n\nOrdinal data\n: the observed labels can be ordered in some meaningful way that implies that the differences between the groups or categories are due to magnitude\n\n\n\n\n\n\n\n\n\n\n\n\nUnivariate analysis\n provides techniques for analyzing and describing a sigle variable. It reveals patterns in the data by looking at the \nrange\n of values, measures of \ndispersion\n, the \ncentral tendecy\n of the values and \nfrequency distribution\n.\n\n\nBivariate analysis\n describes and explains the relationships between two variables and how they change or covary together. It include techniques such as \ncorrelation analysis\n and \nchi-square tests of independance\n.\n\n\nMultivariate/Multivariable analysis\n examines two or more variables at the same time in order to understand the relationships among them. \n\n\nTechniques such as \nmutiple linear regression\n and n-way \nANOVA\n are typically called \nmultivariable\n analysis (only one response variable). \n\n\nTechniques such as \nfactora analysis\n and \nclustering\n are typically called \nmutivariate\n analysis (they consider more than one response variable).\n\n\n\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\nMeasures of central tendencies\n: mean (affected by outliers), median (less sensitive to outliers), mode\n\n\n\n\n\n\n\n\nPercentile\n\n\nQuartile\n\n\n\n\n\n\n\n\n\n\n\n\n25th\n\n\n1st / lower / Q1\n\n\n\n\n\n\n\n\n50th\n\n\n2nd / middle / Q2\n\n\nMedian\n\n\n\n\n\n\n75th\n\n\n3rd / upper / Q3\n\n\n\n\n\n\n\n\n\n\nThe \ninterquartile range (IQR)\n is the difference between Q1 and Q3, it is a \nrobust estimate of the variability\n because changes in the upper/lower 25% of the data do not affect it. If there are \noutliers\n in the data, then the IQR is a more reliable measure of the spread than the overall range.\n\n\nThe \ncoefficient of variation (CV)\n is a measure of the standard deviation expressed as a percentage of the mean ($c_v = \\sigma / 100 \\mu$).\n\n\n\n\nNormal distribution\n\n\n\n\n\n\n\n\nIntervals\n\n\nPercentage contained\n\n\n\n\n\n\n\n\n\n\n$\\mu \\pm \\sigma$\n\n\n68%\n\n\n\n\n\n\n$\\mu \\pm 2 \\sigma$\n\n\n95%\n\n\n\n\n\n\n$\\mu \\pm 3 \\sigma$\n\n\n99%\n\n\n\n\n\n\n\n\nHow to check the normality of a sample?\n\n\n\n\nCompare the \nmean\n and the \nmedian\n: if they are nearly equal, that is an indicator of symmetry (requirement for normality)\n\n\nCheck that \nskewness\n and \nkurtosis\n are close to 0:\n\n\nIf both are greater than 1 or less than -1: data is not normal\n\n\nIf either is greater than 2 or less than -2: data is not normal\n\n\n\n\n\n\n\n\nStatistical summaries\n \n\n\nSkewness\n and \nkurtosis\n measure certain aspects of the shape of a distribution (they are \n0\n and \n3\n for a normal distribution, although SAS has standardized both to 0)\n\n\n\n\nSkewness\n measures the tendency of your data to be more spread out on one side of the mean than on the other (asymmetry of the distribution). \n\n\nYou can think of the direction of skewness as the direction the data is trailing off to. \n\n\nA \nright-skewed\n distribution tells us that the mean is \ngreater than the median\n.\n\n\n\n\n\n\nKurtosis\n measures the tendency of your data to be concentrated toward the center or toward the tails of the distribution (peakedness of the data, tail thickness). \n\n\nA \nnegative kurtosis (platykurtic distribution)\n means that the data has lighter tails than in a normal distribution. \n\n\nA \npositive kurtosis (leptokurtic/heavy-tailed/outlier-prone distribution)\n means that the data has heavier tails and is more concentrated around the mean than a normal distribution.\n\n\nRectangular, bimodal and multimodal distributions tend to have low values of kurtosis.\n\n\nAsymmetric distributions\n also tend to have nonzero kurtosis. In these cases, understanding kurtosis is considerably more complex and can be difficult to assess visually.\n\n\n\n\n\n\n\n\n\n\nPROC SURVEYSELECT\n\n\nHow to generate random (representative) samples (population subsets):\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC SURVEYSELECT DATA=SAS-data-set \n                  OUT=name-of-output-data-set\n                  METHOD=method-of-random-sampling\n                  SEED=seed-value \n                  SAMPSIZE=number-of-observations-desired;\n     \nSTRATA stratification-variable(s);\n\nRUN;\n\n\n\n\n\n\n\n\nMETHOD\n specifies the random sampling method to be used. For simple random sampling without replacement, use \nMETHOD=SRS\n. For simple random sampling with replacement, use \nMETHOD=URS\n. For other selection methods and details on sampling algorithms, see the SAS online documentation for \nPROC SURVEYSELECT\n.\n\n\nSEED\n specifies the initial seed for random number generation. If no \nSEED\n option is specified, SAS uses the system time as its seed value. This creates a different random sample every time the procedure is run.\n\n\nSAMPSIZE\n indicates the number of observations to be included in the sample. To select a certain fraction of the original data set rather than a given number of observations, use the \nSAMPRATE\n option.\n\n\n\n\nPicturing Your Data\n\n\nPROC UNIVARIATE\n\n\nPlots that can be produced with this procedure:\n\n\n\n\nHistograms\n\n\nNormal probability plots\n: expected percentiles from standard normal vs actual data values\n\n\n\n\n\n\nPROC SGSCATTER\n\n\nPlots that can be produced with this procedure:\n\n\n\n\nScatter plots\n: you can create a \nsingle-cell\n (simple Y by X) scatter plot, a \nmulti-cell\n scatter plot with multiple independent scatter plots in a grid and a \nscatter plot matrix\n, which produces a matrix of scatter plots comparing multiple variables.\n\n\n\n\nPROC SGPLOT\n\n\nPlots that can be produced with this procedure:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\nPROC SGPLOT DATA=SAS-data-set \noptions\n;\n        DOT category-variable \n/options\n;\n        HBAR category-variable \n/options\n;\n        VBAR category-variable \n/options\n;\n        HBOX response-variable \n/options\n;\n        VBOX response-variable \n/options\n;\n        HISTOGRAM response-variable \n/options\n;\n        SCATTER X=variable Y=variable \n/options\n;\n        NEEDLE X=variable Y=numeric-variable \n/options\n;\n        REG X=numeric-variable Y=numeric-variable \n/options\n;\nRUN;\n\n\n\n\n\n\nAnywhere in the procedure you can add \nreference lines\n:\n\n\n1\n2\n3\n4\nREFLINE variable | value-1 \n...\n \nvalue-n\n \n/option(s)\n\n\n/* Example: */\nREFLINE 1200 / axis=y lineattrs=(color=blue);\n\n\n\n\n\n\n\n\nNote\n\n\nThe order on which you define the parts of the plot will the determined the order on which it is displayed (if you want to send a \nREFLINE\n to the back, define it first)\n\n\n\n\n\n\nScatter plots (\nSCATTER\n)\n\n\nLine graphs\n\n\nHistograms (\nHISTOGRAM\n)\n with overlaid distribution curves\n\n\nRegression lines (\nREG\n)\n with confidence and prediction bands\n\n\nDot plots (\nDOT\n)\n\n\nBox plots (\nHBOX\n/\nVBOX\n)\n: it makes it easy to see how spread out your data is and if there are any outliers. The box represents the middle 50% of your data (IQR). The lower/middle/upper \nline of the box\n represent Q1/Q2/Q3. The \ndiamond\n denotes the mean (easy to check how close the mean is to the median). The \nwhiskers\n extend as far as the data extends to a maximum length of 1.5 times the IQR above Q3. Any data points farther than this distance are considered possible outliers and are represented in this plot as \ncircles\n.\n\n\nBar charts (\nHBAR\n/\nVBAR\n)\n\n\nNeedle plot (\nNEEDLE\n)\n: creates a plot with needles connecting each point to the baseline\n\n\nYou can also \noverlay plots together\n to produce many different types of graphs\n\n\n\n\nPROC SGPANEL\n\n\nPlots that can be produced with this procedure:\n\n\n\n\nPanels of plots\n for different levels of a factor or several different time periods depending on the classification variable\n\n\nSide-by-side histograms\n which provide a visual comparison for your data\n\n\n\n\nPROC SGRENDER\n\n\n\n\nPlots from graphs templates you have modified or written yourself\n\n\n\n\n\n\nTo specify options for graphs you submit the \nODS GRAPHICS\n statement:\n\n\n1\nODS GRAPHICS ON \noptions\n;\n\n\n\n\n\n\n\n\nTo select/exclude specific test results, graphs or tables from you output, you can use \nODS SELECT\n and \nODS EXCLUDE\n statements.\n\n\nYou can use ODS templates to modify the layout and details of each graph\n\n\nYou can use ODS styles to control the general appearance and consistency of yous graphs and tables (by default \nHTMLBLUE\n).\n\n\n\n\nAnother way to control your output is to use the \nPLOTS\n option which is usually available in the procedure statement:\n\n\n1\nPROC UNIVARIATE DATA=SAS-data-set PLOTS=options;\n\n\n\n\n\n\nThis option enables you to specify which graphs SAS should create, either in addtion or instead of the default plots.\n\n\nConfidence Intervals for the Mean\n\n\n\n\nA \npoint estimator\n is a sample statistic used to estimate a population parameter\n\n\nAn estimator takes on different values from sample to sample, so it's important to know its variance\n\n\nA statistic that measures the variability of your estimator is the \nstandard error\n\n\nIt differs from the standard deviation: the \nstandard deviation\n deals with the variability of your data while \nstandard error\n deals with the variability of you sample statistic\n\n\n\n\nE.g.:\n $standard \\ error \\ of \\ the \\ mean = standard \\ deviation/ \\sqrt{sample \\ size}$\n\n\nThe \ndistribution of sample means\n is always less variable than the data.\n\n\n\n\nBecause we know that point estimators vary from sample to sample, it would be nice to have an estimator of the mean that directly accounts for this natural variability\n\n\nThe \ninterval estimator\n gives us a range of values that is likely to contain the population mean\n\n\nIt is calculated from the \nstandard error\n and a value that is determined by the \ndegree of certainty\n we require (\nsignificance level\n)\n\n\nConfidence intervals\n are a type of interval estimator used to estimate the population mean\n\n\nYou can make the confidence interval narrower by increasing the sample size and by decreasing the confidence level\n\n\n\n\n$CI = sample \\ mean \\pm quantile \\cdot standard \\ error$\n\n\n\n\nThe \nCLM\n option of \nPROC MEANS\n calculates the confidence limits for the mean, you can add \nALPHA=\n to change the default 0.05 value for a 95% confidence level\n\n\nThe \ncentral limit theorem\n states that the distribution of sample means is approximately normal regardless of the population distribution's shape, if the sample size is large enough (~30 observations)\n\n\n\n\nHypothesis Testing\n\n\n\n\nThe \nnull hypothesis\n ($H_0$) is what you assume to be true when you start your analysis\n\n\nThe \nalternative hypothesis\n ($H_a$ or $H_1$) is your initial research hypothesis, that is, your proposed explanation\n\n\n\n\nDecision-making process:\n\n\n\n\nDefine null and alternative hypothesis\n\n\nSpecify significance level (type I error rate)\n\n\nCollect data\n\n\nReject or fail to reject the null hypothesis\n\n\n\n\n\n\n\n\nThe type I and II errors are \ninversely related\n: as one type increases the other decreases \n\n\nThe \npower\n is the probability of a \ncorrect rejection\n = 1 - \n\n\nIt is the ability of the statistical test to detect a true difference\n\n\n\n\nIt is the ability to successfully reject a false null hypothesis\n\n\n\n\n\n\nA \np-value\n measures the probability of observing a value as extreme as the one observed\n\n\n\n\nThe p-value is used to determine \nstatistical significance\n\n\nIt helps you assess whether you should reject the null hypothesis\n\n\n\n\n\n\n\n\nThe \np-value\n is affected by:\n\n\n\n\nThe \neffect size\n: the difference between the observed statistic and the hypothesized value\n\n\nThe \nsample size\n: the larger the sample size, the more sure you are about the sample statistics, the lower the p-value is\n\n\n\n\n\n\n\n\nA reference distribution enables you to quantify the probability (p-value) of observing a particular outcome (the calculated sample statistic) or a more extreme outcome, if the nul hypothesis is true\n\n\n\n\nTwo common reference distributions for statistical hypothesis testing are the \nt distribution\n and the \nF distribution\n\n\nThese distributions are characterized by the \ndegrees of freedom\n associated with your data\n\n\nThe \nt distribution\n arises when you're making inferences about a population mean and the population standard deviation is unknown and has to be estimated from the data\n\n\nIt is \napproximately normal\n as the \nsample size grows larger\n\n\nThe t distribution is a \nsymmetric distribution\n like the normal distribution except that the t distribution has \nthicker tails\n\n\nThe \nt statistic\n is positive/negative when the sample is more/less than the hypothesized mean\n\n\nIf the data doesn't come from a normal distribution, then the t statistic approximately follows a t distribution as long as the sample size is large (\ncentral limit theorem\n)\n\n\n\n\n\n\n\n\nCalculation with \nPROC UNIVARIATE\n:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nODS SELECT TESTSFORLOCATION;\nPROC UNIVARIATE DATA=SAS-data-set MU0=number alpha=number;\n  VAR variable(s);\n  ID variable_to_relate;\n  HISTOGRAM variables \n/options\n;\n  PROBPLOT variables \n/options\n;\n  INSET keywords \n/options\n;\nRUN;\n\n\n\n\n\n\n\n\nTESTSFORLOCATION\n displays only the p-values calculation\n\n\nBy default \nMU0 = 0", 
            "title": "Introduction to Statistics"
        }, 
        {
            "location": "/statistics/introduction/#basic-statistical-concepts", 
            "text": "Descriptive statistics (exploratory data analysis, EDA)   Explore your data   Inferential statistics (explanatory modelling)   How is X related to Y?  Sample sizes are typically small and include few variables  The focus is on the parameters of the model  To assess the model, you use p-values and confidence intervals   Predictive modelling   If you know X, can you predict Y?  Sample sizes are large and include many predictive (input) variables  The focus is on the predictions of observations rather than the parameters of the model  To assess a predictive model, you validate predictions using holdout sample data     Parameters : numerical values (typically unknown, you can't measure the entire population) that summarize characteristics of a population (greek letters)  Statistics : summarizes characteristics of a sample (standard alphabet)  You use  statistics  to estimate  parameters     Independent variable : it can take different values, it affects or determines a  dependent variable . It can be called predictor, explanatory, control or input variable.  Dependent variable : it can take different values in response to an  independent variable . Also known as response, outcome or target variable.    Scale of measurement : variable's classification   Quantitative/numerical variables : counts or measurements, you can perform arithmetical operations with it  Discrete data : variables that can have only a countable number of values within a measurement range  Continuous data : variables that are measured on a scale that has infinite number of values and has no breaks or jumps  Interval scale data : it can be rank-ordered like ordinal data but also has a sensible spacing of observations such that differenes between measurements are meaningful but it lacks a true zero (ratios are meaningless)  Ratio scale data : it is rank-ordered with meaningful spacing and also includes a true zero point and can therefore accurately indicate the ratio difference between two spaces on the measurement scale      Categorical/attribute variables : variables that denote groupings or labels  Nominal data (qualitative/classification variable) : exhibits no ordering within its observed levels, groups or categories  Ordinal data : the observed labels can be ordered in some meaningful way that implies that the differences between the groups or categories are due to magnitude       Univariate analysis  provides techniques for analyzing and describing a sigle variable. It reveals patterns in the data by looking at the  range  of values, measures of  dispersion , the  central tendecy  of the values and  frequency distribution .  Bivariate analysis  describes and explains the relationships between two variables and how they change or covary together. It include techniques such as  correlation analysis  and  chi-square tests of independance .  Multivariate/Multivariable analysis  examines two or more variables at the same time in order to understand the relationships among them.   Techniques such as  mutiple linear regression  and n-way  ANOVA  are typically called  multivariable  analysis (only one response variable).   Techniques such as  factora analysis  and  clustering  are typically called  mutivariate  analysis (they consider more than one response variable).      Descriptive Statistics  Measures of central tendencies : mean (affected by outliers), median (less sensitive to outliers), mode     Percentile  Quartile       25th  1st / lower / Q1     50th  2nd / middle / Q2  Median    75th  3rd / upper / Q3      The  interquartile range (IQR)  is the difference between Q1 and Q3, it is a  robust estimate of the variability  because changes in the upper/lower 25% of the data do not affect it. If there are  outliers  in the data, then the IQR is a more reliable measure of the spread than the overall range.  The  coefficient of variation (CV)  is a measure of the standard deviation expressed as a percentage of the mean ($c_v = \\sigma / 100 \\mu$).   Normal distribution     Intervals  Percentage contained      $\\mu \\pm \\sigma$  68%    $\\mu \\pm 2 \\sigma$  95%    $\\mu \\pm 3 \\sigma$  99%     How to check the normality of a sample?   Compare the  mean  and the  median : if they are nearly equal, that is an indicator of symmetry (requirement for normality)  Check that  skewness  and  kurtosis  are close to 0:  If both are greater than 1 or less than -1: data is not normal  If either is greater than 2 or less than -2: data is not normal     Statistical summaries    Skewness  and  kurtosis  measure certain aspects of the shape of a distribution (they are  0  and  3  for a normal distribution, although SAS has standardized both to 0)   Skewness  measures the tendency of your data to be more spread out on one side of the mean than on the other (asymmetry of the distribution).   You can think of the direction of skewness as the direction the data is trailing off to.   A  right-skewed  distribution tells us that the mean is  greater than the median .    Kurtosis  measures the tendency of your data to be concentrated toward the center or toward the tails of the distribution (peakedness of the data, tail thickness).   A  negative kurtosis (platykurtic distribution)  means that the data has lighter tails than in a normal distribution.   A  positive kurtosis (leptokurtic/heavy-tailed/outlier-prone distribution)  means that the data has heavier tails and is more concentrated around the mean than a normal distribution.  Rectangular, bimodal and multimodal distributions tend to have low values of kurtosis.  Asymmetric distributions  also tend to have nonzero kurtosis. In these cases, understanding kurtosis is considerably more complex and can be difficult to assess visually.", 
            "title": "Basic Statistical Concepts"
        }, 
        {
            "location": "/statistics/introduction/#proc-surveyselect", 
            "text": "How to generate random (representative) samples (population subsets):  1\n2\n3\n4\n5\n6\n7 PROC SURVEYSELECT DATA=SAS-data-set \n                  OUT=name-of-output-data-set\n                  METHOD=method-of-random-sampling\n                  SEED=seed-value \n                  SAMPSIZE=number-of-observations-desired;\n      STRATA stratification-variable(s); \nRUN;    METHOD  specifies the random sampling method to be used. For simple random sampling without replacement, use  METHOD=SRS . For simple random sampling with replacement, use  METHOD=URS . For other selection methods and details on sampling algorithms, see the SAS online documentation for  PROC SURVEYSELECT .  SEED  specifies the initial seed for random number generation. If no  SEED  option is specified, SAS uses the system time as its seed value. This creates a different random sample every time the procedure is run.  SAMPSIZE  indicates the number of observations to be included in the sample. To select a certain fraction of the original data set rather than a given number of observations, use the  SAMPRATE  option.", 
            "title": "PROC SURVEYSELECT"
        }, 
        {
            "location": "/statistics/introduction/#picturing-your-data", 
            "text": "", 
            "title": "Picturing Your Data"
        }, 
        {
            "location": "/statistics/introduction/#proc-univariate", 
            "text": "Plots that can be produced with this procedure:   Histograms  Normal probability plots : expected percentiles from standard normal vs actual data values", 
            "title": "PROC UNIVARIATE"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgscatter", 
            "text": "Plots that can be produced with this procedure:   Scatter plots : you can create a  single-cell  (simple Y by X) scatter plot, a  multi-cell  scatter plot with multiple independent scatter plots in a grid and a  scatter plot matrix , which produces a matrix of scatter plots comparing multiple variables.", 
            "title": "PROC SGSCATTER"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgplot", 
            "text": "Plots that can be produced with this procedure:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 PROC SGPLOT DATA=SAS-data-set  options ;\n        DOT category-variable  /options ;\n        HBAR category-variable  /options ;\n        VBAR category-variable  /options ;\n        HBOX response-variable  /options ;\n        VBOX response-variable  /options ;\n        HISTOGRAM response-variable  /options ;\n        SCATTER X=variable Y=variable  /options ;\n        NEEDLE X=variable Y=numeric-variable  /options ;\n        REG X=numeric-variable Y=numeric-variable  /options ;\nRUN;   Anywhere in the procedure you can add  reference lines :  1\n2\n3\n4 REFLINE variable | value-1  ...   value-n   /option(s) \n\n/* Example: */\nREFLINE 1200 / axis=y lineattrs=(color=blue);    Note  The order on which you define the parts of the plot will the determined the order on which it is displayed (if you want to send a  REFLINE  to the back, define it first)    Scatter plots ( SCATTER )  Line graphs  Histograms ( HISTOGRAM )  with overlaid distribution curves  Regression lines ( REG )  with confidence and prediction bands  Dot plots ( DOT )  Box plots ( HBOX / VBOX ) : it makes it easy to see how spread out your data is and if there are any outliers. The box represents the middle 50% of your data (IQR). The lower/middle/upper  line of the box  represent Q1/Q2/Q3. The  diamond  denotes the mean (easy to check how close the mean is to the median). The  whiskers  extend as far as the data extends to a maximum length of 1.5 times the IQR above Q3. Any data points farther than this distance are considered possible outliers and are represented in this plot as  circles .  Bar charts ( HBAR / VBAR )  Needle plot ( NEEDLE ) : creates a plot with needles connecting each point to the baseline  You can also  overlay plots together  to produce many different types of graphs", 
            "title": "PROC SGPLOT"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgpanel", 
            "text": "Plots that can be produced with this procedure:   Panels of plots  for different levels of a factor or several different time periods depending on the classification variable  Side-by-side histograms  which provide a visual comparison for your data", 
            "title": "PROC SGPANEL"
        }, 
        {
            "location": "/statistics/introduction/#proc-sgrender", 
            "text": "Plots from graphs templates you have modified or written yourself    To specify options for graphs you submit the  ODS GRAPHICS  statement:  1 ODS GRAPHICS ON  options ;    To select/exclude specific test results, graphs or tables from you output, you can use  ODS SELECT  and  ODS EXCLUDE  statements.  You can use ODS templates to modify the layout and details of each graph  You can use ODS styles to control the general appearance and consistency of yous graphs and tables (by default  HTMLBLUE ).   Another way to control your output is to use the  PLOTS  option which is usually available in the procedure statement:  1 PROC UNIVARIATE DATA=SAS-data-set PLOTS=options;   This option enables you to specify which graphs SAS should create, either in addtion or instead of the default plots.", 
            "title": "PROC SGRENDER"
        }, 
        {
            "location": "/statistics/introduction/#confidence-intervals-for-the-mean", 
            "text": "A  point estimator  is a sample statistic used to estimate a population parameter  An estimator takes on different values from sample to sample, so it's important to know its variance  A statistic that measures the variability of your estimator is the  standard error  It differs from the standard deviation: the  standard deviation  deals with the variability of your data while  standard error  deals with the variability of you sample statistic   E.g.:  $standard \\ error \\ of \\ the \\ mean = standard \\ deviation/ \\sqrt{sample \\ size}$  The  distribution of sample means  is always less variable than the data.   Because we know that point estimators vary from sample to sample, it would be nice to have an estimator of the mean that directly accounts for this natural variability  The  interval estimator  gives us a range of values that is likely to contain the population mean  It is calculated from the  standard error  and a value that is determined by the  degree of certainty  we require ( significance level )  Confidence intervals  are a type of interval estimator used to estimate the population mean  You can make the confidence interval narrower by increasing the sample size and by decreasing the confidence level   $CI = sample \\ mean \\pm quantile \\cdot standard \\ error$   The  CLM  option of  PROC MEANS  calculates the confidence limits for the mean, you can add  ALPHA=  to change the default 0.05 value for a 95% confidence level  The  central limit theorem  states that the distribution of sample means is approximately normal regardless of the population distribution's shape, if the sample size is large enough (~30 observations)", 
            "title": "Confidence Intervals for the Mean"
        }, 
        {
            "location": "/statistics/introduction/#hypothesis-testing", 
            "text": "The  null hypothesis  ($H_0$) is what you assume to be true when you start your analysis  The  alternative hypothesis  ($H_a$ or $H_1$) is your initial research hypothesis, that is, your proposed explanation   Decision-making process:   Define null and alternative hypothesis  Specify significance level (type I error rate)  Collect data  Reject or fail to reject the null hypothesis     The type I and II errors are  inversely related : as one type increases the other decreases   The  power  is the probability of a  correct rejection  = 1 -   It is the ability of the statistical test to detect a true difference   It is the ability to successfully reject a false null hypothesis    A  p-value  measures the probability of observing a value as extreme as the one observed   The p-value is used to determine  statistical significance  It helps you assess whether you should reject the null hypothesis     The  p-value  is affected by:   The  effect size : the difference between the observed statistic and the hypothesized value  The  sample size : the larger the sample size, the more sure you are about the sample statistics, the lower the p-value is     A reference distribution enables you to quantify the probability (p-value) of observing a particular outcome (the calculated sample statistic) or a more extreme outcome, if the nul hypothesis is true   Two common reference distributions for statistical hypothesis testing are the  t distribution  and the  F distribution  These distributions are characterized by the  degrees of freedom  associated with your data  The  t distribution  arises when you're making inferences about a population mean and the population standard deviation is unknown and has to be estimated from the data  It is  approximately normal  as the  sample size grows larger  The t distribution is a  symmetric distribution  like the normal distribution except that the t distribution has  thicker tails  The  t statistic  is positive/negative when the sample is more/less than the hypothesized mean  If the data doesn't come from a normal distribution, then the t statistic approximately follows a t distribution as long as the sample size is large ( central limit theorem )     Calculation with  PROC UNIVARIATE :  1\n2\n3\n4\n5\n6\n7\n8 ODS SELECT TESTSFORLOCATION;\nPROC UNIVARIATE DATA=SAS-data-set MU0=number alpha=number;\n  VAR variable(s);\n  ID variable_to_relate;\n  HISTOGRAM variables  /options ;\n  PROBPLOT variables  /options ;\n  INSET keywords  /options ;\nRUN;    TESTSFORLOCATION  displays only the p-values calculation  By default  MU0 = 0", 
            "title": "Hypothesis Testing"
        }, 
        {
            "location": "/statistics/anova/", 
            "text": "Chapter summary in SAS\n\n\n\n\nGraphical Analysis of Associations\n\n\n\n\nBefore analyzing your data, you need to have a general idea of any associations between \npredictor variables\n and \nresponse variables\n\n\nAn \nassociation\n exists between two variables when the expected value of one variable differs at different levels of the other variable\n\n\nOne method for doing this is to conduct a \ngraphical analysis\n of your data\n\n\nAssociations between \ncategorical\n predictor variable and a \ncontinuous\n response variable can be explored with \nSGPLOT\n to product \nbox plots\n (box-and-whisker plots) (\nX\n predictor variable vs \nY\n response variable)\n\n\nIf the \nregression line\n conecting the means of Y at each value of X is not horizontal \nthere might be an association\n between them\n\n\nIf the \nregression line\n is horizontal \nthere is no association\n: knowing the value of X doesn't tell you anything about the value of Y\n\n\n\n\nPROC SGPLOT\n\n\n1\n2\n3\n4\nPROC SGPLOT DATA=SAS-data-set;\n    VBOX response-variable / CATEGORY=predictor-variable\n    CONNECT=MEAN DATALABEL=outlier-ID-variable;\nRUN;\n\n\n\n\n\n\nTwo-Sample t-Tests\n\n\n\n\nYou can use a \none-sample t-test\n to determine if the mean of a population is equal to a particular value or not\n\n\nWhen you collect a random sample of independent observations from two different populations, you can perform a \ntwo-sample t-test\n\n\n\n\nWhen you compare the means of two populations using a \ntwo-sample t-test\n you make three assumptions:\n\n\n\n\nThe data contains independent observations\n\n\nThe distributions of the two populations are normal (check histograms and normal probability/Q-Q plots)\n\n\nThe variances in these normal distributions are equal (\nF-test\n is the formal way to verify this assumption)\n\n\n\n\n$F$ statistic: $F=max(s_1^2,s_2^2)/min(s_1^2,s_2^2) \\ge 1$\n\n\n$H_0$: \n$_1^2$ $=$  \n$_2^2\\rightarrow F \\approx 1$\n\n\n$H_a$: \n$_1^2$ $\\ne$  \n$_2^2\\rightarrow F\\gt 1$\n\n\nThe \nPr\nF\n value in the \nEquality of Variances\n table represents the \np-value\n of the F-test for equal variances\n\n\n\n\nTwo-sided Tests\n\n\nPROC TTEST\n\n\n\n\nPROC TTEST\n performs a two-sided two-sample t-test by default (confidence limits and ODS graphics included)\n\n\nIt \nautomatically test the assumption of equal variances\n and provides an exact two-sample t-test (\npooled\n) when the assumptions are met and an approximate t-test (\nscatterthwaite\n) when it is not met \n\n\nThe pooled and scatterthwaite t-tests are equal when the variances are equal\n\n\n\n\n1\n2\n3\n4\n5\nPROC TTEST DATA=SAS-data-set \noptions\n\n    plots(shownull)=interval;         \\* shownull = vertical reference line at the mean value of H0 *\\\n    CLASS variable;                   \\* Classification variable *\\\n    VAR variable(s);                  \\* Continuous response variables *\\\nRUN;\n\n\n\n\n\n\n\n\nOne-sided Tests\n\n\n\n\nIt \ncan increase the power\n of a statistical test, meaning that if you are right about the direction of the true difference, you will more likely detect a significant difference with a one-sided test than with a tow-sided test\n\n\nThe difference between the mean values for the null hypothesis will be defined by the alphabetical order of the classification variables (e.g.: female - male)\n\n\n\n\n1\n2\n3\n4\n5\nPROC TTEST DATA=SAS-data-set \n    plots(only shownull)=interval H0=0 SIDES=u;     \\* only = suppress the default plots; u/l = upper/lower-tailed t-test  *\\\n    CLASS variable;                                 \\* Classification variable *\\\n    VAR variable(s);                                \\* Continuous response variables *\\\nRUN;\n\n\n\n\n\n\nOne-Way ANOVA\n\n\nWhen you want to determine whether there are significant differences between the \nmeans of two or more populations\n, you can use analysis of variance (ANOVA).\n\n\n\n\nYou have a continuous dependent (\nresponse\n) variable and a categorical independent (\npredictor\n) variable\n\n\nYou can have \nmany levels of the predictor variable\n, but you can have \nonly one predictor variable\n\n\nThe \nsquared value of the t statistic\n for a two-sample t-test is equal to the \nF statistic\n of a one-way ANOVA with two populations\n\n\nWith ANOVA the $H_0$ is that all of the population means are equal and $H_a$ is that not all the population means are equal (at least one mean is different)\n\n\n\n\nTo perform an ANOVA test you make three assumptions:\n\n\n\n\nYou have a \ngood, random, representative sample\n\n\nThe \nerror terms are normally distributed\n\n\nThe \nresiduals\n (each observation minus its group mean) are estimates of the error term in the model so you verify this assumption by examining diagnostic plots of the residuals (if they are approximately normal, the error terms will be too)\n\n\nIf your sample sizes are reasonably large and approximately equal across groups, then only severe departures from normality are considered a problem\n\n\nResiduals always sum to 0, regardless of the number of observations.\n\n\n\n\n\n\nThe \nerror terms have equal variances\n across the predictor variable levels: you can conduct a formal test for equal variances and also plot the residuals vs predicted values as a way to graphically verify this assumption\n\n\n\n\nPROC GLM\n\n\nYou can use \nPROC GLM\n to verify the ANOVA assumptions and perform the ANOVA test. It fits a general linear model of which ANOVA is a special case and also displays the sums of squares associated with each hypothesis it tests.\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);     /* print each plot on a separated page */\n    CLASS variable(s);\n    MODEL dependents=intependents \n/options\n;\n    MEANS effects / HOVTEST \n/options\n;    \nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nHOVTEST\n: homogeneity of variance test option (Levene's test by default) + plot of residuals vs predicted values (means)\n\n\n\n\n\n\n\n\nOf the \nbetween-group variability\n is significantly larger than the \nwithin-group variability\n, you reject the null that all the group means are equal\n\n\nYou partition out the variability using sums of squares: \n\n\nBetween-group\n variation: also called Model Sum of Squares (SSM): $\\sum n_i (\\overline Y_i- \\overline {\\overline Y})^2$\n\n\nWithin-group\n variation: also called Error Sum of Squares (SSE): $\\sum \\sum (Y_{ij}- \\overline Y_i)^2$\n\n\nTotal\n variation: also called the Total Sum of Squares (SST): $\\sum \\sum (Y_{ij}- \\overline {\\overline Y})^2$\n\n\n\n\n\n\nSSM\n and \nSSE\n represent pieces of \nSST\n: the SSM is the variability explanied by the predictor variable levels and SSE the variability not explained by the predictor variable levels\n\n\nYou want the larger piece of the total to be better represented by what you can explain (SSM) vs what you cant't explain (SSE) \n\n\n\n\nANOVA with Data from a Randomized Block Design\n\n\nIn an \nobservational study\n, you often examine what already occurred, and therefore have little control over factors contributing to the outcome. In a \ncontrolled experiment\n, you can manipulate the \nfactors of interest\n and can more reasonably claim causation.\n\n\n\n\nThe variation due to the \nnuisance factors\n (fundamental to the probabilistic model but are no longer of interest) is part of the random variation that the error sum of squares accounts for.\n\n\nIncluding a \nblocking variable\n in the model is in essence like adding a second predictor variable to the model in terms of the way you write it\n\n\nThe way you set up your experiment and data collection is what defines it as a blocking factor\n\n\nAlthough you're not specifically interested in its effect, \ncontrolling the blocking variable makes it easier to detect an effect of the factor of interest\n\n\nIn a model that does not include a blocking variable, its effects are lumped into the error term of the model (unaccounted for variation)\n\n\nWhen you include a blocking variable in your ANOVA model, any effects caused by the nuisance factors that are common within a sector are accounted for in the \nmodel sum of squares rather than the error sum of squares\n\n\n\n\nYou make two more assumptions when you include a blocking factor in the model:\n\n\n\n\nPrimary variable levels are \nrandomly assigned\n within each block\n\n\nThe effects of the primary variable are \nconstant across the levels\n of the blocking factor (the effects don't depend on the block they are in, there are \nno interactions\n with the blocking variable)\n\n\n\n\n\n\nNote\n\n\nLevene's test for homogeneity is \nonly available for one-way ANOVA models\n, so in this case, you have to use the Residuals by Predicted plot.\n\n\n\n\nPROC GLM\n\n\n1\n2\n3\n4\n5\n6\nPROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);   /* print each plot on a separated page */\n    CLASS variable(s) blocking-factor(s);\n    MODEL dependents=intependents blocking-factor(s)\n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nRule of thumb\n: if the \nF-value is \n 1\n, then it helped to add the blocking factor in your model \n\n\nIf you compare the MSE (\nMean Square\n in the table) without and with including the blocking variable in the model,  there is a drop of its value meaning that \nyou have been able to account for a bit more of the unexplained variability due to the nuisance factors\n helping o have more precise estimates of the effect of your primary variable\n\n\nIt is also reflected in the \nR-Square\n value that is increased when a blocking factor is added to the model\n\n\nThanks to adding a blocking variable to your model you can get your primary variable to be significant\n\n\nThe \nType III SS\n at the bottom of the output tests for the difference due to each variable, controlling for or adjusting for the other variable\n\n\n\n\nANOVA Post Hoc Tests\n\n\nThis test is used to determine which means differ from other means and control the error rate using \nmultiple comparison method\n.\n\n\nAssuming the null hypothesis is true for your different comparisons, the probability that you conclude a difference exist at least one time when there really  isn't a difference increases with the more tests you perform. So \nthe chance that you make a Type I error increases each time you conduct a statistical test\n.\n\n\n\n\nThe \ncomparisonwise error rate (CER)\n is the probability of a Type I error on a single pairwise test (\n)\n\n\nThe \nexperimentwise error rate (EER)\n is the probability of making at least one Type I error when performing the whole set of comparisons. It takes into consideration the number of pairwise comparisons you make, so it increases as the number of tests increase: \n\n \n EER=1-(1-\\alpha)^{comparisons} \n \n\n\n\n\nTukey's Multiple Comparison Method\n\n\n\n\nThis method, which is also known as the \nHonestly Significant Difference\n test, is a popular multiple comparison test that \ncontrols the EER\n\n\nThis tests compares all possible pairs of means, so \nit can only be used when you make pairwise comparisons\n\n\nThis method controls $EER=\\alpha$ when \nall possible pairwise comparisons are considered\n and controls $EER\n\\alpha$ when fewer than all pairwise comparisons are considered\n\n\n\n\nDunnett's Multiple Comparison Method\n\n\n\n\nThis method is a specialised multiple comparison test that allows you to \ncompare a single control group to all other groups\n\n\nIt controls $EER \\le \\alpha$ when all groups are compared to the reference group (control)\n\n\nIt accounts for the correlation that exists between the comparisons and \nyou can conduct one-sided tests\n of hypothesis against the reference group\n\n\n\n\nPROC GLM\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nPROC GLM DATA=SAS-data-set;\n    CLASS variable(s);\n    MODEL dependents=intependents \n/options\n;\n    LSMEANS effects \n/options-test-1\n;  \n    LSMEANS effects \n/options-test-2\n;\n    [...]\n    LSMEANS effects \n/options-test-n\n;  \nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nPDIFF=ALL\n requests p-values for the differences between \nALL\n the means and a \ndiffogram\n is produced automatically displaying all pairwise least square means differences and indicating which are significant\n\n\nIt can be undestood as a least squares mean by least squares mean plot\n\n\nThe point estimates for differences between the means for each pairwise comparison can be found at the intersections of the gray grid lines (intersection of appropriate indexes)\n\n\nThe red/blue diagonal lines show the \nconfidence intervals for the true differences of the means\n for each pairwise comparison\n\n\nThe grey 45$^{\\circ}$ reference line represents equality of the means (if the confidence interval crosses over it, then there is no significant difference between the two groups and the diagonal line for the pair will be \ndashed and red\n; if the difference is significant the line will be \nsolid and blue\n)\n\n\n\n\n\n\n\n\nThe \nADJUST=\n option specifies the adjustment method for multiple comparisons\n\n\nIf you don't specify an option SAS uses the \nTukey method by default\n, if you specify \nADJUST=Dunnett\n the GLM procedure produces multiple comparisons using \nDunnett's method\n and a \ncontrol plot\n \n\n\nThe control plot displays the least squares mean and confidence limits of each group compared to the reference group \n\n\nThe middle \nhorizontal line represents its least square mean value\n (you can see the arithmetic mean value un the \nupper right corner\n of the graph)\n\n\nThe \nshaded area\n goes from the \nlower decision limit (LDL)\n to the \nupper decision limit (UDL)\n\n\nThere is a vertical line for each group that you're comparing to the reference (control) group. If a \nvertical line extends past the shaded area\n, then the group represented by the line is \nsignificantly different\n (small p-value) than the reference group \n\n\n\n\n\n\n\n\n\n\n\n\nPDIFF=CONTROLU('value')\n specifies the control group for the Dunnett's case: the direction of the sign in Ha is the same as the direction you are testing, so this is a \none-sided upper-tailed t-test\n\n\nIf you specify \nADJUST=T\n SAS will make no adjustments for multiple comparisons: is not recommended as there's a tendency to find \nmore significant pairwise differences than might actually exist\n\n\n\n\nTwo-Way ANOVA with Interactions\n\n\nWhen you have a continuous response variable and \ntwo categorical predictor variables\n, you use the \ntwo-way ANOVA model\n\n\n\n\nEffect\n: the magnitude of the expected change in the response variable presumably caused by the change in value of a predictor variable in the model\n\n\nIn addition, the variables in a model can be referred to as effects or terms\n\n\nMain effect\n: is the effect of a single predictor variable\n\n\nInteraction effects\n: when the relationship of the response variable with a predictor changes with the changing of another predictor variable (the effect of one variable depends on the value of the other variable)\n\n\n\n\n\n\nWhen you consider an ANOVA with more than one predictor variable, it's called \nn-way ANOVA\n where \nn\n represents the number of predictor variables\n\n\n\n\nThe analysis in a \nrandomized block design\n is actually a \nspecial type of two-way ANOVA\n in which you have one factor of interest and one blocking factor\n\n\nWhen you analyze a two-way ANOVA with interactions, you first look at any tests for \ninteractions among the factors\n\n\nIf there is \nno interaction between the factors\n you can interpret the tests for the individual factor effects to determine their significance/non-significance\n\n\nIf an \ninteraction exists between any factors\n, the tests for the individual factor effects might be misleading due to masking of these effect by the interaction (this is specially true for unbalanced data with different number of observations for each combination of groups)\n\n\n\n\n\n\nWhen the interaction is not statistically significant \nyou can analyze the main effect with the model in its current form\n (generally the method you use when you analyze designed experiments)\n\n\nEven when you analyze designed experiments, some statisticians might suggest that if the interaction is not significant, \nyou can delete the interaction effect from your model, rerun the model and then just analyze the main effects\n increasing the power of the main effects test\n\n\nIf the \ninteraction term is significant\n, it is good practice to keep the main effect terms that make up the interaction in the model, whether they are significant or not (this preserves model hierarchy)\n\n\nYou have to make the \nsame three assumptions used in the ANOVA test\n\n\nThe interaction terms are also called \nproduct terms\n or \ncrossed effects\n\n\n\n\nPROC GLM\n\n\n1\n2\n3\n4\n5\n6\n7\nPROC GLM DATA=SAS-data-set;\n    CLASS independent1 independent2;\n    MODEL dependent = independent1 independent2 independent1*independent2;\n    or\n    MODEL dependent = independent1 | independent2;\nRUN;\nQUIT;\n\n\n\n\n\n\nThis program is \nfitting to this model\n:\n\n\n\n\nY_{ijk}=\\mu + \\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+\\epsilon_{ijk}\n\n\n\n\n dependent = overall mean + intependent1 + independent2 + interaction12 + unaccounted for variation \n\n\n\n\nIn \nmost situations\n you will want to use the \nType III SS\n\n\nThe \nType I SS (sequential)\n are the sums of squares you obtain from fitting the effects in the order you specify in the model \n\n\nThe \nType III SS (marginal)\n are the sums of squares you obtain from fitting each effect after all the other terms in the model, that is the sums of squares for each effect corrected for the other terms in the model\n\n\nWhen examining these results you first have to \nlook at the interaction term and if it's significant\n (p-value), the \nmain effects don't tell you the whole story\n. It that is the case, you don't need to worry all that much about the significance of the main effects at this point for two reasons:\n\n\nYou know that the effect of each variable1 level changes for the different variable2 levels\n\n\nYou want to include the main effects in the model, whether they are significant or not, to preserve model hierarchy\n\n\n\n\n\n\nYou can analyze the interaction between terms by looking at the \ninteraction plot\n that SAS produces by default when you include an interaction term in the model\n\n\nTo analyze and interpret the effect of one of the interacting variables you need to add the \nLSMEANS\n statement to your program\n\n\n\n\n1\n2\n3\n4\n5\n6\nPROC GLM DATA=SAS-data-set ORDER=INTERNAL PLOTS(ONLY)=INTPLOT;\n    CLASS independent1 independent2;\n    MODEL dependent = intependent1 independent2 independent1*independent2;\n    LSMEANS independent1*independent2 / SLICE= independent1;\nRUN;\nQUIT;\n\n\n\n\n\n\nSAS creates two types of mean plots when you use the \nLSMEANS\n statement with an interaction term:\n\n\n\n\nThe first plot displays the \nleast squares mean (LS-Mean) for every effect level\n \n\n\nThe second plot contains the same information rearranged so you can \nlook a little closer at the combination levels\n\n\n\n\n\n\nSTORE\n statement\n\n\nYou can add a \nSTORE\n statement to save your analysis results in an \nitem store\n (a binary file format that cannot be modified). This allows you to \nrun post-processing analysis\n on the stored results even if you no longer have access to the original data set. The \nSTORE\n statement applies to the following SAS/STAT procedures: \nGENMOD\n, \nGLIMMIX\n, \nGLM\n, \nGLMSELECT\n, \nLOGISTIC\n, \nMIXED\n, \nORTHOREG\n, \nPHREG\n, \nPROBIT\n, \nSURVEYLOGISTIC\n, \nSURVEYPHREG\n, and \nSURVEYREG\n.\n\n\n1\n2\nSTORE \nOUT\n=\nitem-store-name\n    \n/ LABEL=\nlabel\n;\n\n\n\n\n\n\n\n\nitem-store-name\n is a usual one- or two-level SAS name, similar to the names that are used for SAS data sets\n\n\nlabel\n identifies the estimate on the output (is optional)\n\n\n\n\n\n\nPROC PLM\n\n\nTo perform post-fitting statistical analysis and plotting for the contents of the store item, you use \nPROC PLM\n. The statements and options that are available vary depending upon which procedure you used to produce the item store.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nPROC PLM RESTORE=item-store-specification \noptions\n;\n    EFFECTPLOT INTERACTION(SLICEBY=variable) \nplot-type\n \n(plot-definition\n \noptions)\n / CLM \n/ options\n;\n    LSMEANS \nmodel-effects\n \n/ options\n;\n    LSMESTIMATE model-effect \nlabel\n values\n        \ndivisor\n=n\n,...\nlabel\n values\n        \ndivisor\n=n\n \n/ options\n;\n    SHOW options;\n    SLICE model-effect / SLICEBY=variable ADJUST=tukey \n/ options\n;\n    WHERE expression;\nRUN;\n\n\n\n\n\n\n\n\nRESTORE\n specifies the source item store for processing\n\n\nEFFECTPLOT\n produces a display of the fitted model and provides options for changing and enhancing the displays\n\n\nLSMEANS\n computes and compares least squares means (LS-means) of fixed effects\n\n\nLSMESTIMATE\n provides custom hypothesis tests among least squares means\n\n\nSHOW\n uses \nODS\n to display contents of the item store. This statement is useful for verifying that the contents of the item store apply to the analysis and for generating \nODS\n tables.\n\n\nSLICE\n provides a general mechanism for performing a partitioned analysis of the LS-means for an interaction (analysis of simple effects) and it uses the same options as the \nLSMEANS\n statement\n\n\nWHERE\n is used in the PLM procedure when the item store contains \nBY-variable\n information and you want to apply the \nPROC PLM\n statements to only a subset of the BY groups", 
            "title": "Analysis of Variance (ANOVA)"
        }, 
        {
            "location": "/statistics/anova/#graphical-analysis-of-associations", 
            "text": "Before analyzing your data, you need to have a general idea of any associations between  predictor variables  and  response variables  An  association  exists between two variables when the expected value of one variable differs at different levels of the other variable  One method for doing this is to conduct a  graphical analysis  of your data  Associations between  categorical  predictor variable and a  continuous  response variable can be explored with  SGPLOT  to product  box plots  (box-and-whisker plots) ( X  predictor variable vs  Y  response variable)  If the  regression line  conecting the means of Y at each value of X is not horizontal  there might be an association  between them  If the  regression line  is horizontal  there is no association : knowing the value of X doesn't tell you anything about the value of Y", 
            "title": "Graphical Analysis of Associations"
        }, 
        {
            "location": "/statistics/anova/#proc-sgplot", 
            "text": "1\n2\n3\n4 PROC SGPLOT DATA=SAS-data-set;\n    VBOX response-variable / CATEGORY=predictor-variable\n    CONNECT=MEAN DATALABEL=outlier-ID-variable;\nRUN;", 
            "title": "PROC SGPLOT"
        }, 
        {
            "location": "/statistics/anova/#two-sample-t-tests", 
            "text": "You can use a  one-sample t-test  to determine if the mean of a population is equal to a particular value or not  When you collect a random sample of independent observations from two different populations, you can perform a  two-sample t-test   When you compare the means of two populations using a  two-sample t-test  you make three assumptions:   The data contains independent observations  The distributions of the two populations are normal (check histograms and normal probability/Q-Q plots)  The variances in these normal distributions are equal ( F-test  is the formal way to verify this assumption)   $F$ statistic: $F=max(s_1^2,s_2^2)/min(s_1^2,s_2^2) \\ge 1$  $H_0$:  $_1^2$ $=$   $_2^2\\rightarrow F \\approx 1$  $H_a$:  $_1^2$ $\\ne$   $_2^2\\rightarrow F\\gt 1$  The  Pr F  value in the  Equality of Variances  table represents the  p-value  of the F-test for equal variances   Two-sided Tests", 
            "title": "Two-Sample t-Tests"
        }, 
        {
            "location": "/statistics/anova/#proc-ttest", 
            "text": "PROC TTEST  performs a two-sided two-sample t-test by default (confidence limits and ODS graphics included)  It  automatically test the assumption of equal variances  and provides an exact two-sample t-test ( pooled ) when the assumptions are met and an approximate t-test ( scatterthwaite ) when it is not met   The pooled and scatterthwaite t-tests are equal when the variances are equal   1\n2\n3\n4\n5 PROC TTEST DATA=SAS-data-set  options \n    plots(shownull)=interval;         \\* shownull = vertical reference line at the mean value of H0 *\\\n    CLASS variable;                   \\* Classification variable *\\\n    VAR variable(s);                  \\* Continuous response variables *\\\nRUN;    One-sided Tests   It  can increase the power  of a statistical test, meaning that if you are right about the direction of the true difference, you will more likely detect a significant difference with a one-sided test than with a tow-sided test  The difference between the mean values for the null hypothesis will be defined by the alphabetical order of the classification variables (e.g.: female - male)   1\n2\n3\n4\n5 PROC TTEST DATA=SAS-data-set \n    plots(only shownull)=interval H0=0 SIDES=u;     \\* only = suppress the default plots; u/l = upper/lower-tailed t-test  *\\\n    CLASS variable;                                 \\* Classification variable *\\\n    VAR variable(s);                                \\* Continuous response variables *\\\nRUN;", 
            "title": "PROC TTEST"
        }, 
        {
            "location": "/statistics/anova/#one-way-anova", 
            "text": "When you want to determine whether there are significant differences between the  means of two or more populations , you can use analysis of variance (ANOVA).   You have a continuous dependent ( response ) variable and a categorical independent ( predictor ) variable  You can have  many levels of the predictor variable , but you can have  only one predictor variable  The  squared value of the t statistic  for a two-sample t-test is equal to the  F statistic  of a one-way ANOVA with two populations  With ANOVA the $H_0$ is that all of the population means are equal and $H_a$ is that not all the population means are equal (at least one mean is different)   To perform an ANOVA test you make three assumptions:   You have a  good, random, representative sample  The  error terms are normally distributed  The  residuals  (each observation minus its group mean) are estimates of the error term in the model so you verify this assumption by examining diagnostic plots of the residuals (if they are approximately normal, the error terms will be too)  If your sample sizes are reasonably large and approximately equal across groups, then only severe departures from normality are considered a problem  Residuals always sum to 0, regardless of the number of observations.    The  error terms have equal variances  across the predictor variable levels: you can conduct a formal test for equal variances and also plot the residuals vs predicted values as a way to graphically verify this assumption", 
            "title": "One-Way ANOVA"
        }, 
        {
            "location": "/statistics/anova/#proc-glm", 
            "text": "You can use  PROC GLM  to verify the ANOVA assumptions and perform the ANOVA test. It fits a general linear model of which ANOVA is a special case and also displays the sums of squares associated with each hypothesis it tests.  1\n2\n3\n4\n5\n6\n7 PROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);     /* print each plot on a separated page */\n    CLASS variable(s);\n    MODEL dependents=intependents  /options ;\n    MEANS effects / HOVTEST  /options ;    \nRUN;\nQUIT;    HOVTEST : homogeneity of variance test option (Levene's test by default) + plot of residuals vs predicted values (means)     Of the  between-group variability  is significantly larger than the  within-group variability , you reject the null that all the group means are equal  You partition out the variability using sums of squares:   Between-group  variation: also called Model Sum of Squares (SSM): $\\sum n_i (\\overline Y_i- \\overline {\\overline Y})^2$  Within-group  variation: also called Error Sum of Squares (SSE): $\\sum \\sum (Y_{ij}- \\overline Y_i)^2$  Total  variation: also called the Total Sum of Squares (SST): $\\sum \\sum (Y_{ij}- \\overline {\\overline Y})^2$    SSM  and  SSE  represent pieces of  SST : the SSM is the variability explanied by the predictor variable levels and SSE the variability not explained by the predictor variable levels  You want the larger piece of the total to be better represented by what you can explain (SSM) vs what you cant't explain (SSE)", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#anova-with-data-from-a-randomized-block-design", 
            "text": "In an  observational study , you often examine what already occurred, and therefore have little control over factors contributing to the outcome. In a  controlled experiment , you can manipulate the  factors of interest  and can more reasonably claim causation.   The variation due to the  nuisance factors  (fundamental to the probabilistic model but are no longer of interest) is part of the random variation that the error sum of squares accounts for.  Including a  blocking variable  in the model is in essence like adding a second predictor variable to the model in terms of the way you write it  The way you set up your experiment and data collection is what defines it as a blocking factor  Although you're not specifically interested in its effect,  controlling the blocking variable makes it easier to detect an effect of the factor of interest  In a model that does not include a blocking variable, its effects are lumped into the error term of the model (unaccounted for variation)  When you include a blocking variable in your ANOVA model, any effects caused by the nuisance factors that are common within a sector are accounted for in the  model sum of squares rather than the error sum of squares   You make two more assumptions when you include a blocking factor in the model:   Primary variable levels are  randomly assigned  within each block  The effects of the primary variable are  constant across the levels  of the blocking factor (the effects don't depend on the block they are in, there are  no interactions  with the blocking variable)    Note  Levene's test for homogeneity is  only available for one-way ANOVA models , so in this case, you have to use the Residuals by Predicted plot.", 
            "title": "ANOVA with Data from a Randomized Block Design"
        }, 
        {
            "location": "/statistics/anova/#proc-glm_1", 
            "text": "1\n2\n3\n4\n5\n6 PROC GLM DATA=SAS-data-set\n         PLOTS(ONLY)=DIAGNOSTICS(UNPACK);   /* print each plot on a separated page */\n    CLASS variable(s) blocking-factor(s);\n    MODEL dependents=intependents blocking-factor(s) /options ;\nRUN;\nQUIT;    Rule of thumb : if the  F-value is   1 , then it helped to add the blocking factor in your model   If you compare the MSE ( Mean Square  in the table) without and with including the blocking variable in the model,  there is a drop of its value meaning that  you have been able to account for a bit more of the unexplained variability due to the nuisance factors  helping o have more precise estimates of the effect of your primary variable  It is also reflected in the  R-Square  value that is increased when a blocking factor is added to the model  Thanks to adding a blocking variable to your model you can get your primary variable to be significant  The  Type III SS  at the bottom of the output tests for the difference due to each variable, controlling for or adjusting for the other variable", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#anova-post-hoc-tests", 
            "text": "This test is used to determine which means differ from other means and control the error rate using  multiple comparison method .  Assuming the null hypothesis is true for your different comparisons, the probability that you conclude a difference exist at least one time when there really  isn't a difference increases with the more tests you perform. So  the chance that you make a Type I error increases each time you conduct a statistical test .   The  comparisonwise error rate (CER)  is the probability of a Type I error on a single pairwise test ( )  The  experimentwise error rate (EER)  is the probability of making at least one Type I error when performing the whole set of comparisons. It takes into consideration the number of pairwise comparisons you make, so it increases as the number of tests increase:     EER=1-(1-\\alpha)^{comparisons}      Tukey's Multiple Comparison Method   This method, which is also known as the  Honestly Significant Difference  test, is a popular multiple comparison test that  controls the EER  This tests compares all possible pairs of means, so  it can only be used when you make pairwise comparisons  This method controls $EER=\\alpha$ when  all possible pairwise comparisons are considered  and controls $EER \\alpha$ when fewer than all pairwise comparisons are considered   Dunnett's Multiple Comparison Method   This method is a specialised multiple comparison test that allows you to  compare a single control group to all other groups  It controls $EER \\le \\alpha$ when all groups are compared to the reference group (control)  It accounts for the correlation that exists between the comparisons and  you can conduct one-sided tests  of hypothesis against the reference group", 
            "title": "ANOVA Post Hoc Tests"
        }, 
        {
            "location": "/statistics/anova/#proc-glm_2", 
            "text": "1\n2\n3\n4\n5\n6\n7\n8\n9 PROC GLM DATA=SAS-data-set;\n    CLASS variable(s);\n    MODEL dependents=intependents  /options ;\n    LSMEANS effects  /options-test-1 ;  \n    LSMEANS effects  /options-test-2 ;\n    [...]\n    LSMEANS effects  /options-test-n ;  \nRUN;\nQUIT;    PDIFF=ALL  requests p-values for the differences between  ALL  the means and a  diffogram  is produced automatically displaying all pairwise least square means differences and indicating which are significant  It can be undestood as a least squares mean by least squares mean plot  The point estimates for differences between the means for each pairwise comparison can be found at the intersections of the gray grid lines (intersection of appropriate indexes)  The red/blue diagonal lines show the  confidence intervals for the true differences of the means  for each pairwise comparison  The grey 45$^{\\circ}$ reference line represents equality of the means (if the confidence interval crosses over it, then there is no significant difference between the two groups and the diagonal line for the pair will be  dashed and red ; if the difference is significant the line will be  solid and blue )     The  ADJUST=  option specifies the adjustment method for multiple comparisons  If you don't specify an option SAS uses the  Tukey method by default , if you specify  ADJUST=Dunnett  the GLM procedure produces multiple comparisons using  Dunnett's method  and a  control plot    The control plot displays the least squares mean and confidence limits of each group compared to the reference group   The middle  horizontal line represents its least square mean value  (you can see the arithmetic mean value un the  upper right corner  of the graph)  The  shaded area  goes from the  lower decision limit (LDL)  to the  upper decision limit (UDL)  There is a vertical line for each group that you're comparing to the reference (control) group. If a  vertical line extends past the shaded area , then the group represented by the line is  significantly different  (small p-value) than the reference group        PDIFF=CONTROLU('value')  specifies the control group for the Dunnett's case: the direction of the sign in Ha is the same as the direction you are testing, so this is a  one-sided upper-tailed t-test  If you specify  ADJUST=T  SAS will make no adjustments for multiple comparisons: is not recommended as there's a tendency to find  more significant pairwise differences than might actually exist", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#two-way-anova-with-interactions", 
            "text": "When you have a continuous response variable and  two categorical predictor variables , you use the  two-way ANOVA model   Effect : the magnitude of the expected change in the response variable presumably caused by the change in value of a predictor variable in the model  In addition, the variables in a model can be referred to as effects or terms  Main effect : is the effect of a single predictor variable  Interaction effects : when the relationship of the response variable with a predictor changes with the changing of another predictor variable (the effect of one variable depends on the value of the other variable)    When you consider an ANOVA with more than one predictor variable, it's called  n-way ANOVA  where  n  represents the number of predictor variables   The analysis in a  randomized block design  is actually a  special type of two-way ANOVA  in which you have one factor of interest and one blocking factor  When you analyze a two-way ANOVA with interactions, you first look at any tests for  interactions among the factors  If there is  no interaction between the factors  you can interpret the tests for the individual factor effects to determine their significance/non-significance  If an  interaction exists between any factors , the tests for the individual factor effects might be misleading due to masking of these effect by the interaction (this is specially true for unbalanced data with different number of observations for each combination of groups)    When the interaction is not statistically significant  you can analyze the main effect with the model in its current form  (generally the method you use when you analyze designed experiments)  Even when you analyze designed experiments, some statisticians might suggest that if the interaction is not significant,  you can delete the interaction effect from your model, rerun the model and then just analyze the main effects  increasing the power of the main effects test  If the  interaction term is significant , it is good practice to keep the main effect terms that make up the interaction in the model, whether they are significant or not (this preserves model hierarchy)  You have to make the  same three assumptions used in the ANOVA test  The interaction terms are also called  product terms  or  crossed effects", 
            "title": "Two-Way ANOVA with Interactions"
        }, 
        {
            "location": "/statistics/anova/#proc-glm_3", 
            "text": "1\n2\n3\n4\n5\n6\n7 PROC GLM DATA=SAS-data-set;\n    CLASS independent1 independent2;\n    MODEL dependent = independent1 independent2 independent1*independent2;\n    or\n    MODEL dependent = independent1 | independent2;\nRUN;\nQUIT;   This program is  fitting to this model :   Y_{ijk}=\\mu + \\alpha_i+\\beta_j+(\\alpha\\beta)_{ij}+\\epsilon_{ijk}    dependent = overall mean + intependent1 + independent2 + interaction12 + unaccounted for variation    In  most situations  you will want to use the  Type III SS  The  Type I SS (sequential)  are the sums of squares you obtain from fitting the effects in the order you specify in the model   The  Type III SS (marginal)  are the sums of squares you obtain from fitting each effect after all the other terms in the model, that is the sums of squares for each effect corrected for the other terms in the model  When examining these results you first have to  look at the interaction term and if it's significant  (p-value), the  main effects don't tell you the whole story . It that is the case, you don't need to worry all that much about the significance of the main effects at this point for two reasons:  You know that the effect of each variable1 level changes for the different variable2 levels  You want to include the main effects in the model, whether they are significant or not, to preserve model hierarchy    You can analyze the interaction between terms by looking at the  interaction plot  that SAS produces by default when you include an interaction term in the model  To analyze and interpret the effect of one of the interacting variables you need to add the  LSMEANS  statement to your program   1\n2\n3\n4\n5\n6 PROC GLM DATA=SAS-data-set ORDER=INTERNAL PLOTS(ONLY)=INTPLOT;\n    CLASS independent1 independent2;\n    MODEL dependent = intependent1 independent2 independent1*independent2;\n    LSMEANS independent1*independent2 / SLICE= independent1;\nRUN;\nQUIT;   SAS creates two types of mean plots when you use the  LSMEANS  statement with an interaction term:   The first plot displays the  least squares mean (LS-Mean) for every effect level    The second plot contains the same information rearranged so you can  look a little closer at the combination levels", 
            "title": "PROC GLM"
        }, 
        {
            "location": "/statistics/anova/#store-statement", 
            "text": "You can add a  STORE  statement to save your analysis results in an  item store  (a binary file format that cannot be modified). This allows you to  run post-processing analysis  on the stored results even if you no longer have access to the original data set. The  STORE  statement applies to the following SAS/STAT procedures:  GENMOD ,  GLIMMIX ,  GLM ,  GLMSELECT ,  LOGISTIC ,  MIXED ,  ORTHOREG ,  PHREG ,  PROBIT ,  SURVEYLOGISTIC ,  SURVEYPHREG , and  SURVEYREG .  1\n2 STORE  OUT = item-store-name\n     / LABEL= label ;    item-store-name  is a usual one- or two-level SAS name, similar to the names that are used for SAS data sets  label  identifies the estimate on the output (is optional)", 
            "title": "STORE statement"
        }, 
        {
            "location": "/statistics/anova/#proc-plm", 
            "text": "To perform post-fitting statistical analysis and plotting for the contents of the store item, you use  PROC PLM . The statements and options that are available vary depending upon which procedure you used to produce the item store.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 PROC PLM RESTORE=item-store-specification  options ;\n    EFFECTPLOT INTERACTION(SLICEBY=variable)  plot-type   (plot-definition   options)  / CLM  / options ;\n    LSMEANS  model-effects   / options ;\n    LSMESTIMATE model-effect  label  values\n         divisor =n ,... label  values\n         divisor =n   / options ;\n    SHOW options;\n    SLICE model-effect / SLICEBY=variable ADJUST=tukey  / options ;\n    WHERE expression;\nRUN;    RESTORE  specifies the source item store for processing  EFFECTPLOT  produces a display of the fitted model and provides options for changing and enhancing the displays  LSMEANS  computes and compares least squares means (LS-means) of fixed effects  LSMESTIMATE  provides custom hypothesis tests among least squares means  SHOW  uses  ODS  to display contents of the item store. This statement is useful for verifying that the contents of the item store apply to the analysis and for generating  ODS  tables.  SLICE  provides a general mechanism for performing a partitioned analysis of the LS-means for an interaction (analysis of simple effects) and it uses the same options as the  LSMEANS  statement  WHERE  is used in the PLM procedure when the item store contains  BY-variable  information and you want to apply the  PROC PLM  statements to only a subset of the BY groups", 
            "title": "PROC PLM"
        }, 
        {
            "location": "/statistics/regression/", 
            "text": "Chapter summary in SAS\n\n\n\n\nExploratory Data Analysis\n\n\nA useful set of techniques for investigating your data is known as \nexploratory data analysis\n.\n\n\nPROC SGCATTER\n: Scatter Plots\n\n\n1\n2\n3\nPROC SGSCATTER DATA=SAS-data-base;\n    PLOT vairableY*(variableX1 variableX2) / REG;\nRUN;\n\n\n\n\n\n\n\n\nIf you have \nso many observations\n that the scatter plot of the whole data set is difficult to interpret, you might run \nPROC SGSCATTER\n on a \nrandom sample of observations\n\n\n\n\nPROC CORR\n: Correlation Analysis\n\n\nThe closer the \nPearson\n correlation coefficient is to +1/-1, the stronger the positive/negative linear relationship is between the two variables. The closer the correlation coefficient is to 0, the weaker the linear relationship and if it is 0 variables are uncorrelated.\n\n\n\n\nWhen you interpret the correlation, be cautious about the effect of \nlarge sample sizes\n: even a correlation of 0.01 can be statistically significant with a large enough sample size and you would almost always reject the hypothesis $H_0$: $\\rho =0$, even if the value of your correlation is small for all practical purposes\n\n\nSome \ncommon errors\n on interpreting correlations are concluding a \ncause-and-effect relationship\n between the variables misinterpreting the kind of relationship between the variables and failing to recognize the influence of outliers on the correlation\n\n\nThe variables might be related but not causally\n\n\nCorrelation coefficients can be large because both variables are affected by other variables\n\n\nVariables might be strongly correlated by chance\n\n\n\n\n\n\nJust because the correlation coefficient is close to 0 doesn't mean that no relationship exists between the two variables: they might have a \nnon-linear relationship\n\n\nAnother common error is failing to recognize the \ninfluence of outliers\n on the correlation\n\n\nIf you have an outlier you should report both correlation coefficients (with and without the outlier) to report how influential the unusual data point is in your analysis\n\n\n\n\n\n\n\n\nThe \nPROC CORR\n also produces \nscatter plots\n or a \nscatter plot matrix\n.\n\n\n1\n2\n3\n4\n5\nPROC CORR DATA=SAS-data-set RANK|NOSIMPLE PLOTS(ONLY)=MATRIX(NVAR=ALL HISTOGRAM)|SCATTER(NVAR=ALL ELLIPSE=NONE) \noptions\n;\n    VAR variable(s)X;\n    WITH variable(s)Y;\n    ID variable4label;\nRUN;\n\n\n\n\n\n\nSimple Linear Regression\n\n\nYou use correlation analysis to determine the strength of the linear relationship between continuous response variables. Now you need to go a step further and \ndefine the linear relationship itself\n: $Y= \\beta_0+\\beta_1 \\cdot X+\\epsilon$\n\n\n\n\n$Y$ is the response variable \n\n\n$X$ is the predictor variable\n\n\n$\\beta_0$ is the intercept parameter\n\n\n$\\beta_1$ is the slope parameter\n\n\n$\\epsilon$ is the error term\n\n\n\n\nThe method of \nleast squares\n produces parameter estimates $\\hat \\beta_0$ and $\\hat \\beta_1$ with certain \noptimum properties\n which make them the Best Linear Unbiased Estimators (\nBLUE\n):\n\n\n\n\nThey are \nunbiased estimates\n of the population parameters\n\n\nThey have \nminimum variance\n\n\n\n\nTo find out how much better is the model that takes the predictor variable into account than a model that ignores the predictor variable, you can compare the \nsimple linear regression model\n to a \nbaseline model\n ($Y= \\bar Y$ independent of $X$). For your comparison, you calculate the \nexplained\n, \nunexplained\n and \ntotal variability\n in the simple linear regression model.\n\n\n\n\nThe \nexplained variability (SSM)\n is the difference between the regression line and the mean of the response variable: $\\sum(\\hat Y_i-\\bar Y)^2$\n\n\nThe \nunexplained variability (SSE)\n is the difference between the observed values and the regression line: $\\sum(Y_i-\\hat Y_i)^2$\n\n\nThe \ntotal variability\n is the difference between the observed values and the mean of the response variable: $\\sum(Y_i-\\bar Y)^2$\n\n\n\n\nIf we consider \nhypothesis testing\n for linear regression:\n\n\n\n\n$H_0$: the regression model does not fit the data better than the baseline model (slope $= 0$)\n\n\n$H_a$: the regression model does fit the data better than the baseline model (slope $= \\hat\\beta_1 \\ne 0$)\n\n\n\n\nThese \nassumptions\n underlie the hypothesis test for the regression model and have to be met for a simple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):\n\n\n\n\nThe mean of the response variable is linearly related to the value of the predictor variable\n\n\nThe error terms are normally distributed with a mean of 0\n\n\nThe error terms have equal variances\n\n\nThe error terms are independent at each value of the predictor variable\n\n\n\n\nPROC REG\n\n\n1\n2\n3\n4\n5\nPROC REG DATA=SAS-data-set \noptions\n;\n    MODEL dependent=regressor / CLM CLI \n/options\n;\n    ID regressor;\nRUN;\nQUIT;\n\n\n\n\n\n\nTo asses the level of precision around the mean estimates you can produce \nconfidence intervals\n around the means. Confidence intervals become wider as you move away from the mean of the predictor variable. The wider the confidence interval the less precise it is. You might also want to construct \nprediction intervals\n for a single observation. A prediction interval is wider than a confidence interval because \nsingle observations have more variability than sample means\n.\n\n\nFor producing \npredicted values\n with \nPROC REG\n:\n\n\n\n\nCreate a data set containing the values of the independent variables for which you want to make predictions\n\n\nConcatenate the new data set with the original data set\n\n\nFit a simple linear regression model to the new data set and specify the \nP\n option in the \nMODEL\n statement\n\n\n\n\nBecause the concatenated observations contain \nmissing values\n for the response variable, \nPROC REG\n does not include these observations when fitting the regression model. However, \nPROC REG\n does \nproduce predicted values\n for these observations.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nDATA SAS-predictions-data-set;\n    INPUT dependent @@;\n    DATALINES;\n[new values separated with blanks]\n;\nRUN;\n\nDATA SAS-new-data-set;\n    SET SAS-predictions-data-set SAS-original-data-set;\nRUN;\n\nPROC REG DATA=SAS-new-data-set;\n    MODEL dependent=regressor / P;\n    ID regressor;\nRUN;\nQUIT;\n\n\n\n\n\n\nWhen you use a model to predict future values of the response variable given certain values of the predictor variable, you must \nstay within (or near) the range of values for the predictor variable used to create the model\n. The relationship between the predictor variable and the response variable might be different beyond the range of the data.\n\n\nIf you have a large data set and have already fitted the regression model, you can predict values more efficiently by using \nPROC REG\n and \nPROC SCORE\n:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\nPROC REG DATA=SAS-original-data-set NOPRINT OUTEST=SAS-estimates-data-set;\n    MODEL dependent=regressor \n/options\n;\n    ID regressor;\nRUN;\nQUIT;\n\nPROC SCORE DATA=SAS-predictions-data-set\n        SCORE=SAS-estimates-data-set\n        OUT=SAS-scored-data-set\n        TYPE=PARMS\n        \noptions\n;\n    VAR variable(s);\nRUN;\nQUIT;\n\n\n\n\n\n\nMultiple Regression\n\n\nIn \nmultiple regression\n you can model the relationship between the response variable and \nmore than one predictor variable\n. It is a powerful tool for both \nanalytical or explanatory analysis and for prediction\n.\n\n\n$Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k+\\epsilon$ ($k+1$ parameters)\n\n\nAdvantages\n\n\n\n\nMultiple linear regression is a more powerful tool\n\n\nYou can determine whether a relationship exists between the response variable and more than one predictor variable at the same time\n\n\n\n\nDisadvantages\n\n\n\n\nYou need to perform a selection process to decide which model to use\n\n\nThe more predictors you have, the more complicated interpreting the model becomes\n\n\n\n\nIf we consider \nhypothesis testing\n for linear regression:\n\n\n\n\n$H_0$: the regression model does not fit the data better than the baseline model $(\\beta_1=\\beta_2=...=\\beta_k= 0)$\n\n\n$H_a$: the regression model does fit the data better than the baseline model (at least one $\\beta_i \\ne 0$)\n\n\n\n\nThese \nassumptions\n have to be met for a multiple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):\n\n\n\n\nA linear function of the $X$s accurately models the mean of the $Y$s\n\n\nThe error terms are normally distributed with a mean of 0\n\n\nThe error terms have constant variances\n\n\nThe error terms are independent at each value of the predictor variable\n\n\n\n\nThe \nregular $R^2$\n values never decrease when you add more terms to the model, but the \nadjusted $R^2$\n value takes into account the number of terms in the model by including a penalty for the complexity of the model. The \nadjusted $R^2$\n value increases only if new terms that you add significantly improve the model enough to warrant increasing the complexity of the model. It enables proper comparison between models with different parameter counts. When an \nadjusted $R^2$ increases by removing a variable\n from the models, it strongly implies that the removed \nvariable was not necessary\n.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\nPROC REG DATA=SAS-data-set \noptions\n;\n    MODEL dependent=regressor1 regressor2 \n/options\n;\nRUN;\nQUIT;\n\nPROC GLM DATA=SAS-data-set\n    PLOTS(ONLY)=(CONTOURFIT);\n    MODEL dependent=regressor1 regressor2;\n    STORE OUT=SAS-multiple-data-set;\nRUN;\nQUIT;\n\nPROC PLM RESTORE=SAS-multiple-data-set PLOTS=ALL;\n    EFFECTPLOT CONTOUR (Y=regressor1 X=regressor2);\n    EFFECTPLOT SLICEFIT (X=regressor2 SLICEBY=regressor1=250 to 1000 by 250);\nRUN;\n\n\n\n\n\n\n\n\nIn \nPROC GLM\n, when you run a linear regression model with only two predictor variables, the output includes a contour fit plot by default. We specify \nCONTOURFIT\n to tell SAS to overlay the contour plot with a scatter plot of the observed data\n\n\n\n\n\n\nThe plot shows \npredicted values\n of the response variable as \ngradations of the background color\n from blue, representing low values, to red, representing high values. The \ndots\n, which are similarly coloured, represent the \nactual data\n. Observations that are perfectly fit would show the same color within the circle as outside the circle. The \nlines on the graph\n help you read the actual predictions at even intervals.\n\n\n\n\nThe \nCONTOUR\n option displays a contour plot of predicted values against two continuous covariates\n\n\nThe \nSLICEFIT\n option displys a curve of predicted values vs a continuous variable grouped by the levels of another effect\n\n\n\n\nClearly the \nPROC GLM\n contour fit plot is \nmore useful\n. However, if you do not have access to the original data set and can run \nPROC PLM\n only on the item store, this plot still gives you an idea of the relationship between the predictor variables and predicted values.\n\n\nModel Building and Interpretation\n\n\nThe brute force approach to find a good model is to start including all the predictor variables available and rerun the model \nremoving the least significant remaining term\n each time \nuntil\n you're left with a model where \nonly significant terms remain\n. With a small number of predictor variables a manual approach isn't too difficult but with a large number of predictor variables it's very tedious. Fortunately, if you specify the model selection technique to use, SAS finds good candidate models in an automatic way.\n\n\n\n\nAll-possible regression methods\n\n\nSAS computes all possible models and ranks the results. Then, to evaluate the models, you compare statistics side by side ($R^2$, adjusted $R^2$ and $C_p$ statistic).\n\n\n\n\n\n\nMallows' $C_p$\n statistic helps you detect model bias if you are underfitting/overfitting the model, it is a simple indicator of effective variable selection within a model\n\n\n\n\n\n\nTo select the best model for prediction (most accurate model for predicting future values of $Y$), you should use the \nMallows' criterion\n:  $C_p \\le p$, which is the \nnumber of parameters\n in the model including the intercept\n\n\n\n\nTo select the best model for parameter estimation (analytical or explanatory analysis), you should use \nHocking's criterion\n: $C_p\\le2p-p_{full}+1$\n\n\n\n\n1\n2\n3\n4\nPROC REG DATA=SASdata-set PLOTS(ONLY)=(CP) \noptions\n;\n    \nlabel:\n MODEL dependent=regressors  / SELECTION=CP RSQUARE ADJRSQ BEST=n \n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nBEST\n prints an specific number of the best candidate models according to a few different statistical criteria\n\n\nSELECTION\n option is used to specify the method used to select the model (\nCP\n, \nRSQUARE\n and \nADJRSQ\n to calculate with the all-possible regression model; the first statistic determines the sorting order)\n\n\nFor this all-possible regression model, we add the label \nALL_REG:\n\n\nWith \nPLOTS=(CP)\n we produce a plot:\n\n\n\n\n\n\nEach \nstar\n represents the \nbest model\n for a given number of parameters. The solid \nblue line\n represents \nMallows' criterion\n for $C_p$, so using this line helps us find a good candidate model for prediction. Because we want the \nsmallest model possible\n, we start at the left side of the graph, with the fewest number of parameters moving to the right until we find the \nfirst model that falls below the solid blue line\n. To find models for parameter estimation we have to look for models that falls below the \nred solid line\n which represent the \nHocking's criterion\n for $C_p$ parameter estimation. If we hover over the star, we can see which variables are included in this model.\n\n\n\n\nStepwise selection methods\n\n\nHere you choose a selection method (\nstepwise\n, \nforward\n or \nbackward\n approaches) and SAS constructs a model based on that method. When you have a \nlarge number of potential predictor variables\n, the stepwise regression methods might be a better option. You can use either the \nREG\n procedure or the \nGLMSELECT\n procedure to perform stepwise selection methods\n\n\n\n\nForward selection\n starts with no predictor variables in the model\n\n\nIt selects the best one-variable model\n\n\nIt selects the best two-variable model that includes the variable from the first model (after a variable is added to the model, it stays in even if it becomes insignificant later)\n\n\nIt keeps adding variables, one at a time, until no significant terms are left to add\n\n\n\n\n\n\nBackward selection/elimination\n starts with all predictor variables in the model\n\n\nIt removes variables one at a time, starting with the most non-significant variable (after a variable is removed from the model, it cannot reenter)\n\n\nIt stops when only significant terms are left in the model\n\n\n\n\n\n\nStepwise selection\n combines aspects of both forward and backward selection\n\n\nIt starts with no predictor variables in the model and starts adding variables, one at a time, as in forward selection\n\n\nHowever, as in backward selection, stepwise selection can drop non-significant variables, one at a time\n\n\nIt stops when everything in the model is currently significant and everything not in the model is not significant\n\n\n\n\n\n\n\n\nStatisticians in general agree on first using \nstepwise methods\n to identify several good candidates models and then applying your \nsubject matter expertise\n to choose the best model. Because the techniques for selecting or eliminating variables differ between the three selection methods, \nthey don't always produce the same final model\n. There is no one method that is best and \nyou need to be cautious\n when reporting statistical quantities produced by these methods:\n\n\n\n\nUsing automated model selections results in \nbiases in parameter estimates\n, \npredictions\n and \nstandard errors\n\n\nIncorrect\n calculation of \ndegrees of freedom\n\n\np-values\n that tend to err on the side of \noverestimating significance\n\n\n\n\nHow can you \navoid these issues\n?\n\n\n\n\nYou can hold out some of your data in order to perform an honest assessment of how well your model performs on a different sample of data (\nholdout/validation data\n) than you use to develop the model (\ntraining data\n)\n\n\nOther honest assessment approaches include \ncross-validation\n (if your data set is not large enough to split) or \nbootstraping\n (a resampling method that tries to approximate the distribution of the parameter estimates to estimate the standard error and p-values)\n\n\n\n\nPROC GLMSELECT\n\n\n1\n2\n3\n4\n5\nPROC GLMSELECT DATA=SAS-data-set \noptions\n;\n    CLASS variables;\n    \nlabel:\n MODEL dependent(s) = regressor(s) / \n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nThe \nSELECTION\n option specifies the method to be used to select the model (\nFORWARD\n | \nBACKWARD\n | \nSTEPWISE\n = default value)\n\n\nThe \nSELECT\n option specifies the criterion to be used to determine which variable to add/remove from the model (\nSL\n = significance level as the selection criterion)\n\n\nThe \nSLENTRY\n option determines the significance level for a variable to enter the model (default = 0.5 for forward and 0.15 for stepwise)\n\n\nThe \nSLSTAY\n option determines the significance level for a variable to stay in the model (default = 0.1 for backward and 0.15 for stepwise)\n\n\nYou can display p-values in the \nParameter Estimates\n table by including the \nSHOWPVALUES\n option int he MODEL statement\n\n\nThe \nDETAILS\n option specifies the level of detail produced (\nALL\n | \nSTEPS\n | \nSUMMARY\n)\n\n\n\n\n\n\nRecommendations to decide which model is best for your needs:\n\n\n\n\nRun all model selection methods\n\n\nLook for commonalities across the results \n\n\nNarrow down your choice of models by using your subject matter knowledge\n\n\n\n\nInformation Criterion and Other Selection Options\n\n\nThere are other selection criteria that you can use to select variables for a model as well as evaluate competing models. These statistics are collectively referred to as \ninformation criteria\n. Each information criterion searched for a model that minimizes the \nunexplained variability\n with as \nfew effects in the model as possible\n. The model with the \nsmaller information criterion is considered to be better\n. For types are available in \nPROC GLMSELECT\n:\n\n\n\n\nAkaike's information criterion (\nSELECT=AIC\n)\n\n\nCorrecterd Akaike's information criterion (\nSELECT=AICC\n)\n\n\nSawa Bayesian information criterion (\nSELECT=BIC\n)\n\n\nSchwarz Bayesian information criterion (\nSELECT=SBC\n, it could be called \nBIC\n in some other SAS procedures)\n\n\n\n\nThe calculations of all information criteria begin the same way:\n\n\n\n\nFirst you calculate $n\\cdot log(SSE/n)$ \n\n\nThen, each criterion adds a penalty that represents the complexity of the model (each type of information criterion invokes a different penalty component)\n\n\nAIC\n: $2p+n+2$\n\n\nAICC\n: $n(n+p)/(n-p-2)$\n\n\nBIC\n: $2(p+2)1-2q^2$\n\n\nSBC\n: $p\\cdot log(n)$", 
            "title": "Regression"
        }, 
        {
            "location": "/statistics/regression/#exploratory-data-analysis", 
            "text": "A useful set of techniques for investigating your data is known as  exploratory data analysis .", 
            "title": "Exploratory Data Analysis"
        }, 
        {
            "location": "/statistics/regression/#proc-sgcatter-scatter-plots", 
            "text": "1\n2\n3 PROC SGSCATTER DATA=SAS-data-base;\n    PLOT vairableY*(variableX1 variableX2) / REG;\nRUN;    If you have  so many observations  that the scatter plot of the whole data set is difficult to interpret, you might run  PROC SGSCATTER  on a  random sample of observations", 
            "title": "PROC SGCATTER: Scatter Plots"
        }, 
        {
            "location": "/statistics/regression/#proc-corr-correlation-analysis", 
            "text": "The closer the  Pearson  correlation coefficient is to +1/-1, the stronger the positive/negative linear relationship is between the two variables. The closer the correlation coefficient is to 0, the weaker the linear relationship and if it is 0 variables are uncorrelated.   When you interpret the correlation, be cautious about the effect of  large sample sizes : even a correlation of 0.01 can be statistically significant with a large enough sample size and you would almost always reject the hypothesis $H_0$: $\\rho =0$, even if the value of your correlation is small for all practical purposes  Some  common errors  on interpreting correlations are concluding a  cause-and-effect relationship  between the variables misinterpreting the kind of relationship between the variables and failing to recognize the influence of outliers on the correlation  The variables might be related but not causally  Correlation coefficients can be large because both variables are affected by other variables  Variables might be strongly correlated by chance    Just because the correlation coefficient is close to 0 doesn't mean that no relationship exists between the two variables: they might have a  non-linear relationship  Another common error is failing to recognize the  influence of outliers  on the correlation  If you have an outlier you should report both correlation coefficients (with and without the outlier) to report how influential the unusual data point is in your analysis     The  PROC CORR  also produces  scatter plots  or a  scatter plot matrix .  1\n2\n3\n4\n5 PROC CORR DATA=SAS-data-set RANK|NOSIMPLE PLOTS(ONLY)=MATRIX(NVAR=ALL HISTOGRAM)|SCATTER(NVAR=ALL ELLIPSE=NONE)  options ;\n    VAR variable(s)X;\n    WITH variable(s)Y;\n    ID variable4label;\nRUN;", 
            "title": "PROC CORR: Correlation Analysis"
        }, 
        {
            "location": "/statistics/regression/#simple-linear-regression", 
            "text": "You use correlation analysis to determine the strength of the linear relationship between continuous response variables. Now you need to go a step further and  define the linear relationship itself : $Y= \\beta_0+\\beta_1 \\cdot X+\\epsilon$   $Y$ is the response variable   $X$ is the predictor variable  $\\beta_0$ is the intercept parameter  $\\beta_1$ is the slope parameter  $\\epsilon$ is the error term   The method of  least squares  produces parameter estimates $\\hat \\beta_0$ and $\\hat \\beta_1$ with certain  optimum properties  which make them the Best Linear Unbiased Estimators ( BLUE ):   They are  unbiased estimates  of the population parameters  They have  minimum variance   To find out how much better is the model that takes the predictor variable into account than a model that ignores the predictor variable, you can compare the  simple linear regression model  to a  baseline model  ($Y= \\bar Y$ independent of $X$). For your comparison, you calculate the  explained ,  unexplained  and  total variability  in the simple linear regression model.   The  explained variability (SSM)  is the difference between the regression line and the mean of the response variable: $\\sum(\\hat Y_i-\\bar Y)^2$  The  unexplained variability (SSE)  is the difference between the observed values and the regression line: $\\sum(Y_i-\\hat Y_i)^2$  The  total variability  is the difference between the observed values and the mean of the response variable: $\\sum(Y_i-\\bar Y)^2$   If we consider  hypothesis testing  for linear regression:   $H_0$: the regression model does not fit the data better than the baseline model (slope $= 0$)  $H_a$: the regression model does fit the data better than the baseline model (slope $= \\hat\\beta_1 \\ne 0$)   These  assumptions  underlie the hypothesis test for the regression model and have to be met for a simple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):   The mean of the response variable is linearly related to the value of the predictor variable  The error terms are normally distributed with a mean of 0  The error terms have equal variances  The error terms are independent at each value of the predictor variable", 
            "title": "Simple Linear Regression"
        }, 
        {
            "location": "/statistics/regression/#proc-reg", 
            "text": "1\n2\n3\n4\n5 PROC REG DATA=SAS-data-set  options ;\n    MODEL dependent=regressor / CLM CLI  /options ;\n    ID regressor;\nRUN;\nQUIT;   To asses the level of precision around the mean estimates you can produce  confidence intervals  around the means. Confidence intervals become wider as you move away from the mean of the predictor variable. The wider the confidence interval the less precise it is. You might also want to construct  prediction intervals  for a single observation. A prediction interval is wider than a confidence interval because  single observations have more variability than sample means .  For producing  predicted values  with  PROC REG :   Create a data set containing the values of the independent variables for which you want to make predictions  Concatenate the new data set with the original data set  Fit a simple linear regression model to the new data set and specify the  P  option in the  MODEL  statement   Because the concatenated observations contain  missing values  for the response variable,  PROC REG  does not include these observations when fitting the regression model. However,  PROC REG  does  produce predicted values  for these observations.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 DATA SAS-predictions-data-set;\n    INPUT dependent @@;\n    DATALINES;\n[new values separated with blanks]\n;\nRUN;\n\nDATA SAS-new-data-set;\n    SET SAS-predictions-data-set SAS-original-data-set;\nRUN;\n\nPROC REG DATA=SAS-new-data-set;\n    MODEL dependent=regressor / P;\n    ID regressor;\nRUN;\nQUIT;   When you use a model to predict future values of the response variable given certain values of the predictor variable, you must  stay within (or near) the range of values for the predictor variable used to create the model . The relationship between the predictor variable and the response variable might be different beyond the range of the data.  If you have a large data set and have already fitted the regression model, you can predict values more efficiently by using  PROC REG  and  PROC SCORE :   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14 PROC REG DATA=SAS-original-data-set NOPRINT OUTEST=SAS-estimates-data-set;\n    MODEL dependent=regressor  /options ;\n    ID regressor;\nRUN;\nQUIT;\n\nPROC SCORE DATA=SAS-predictions-data-set\n        SCORE=SAS-estimates-data-set\n        OUT=SAS-scored-data-set\n        TYPE=PARMS\n         options ;\n    VAR variable(s);\nRUN;\nQUIT;", 
            "title": "PROC REG"
        }, 
        {
            "location": "/statistics/regression/#multiple-regression", 
            "text": "In  multiple regression  you can model the relationship between the response variable and  more than one predictor variable . It is a powerful tool for both  analytical or explanatory analysis and for prediction .  $Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+...+\\beta_kX_k+\\epsilon$ ($k+1$ parameters)  Advantages   Multiple linear regression is a more powerful tool  You can determine whether a relationship exists between the response variable and more than one predictor variable at the same time   Disadvantages   You need to perform a selection process to decide which model to use  The more predictors you have, the more complicated interpreting the model becomes   If we consider  hypothesis testing  for linear regression:   $H_0$: the regression model does not fit the data better than the baseline model $(\\beta_1=\\beta_2=...=\\beta_k= 0)$  $H_a$: the regression model does fit the data better than the baseline model (at least one $\\beta_i \\ne 0$)   These  assumptions  have to be met for a multiple linear regression analysis to be valid (last three assumptions are the same as for ANOVA):   A linear function of the $X$s accurately models the mean of the $Y$s  The error terms are normally distributed with a mean of 0  The error terms have constant variances  The error terms are independent at each value of the predictor variable   The  regular $R^2$  values never decrease when you add more terms to the model, but the  adjusted $R^2$  value takes into account the number of terms in the model by including a penalty for the complexity of the model. The  adjusted $R^2$  value increases only if new terms that you add significantly improve the model enough to warrant increasing the complexity of the model. It enables proper comparison between models with different parameter counts. When an  adjusted $R^2$ increases by removing a variable  from the models, it strongly implies that the removed  variable was not necessary .   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 PROC REG DATA=SAS-data-set  options ;\n    MODEL dependent=regressor1 regressor2  /options ;\nRUN;\nQUIT;\n\nPROC GLM DATA=SAS-data-set\n    PLOTS(ONLY)=(CONTOURFIT);\n    MODEL dependent=regressor1 regressor2;\n    STORE OUT=SAS-multiple-data-set;\nRUN;\nQUIT;\n\nPROC PLM RESTORE=SAS-multiple-data-set PLOTS=ALL;\n    EFFECTPLOT CONTOUR (Y=regressor1 X=regressor2);\n    EFFECTPLOT SLICEFIT (X=regressor2 SLICEBY=regressor1=250 to 1000 by 250);\nRUN;    In  PROC GLM , when you run a linear regression model with only two predictor variables, the output includes a contour fit plot by default. We specify  CONTOURFIT  to tell SAS to overlay the contour plot with a scatter plot of the observed data    The plot shows  predicted values  of the response variable as  gradations of the background color  from blue, representing low values, to red, representing high values. The  dots , which are similarly coloured, represent the  actual data . Observations that are perfectly fit would show the same color within the circle as outside the circle. The  lines on the graph  help you read the actual predictions at even intervals.   The  CONTOUR  option displays a contour plot of predicted values against two continuous covariates  The  SLICEFIT  option displys a curve of predicted values vs a continuous variable grouped by the levels of another effect   Clearly the  PROC GLM  contour fit plot is  more useful . However, if you do not have access to the original data set and can run  PROC PLM  only on the item store, this plot still gives you an idea of the relationship between the predictor variables and predicted values.", 
            "title": "Multiple Regression"
        }, 
        {
            "location": "/statistics/regression/#model-building-and-interpretation", 
            "text": "The brute force approach to find a good model is to start including all the predictor variables available and rerun the model  removing the least significant remaining term  each time  until  you're left with a model where  only significant terms remain . With a small number of predictor variables a manual approach isn't too difficult but with a large number of predictor variables it's very tedious. Fortunately, if you specify the model selection technique to use, SAS finds good candidate models in an automatic way.   All-possible regression methods  SAS computes all possible models and ranks the results. Then, to evaluate the models, you compare statistics side by side ($R^2$, adjusted $R^2$ and $C_p$ statistic).    Mallows' $C_p$  statistic helps you detect model bias if you are underfitting/overfitting the model, it is a simple indicator of effective variable selection within a model    To select the best model for prediction (most accurate model for predicting future values of $Y$), you should use the  Mallows' criterion :  $C_p \\le p$, which is the  number of parameters  in the model including the intercept   To select the best model for parameter estimation (analytical or explanatory analysis), you should use  Hocking's criterion : $C_p\\le2p-p_{full}+1$   1\n2\n3\n4 PROC REG DATA=SASdata-set PLOTS(ONLY)=(CP)  options ;\n     label:  MODEL dependent=regressors  / SELECTION=CP RSQUARE ADJRSQ BEST=n  /options ;\nRUN;\nQUIT;    BEST  prints an specific number of the best candidate models according to a few different statistical criteria  SELECTION  option is used to specify the method used to select the model ( CP ,  RSQUARE  and  ADJRSQ  to calculate with the all-possible regression model; the first statistic determines the sorting order)  For this all-possible regression model, we add the label  ALL_REG:  With  PLOTS=(CP)  we produce a plot:    Each  star  represents the  best model  for a given number of parameters. The solid  blue line  represents  Mallows' criterion  for $C_p$, so using this line helps us find a good candidate model for prediction. Because we want the  smallest model possible , we start at the left side of the graph, with the fewest number of parameters moving to the right until we find the  first model that falls below the solid blue line . To find models for parameter estimation we have to look for models that falls below the  red solid line  which represent the  Hocking's criterion  for $C_p$ parameter estimation. If we hover over the star, we can see which variables are included in this model.   Stepwise selection methods  Here you choose a selection method ( stepwise ,  forward  or  backward  approaches) and SAS constructs a model based on that method. When you have a  large number of potential predictor variables , the stepwise regression methods might be a better option. You can use either the  REG  procedure or the  GLMSELECT  procedure to perform stepwise selection methods   Forward selection  starts with no predictor variables in the model  It selects the best one-variable model  It selects the best two-variable model that includes the variable from the first model (after a variable is added to the model, it stays in even if it becomes insignificant later)  It keeps adding variables, one at a time, until no significant terms are left to add    Backward selection/elimination  starts with all predictor variables in the model  It removes variables one at a time, starting with the most non-significant variable (after a variable is removed from the model, it cannot reenter)  It stops when only significant terms are left in the model    Stepwise selection  combines aspects of both forward and backward selection  It starts with no predictor variables in the model and starts adding variables, one at a time, as in forward selection  However, as in backward selection, stepwise selection can drop non-significant variables, one at a time  It stops when everything in the model is currently significant and everything not in the model is not significant     Statisticians in general agree on first using  stepwise methods  to identify several good candidates models and then applying your  subject matter expertise  to choose the best model. Because the techniques for selecting or eliminating variables differ between the three selection methods,  they don't always produce the same final model . There is no one method that is best and  you need to be cautious  when reporting statistical quantities produced by these methods:   Using automated model selections results in  biases in parameter estimates ,  predictions  and  standard errors  Incorrect  calculation of  degrees of freedom  p-values  that tend to err on the side of  overestimating significance   How can you  avoid these issues ?   You can hold out some of your data in order to perform an honest assessment of how well your model performs on a different sample of data ( holdout/validation data ) than you use to develop the model ( training data )  Other honest assessment approaches include  cross-validation  (if your data set is not large enough to split) or  bootstraping  (a resampling method that tries to approximate the distribution of the parameter estimates to estimate the standard error and p-values)", 
            "title": "Model Building and Interpretation"
        }, 
        {
            "location": "/statistics/regression/#proc-glmselect", 
            "text": "1\n2\n3\n4\n5 PROC GLMSELECT DATA=SAS-data-set  options ;\n    CLASS variables;\n     label:  MODEL dependent(s) = regressor(s) /  /options ;\nRUN;\nQUIT;    The  SELECTION  option specifies the method to be used to select the model ( FORWARD  |  BACKWARD  |  STEPWISE  = default value)  The  SELECT  option specifies the criterion to be used to determine which variable to add/remove from the model ( SL  = significance level as the selection criterion)  The  SLENTRY  option determines the significance level for a variable to enter the model (default = 0.5 for forward and 0.15 for stepwise)  The  SLSTAY  option determines the significance level for a variable to stay in the model (default = 0.1 for backward and 0.15 for stepwise)  You can display p-values in the  Parameter Estimates  table by including the  SHOWPVALUES  option int he MODEL statement  The  DETAILS  option specifies the level of detail produced ( ALL  |  STEPS  |  SUMMARY )    Recommendations to decide which model is best for your needs:   Run all model selection methods  Look for commonalities across the results   Narrow down your choice of models by using your subject matter knowledge", 
            "title": "PROC GLMSELECT"
        }, 
        {
            "location": "/statistics/regression/#information-criterion-and-other-selection-options", 
            "text": "There are other selection criteria that you can use to select variables for a model as well as evaluate competing models. These statistics are collectively referred to as  information criteria . Each information criterion searched for a model that minimizes the  unexplained variability  with as  few effects in the model as possible . The model with the  smaller information criterion is considered to be better . For types are available in  PROC GLMSELECT :   Akaike's information criterion ( SELECT=AIC )  Correcterd Akaike's information criterion ( SELECT=AICC )  Sawa Bayesian information criterion ( SELECT=BIC )  Schwarz Bayesian information criterion ( SELECT=SBC , it could be called  BIC  in some other SAS procedures)   The calculations of all information criteria begin the same way:   First you calculate $n\\cdot log(SSE/n)$   Then, each criterion adds a penalty that represents the complexity of the model (each type of information criterion invokes a different penalty component)  AIC : $2p+n+2$  AICC : $n(n+p)/(n-p-2)$  BIC : $2(p+2)1-2q^2$  SBC : $p\\cdot log(n)$", 
            "title": "Information Criterion and Other Selection Options"
        }, 
        {
            "location": "/statistics/inference/", 
            "text": "Chapter summary in SAS\n\n\nHow to \nverify the assumptions\n and \ndiagnose problems\n that you encounter in \nlinear regression\n?\n\n\nExamining Residuals\n\n\nYou can use the \nresidual values\n (difference between each observed value of $Y$ and its predicted value) from the regression analysis to verify the \nassumptions of the linear regression\n. Residuals are estimates of the errors, so you can \nplot the residuals to check the assumptions of the errors\n.\n\n\n\n\nYou can plot residuals vs the predicted values to check for \nviolations of equal variances\n\n\nYou can also use this plot to check for \nviolations of linearity and independence\n\n\nYou can plot the residuals vs the values of the independent variables to \nfurther examine any violations of equal variances\n (you can see which predictor contributes to the violation of the assumption)\n\n\nYou can use a histogram or a normal probability plot of the residuals to determine whether or not the \nerrors are normally distributed\n\n\n\n\nYou want to see a \nrandom scatter of the residual values\n above and below the reference line at 0. If you see \npatterns or trends\n in the residual values, the assumptions might not be valid and the models might have problems.\n\n\n\n\n\n\nNote\n\n\n\n\nTo take autocorrelation (correlated over time) into account, you might need to use a regression procedure such as \nPROC AUTOREG\n\n\nYou can also use these plots to \ndetect outliers\n, which often reflect data errors or unusual circumstances. They can affect your regression results, so you want to know whether any outliers are present and causing problems and investigate if they result from \ndata entry error or some other problem\n that you can correct.\n\n\n1\n2\n3\n4\n5\nPROC REG DATA=SAS-data-set PLOTS(ONLY)=(QQ RESIDUALBYPREDICTED RESIDUALS)\noptions\n;\n    \nlabel:\n MODEL dependent=regressor(s) \n/options\n;\n    ID variable4identification;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nQQ\n requests a residual quantile-quantile plot to assess the normality of the residual error\n\n\nRESIDUALBYPREDICTED\n requests a plot of residuals by predicted values to verify the equal variance assumption, the independence assumption and model adequacy\n\n\nRESIDUALS\n requests a panel of plots of residuals by the predictor variables in the model. If any of the \nResidual by Regressors\n plots show signs of unequal variance, we can determine which predictor variable is involved in the problem.\n\n\n\n\nIdentifying Influential Observations\n\n\nAn influential observation is different from an outlier. An \noutlier\n is an unusual observation that has a large residual compare to the rest of the points. An \ninfluential observation\n can sometimes have a large residual compared to the rest of the points, but it is an observation so far away from the rest of the data that it singlehandedly exerts influence on the slope of the regression line.\n\n\n\n\n\n\nUsing STUDENT residuals to detect outliers\n\n\nAlso known as \nstudientized or standardized residuals\n, the STUDENT residuals are calculated by dividing the \nresidual by their standard errors\n, so you can think of them as roughly equivalent to a z-score. \n\n\n\n\nFor \nrelatively small sample sizes\n, if the absolute value of the STUDENT \nresidual is $\n2$\n, you can suspect that the corresponding observation is an outlier\n\n\nFor \nlarge sample sizes\n, it's very likely that even more STUDENT \nresiduals greater than $\\pm2$\n will occur just by chance, so you should typically use a larger cutoff value of $\n3$\n\n\n\n\n\n\nUsing Cook's D statistics to detect influential observations\n\n\nFore each observation, the Cook's D statistic is \ncalculated as if that observation weren't in the data set\n as well as the set of parameter estimates with all the observations in your regression analysis. \n\n\n\n\nIf any observation has a Cook's D \nstatistic $\n4/n$\n that observation is influential\n\n\nThe Cook's D statistic is most useful for identifying influential observations when the purpose of your model is \nparameter estimation\n\n\n\n\n\n\nUsing RSTUDENT residuals to detect influential observations\n\n\nRSTUDENT residuals are similar to STUDENT residuals. For each observation, the RSTUDENT residual is the \nresidual divided by the standard error estimated with the current observation deleted\n.\n\n\n\n\nIf the RSTUDENT residual is different from the STUDENT residual, the observation is probably influential\n\n\nIf the absolute value of the RSTUDENT residuals is $\n2$ or $\n3$, you've probably detected an influential observation\n\n\n\n\n\n\nUsing DFFITS statistics to detect influential observations\n\n\nDFFITS measures the impact that each observation has on its own predicted value. For each observation, DFFITS is \ncalculated using two predicted values\n:\n\n\n\n\nThe first predicted value is calculated from a model using the entire data set to estimate model parameters\n\n\nThe second predicted value is calculated from a model using the data set with that particular observation removed to estimate model parameters\n\n\nThe difference between the two predicted values is divided by the standard error of the predicted value, without the observation\n\n\n\n\nIf the \nstandardized difference\n between these predicted values \nis large\n, that particular observation has a \nlarge effect on the model fit\n.\n\n\n\n\nThe \ngeneral cutoff\n value is $2$\n\n\nThe more \nprecise cutoff\n is $2 \\cdot sqrt(p/n)$\n\n\nIf the absolute value of DFFITS for any observation is $\n$ cutoff value, you've detected an influential observation\n\n\nDFFITS is most useful for \npredictive models\n\n\n\n\n\n\nUsing DFBETAS statistics to explore the influenced predictor variable\n\n\nTo help identifying which parameter the observation might be influencing most you can use DFBETAS (difference in betas). It measure the change in each parameter estimate. \n\n\n\n\nOne DFBETA is calculated per predictor variable per observation\n\n\nEach value is calculated by taking the estimated coefficient for that particular predictor variable \nusing all the data\n, subtracting the estimated coefficient for that particular predictor variable with the \ncurrent observation removed\n and dividing by its standard error\n\n\nLarge DFBETAS indicate observations that are influential in estimating a given parameter:\n\n\nThe \ngeneral cutoff\n value is $2$\n\n\nThe more \nprecise cutoff\n is $2 \\cdot sqrt(1/n)$\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\nPROC GLMSELECT DATA=SAS-data-set \noptions\n;\n    \nlabel:\n MODEL dependent(s) = regressor(s) / \n/options\n;\nRUN;\nQUIT;\n\nODS OUTPUT RSTUDENTBYPREDICTED=name-rstud-data-set\n           COOKSDPLOT=name-cooksd-data-set\n           DFFITSPLOT=name-dffits-data-set\n           DFBETASPANEL=name-dfbs-data-set;\n\nPROC REG DATA=SAS-data-set PLOTS(ONLY LABEL)=\n                                (RSTUDENTBYPREDICTED \n                                 COOKSD \n                                 DFFITS \n                                 DFBETAS) \noptions\n;\n    \nlabel:\n MODEL dependent=\n_GLSIND \n/options\n;\n    ID variable4identification;\nRUN;\nQUIT;\n\nDATA influential;\n    MERGE name-rstud-data-set\n          name-cooksd-data-set\n          name-dffits-data-set\n          name-dfbs-data-set;\n    BY observation;\n\n    IF (ABS(RSTUDENT)\n3) OR (COOKSDLABEL NE \n \n) OR DFFITSOUT THEN FLAG=1;\n    ARRAY DFBETAS{*} _DFBETASOUT: ;\n    DO I=2 TO DIM(DFBETAS);\n        IF DFBETAS{I} THEN FLAG=1;\n    END;\n\n    IF ABS(RSTUDENT)\n=3 THEN RSTUDENT=.;\n    IF COOKSDLABEL EQ \n \n THEN COOKSD=.;\n\n    IF FLAG=1;\n    DROP I FLAG;\nRUN;\n\nPROC PRINT DATA=influential;\n    ID observation;\n    VAR RSTUDENT COOKSD DFFITSOUT _DFBETASOUT: ;\nRUN;\n\n\n\n\n\n\n\n\nPROC GLMSELECT automatically creates the \n_GLSIND\n macro variable which stores the list of effects that are in the model whose variable order you can check in the \nInfluence Diagnostics\n panel\n\n\nThe \nODS\n statement takes the data that creates each of the requested plots and saves it in the specified data set\n\n\nThe \nLABEL\n option includes a label for the extreme observations in the plot (labeled with the observation numbers if there is not ID specified)\n\n\n\n\nHaving \ninfluential observations doesn't violate regression assumptions\n, but it's a major nuisance that you need to address:\n\n\n\n\nRecheck\n for data entry errors\n\n\nIf the data appears to be valid, \nconsider whether you have an adequate model\n (a different model might fit the data better). Divide the number of influential observations you detect by the number of observations in you data set: if the result is \n$\n5\\%$ you probably have the wrong model\n.\n\n\nDetermine whether the influential observation is \nvalid but just unusual\n\n\nAs a general rule you should \nnot exclude data\n (some unusual observations contain important information)\n\n\nIf you choose to exclude some observations, include in your report a \ndescription of the types of observations that you excluded and why\n and discuss the limitation of the conclusions given the exclusions\n\n\n\n\nDetecting Collinearity\n\n\nCollinearity (or multicollinearity) is a problem that you face in multiple regression. It occurs when two or more \npredictor variables are highly correlated with each other\n (\nredundant information\n among them, the predictor variables explain much of the same variation in the response). Collinearity doesn't violate the assumptions of multiple regression.\n\n\n\n\nCollinearity can \nhide significant effects\n (if you include only one of the collinear variables in the model it is significant but when there are more than one included none of them are significant)\n\n\nCollinearity \nincreases the variance\n of the parameter estimates, making them \nunstable\n (the data points don't spread out enough in the space to provide stable support for the plane defined by the model) and, in turn, this \nincreases the prediction error\n of the model\n\n\n\n\nWhen an overall model is highly significant but the individual variables don't tell the same story, it's a \nwarning sign of collinearity\n. When the \nstandard error for an estimate is larger than the parameter estimate\n itself, it's not going to be statistically significant. The SE tells us how variable the corresponding parameter estimate is: when the standard errors are high, the \nmodel lacks stability\n.\n\n\n1\n2\n3\n4\nPROC REG DATA=SAS-data-set \noptions\n;\n    \nlabel:\n MODEL dependent = regressors / VIF \n/options\n;\nRUN;\nQUIT;\n\n\n\n\n\n\n\n\nThe \nVIF\n (variance inflation factor, $VIF_i=1/(1-R_i^2)$) option measures the magnitude of collinearity in a model (VIF$\n10$ for any predictor in the model, those predictors are probably involved in collinearity)\n\n\nOther options are \nCOLLIN\n (includes the intercept when analyzing collinearity and helps identify the predictors that are causing the problem) and \nCOLLINOINT\n (requests the same analysis as COLLIN but excludes the intercept)\n\n\n\n\n\n\nEffective modeling cycle\n\n\n\n\nYou want to get to know your data by \nperforming preliminary analysis\n: \n\n\nPlot your data\n\n\nCalculate descriptive statistics \n\n\nPerform correlation analysis\n\n\n\n\n\n\nIdentify some \ngood candidate models\n using PROC REG: \n\n\nFirst check for collinearity \n\n\nUse all-possible regression or stepwise selection methods and subject matter knowledge to select model candidates\n\n\nIdentify the good ones with the Mallows' (prediction) or Hocking's (explanatory) criterion for $C_p$\n\n\n\n\nCheck and validate your assumtions\n by creating residual plots and conducting a few other statistical tests\n\n\n\n\n\n\nDeal with any \nproblems in your data\n: \n\n\n\n\nDetermine whether any influential observations might be throwing off your model calculations\n\n\nDetermine whether any variables are collinear\n\n\n\n\nRevise your model\n\n\n\n\n\n\nValidate your model\n with data not used to build the  model (prediction testing)", 
            "title": "Model Post-Fitting for Inference"
        }, 
        {
            "location": "/statistics/inference/#examining-residuals", 
            "text": "You can use the  residual values  (difference between each observed value of $Y$ and its predicted value) from the regression analysis to verify the  assumptions of the linear regression . Residuals are estimates of the errors, so you can  plot the residuals to check the assumptions of the errors .   You can plot residuals vs the predicted values to check for  violations of equal variances  You can also use this plot to check for  violations of linearity and independence  You can plot the residuals vs the values of the independent variables to  further examine any violations of equal variances  (you can see which predictor contributes to the violation of the assumption)  You can use a histogram or a normal probability plot of the residuals to determine whether or not the  errors are normally distributed   You want to see a  random scatter of the residual values  above and below the reference line at 0. If you see  patterns or trends  in the residual values, the assumptions might not be valid and the models might have problems.    Note   To take autocorrelation (correlated over time) into account, you might need to use a regression procedure such as  PROC AUTOREG  You can also use these plots to  detect outliers , which often reflect data errors or unusual circumstances. They can affect your regression results, so you want to know whether any outliers are present and causing problems and investigate if they result from  data entry error or some other problem  that you can correct.  1\n2\n3\n4\n5 PROC REG DATA=SAS-data-set PLOTS(ONLY)=(QQ RESIDUALBYPREDICTED RESIDUALS) options ;\n     label:  MODEL dependent=regressor(s)  /options ;\n    ID variable4identification;\nRUN;\nQUIT;    QQ  requests a residual quantile-quantile plot to assess the normality of the residual error  RESIDUALBYPREDICTED  requests a plot of residuals by predicted values to verify the equal variance assumption, the independence assumption and model adequacy  RESIDUALS  requests a panel of plots of residuals by the predictor variables in the model. If any of the  Residual by Regressors  plots show signs of unequal variance, we can determine which predictor variable is involved in the problem.", 
            "title": "Examining Residuals"
        }, 
        {
            "location": "/statistics/inference/#identifying-influential-observations", 
            "text": "An influential observation is different from an outlier. An  outlier  is an unusual observation that has a large residual compare to the rest of the points. An  influential observation  can sometimes have a large residual compared to the rest of the points, but it is an observation so far away from the rest of the data that it singlehandedly exerts influence on the slope of the regression line.    Using STUDENT residuals to detect outliers  Also known as  studientized or standardized residuals , the STUDENT residuals are calculated by dividing the  residual by their standard errors , so you can think of them as roughly equivalent to a z-score.    For  relatively small sample sizes , if the absolute value of the STUDENT  residual is $ 2$ , you can suspect that the corresponding observation is an outlier  For  large sample sizes , it's very likely that even more STUDENT  residuals greater than $\\pm2$  will occur just by chance, so you should typically use a larger cutoff value of $ 3$    Using Cook's D statistics to detect influential observations  Fore each observation, the Cook's D statistic is  calculated as if that observation weren't in the data set  as well as the set of parameter estimates with all the observations in your regression analysis.    If any observation has a Cook's D  statistic $ 4/n$  that observation is influential  The Cook's D statistic is most useful for identifying influential observations when the purpose of your model is  parameter estimation    Using RSTUDENT residuals to detect influential observations  RSTUDENT residuals are similar to STUDENT residuals. For each observation, the RSTUDENT residual is the  residual divided by the standard error estimated with the current observation deleted .   If the RSTUDENT residual is different from the STUDENT residual, the observation is probably influential  If the absolute value of the RSTUDENT residuals is $ 2$ or $ 3$, you've probably detected an influential observation    Using DFFITS statistics to detect influential observations  DFFITS measures the impact that each observation has on its own predicted value. For each observation, DFFITS is  calculated using two predicted values :   The first predicted value is calculated from a model using the entire data set to estimate model parameters  The second predicted value is calculated from a model using the data set with that particular observation removed to estimate model parameters  The difference between the two predicted values is divided by the standard error of the predicted value, without the observation   If the  standardized difference  between these predicted values  is large , that particular observation has a  large effect on the model fit .   The  general cutoff  value is $2$  The more  precise cutoff  is $2 \\cdot sqrt(p/n)$  If the absolute value of DFFITS for any observation is $ $ cutoff value, you've detected an influential observation  DFFITS is most useful for  predictive models    Using DFBETAS statistics to explore the influenced predictor variable  To help identifying which parameter the observation might be influencing most you can use DFBETAS (difference in betas). It measure the change in each parameter estimate.    One DFBETA is calculated per predictor variable per observation  Each value is calculated by taking the estimated coefficient for that particular predictor variable  using all the data , subtracting the estimated coefficient for that particular predictor variable with the  current observation removed  and dividing by its standard error  Large DFBETAS indicate observations that are influential in estimating a given parameter:  The  general cutoff  value is $2$  The more  precise cutoff  is $2 \\cdot sqrt(1/n)$     1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44 PROC GLMSELECT DATA=SAS-data-set  options ;\n     label:  MODEL dependent(s) = regressor(s) /  /options ;\nRUN;\nQUIT;\n\nODS OUTPUT RSTUDENTBYPREDICTED=name-rstud-data-set\n           COOKSDPLOT=name-cooksd-data-set\n           DFFITSPLOT=name-dffits-data-set\n           DFBETASPANEL=name-dfbs-data-set;\n\nPROC REG DATA=SAS-data-set PLOTS(ONLY LABEL)=\n                                (RSTUDENTBYPREDICTED \n                                 COOKSD \n                                 DFFITS \n                                 DFBETAS)  options ;\n     label:  MODEL dependent= _GLSIND  /options ;\n    ID variable4identification;\nRUN;\nQUIT;\n\nDATA influential;\n    MERGE name-rstud-data-set\n          name-cooksd-data-set\n          name-dffits-data-set\n          name-dfbs-data-set;\n    BY observation;\n\n    IF (ABS(RSTUDENT) 3) OR (COOKSDLABEL NE    ) OR DFFITSOUT THEN FLAG=1;\n    ARRAY DFBETAS{*} _DFBETASOUT: ;\n    DO I=2 TO DIM(DFBETAS);\n        IF DFBETAS{I} THEN FLAG=1;\n    END;\n\n    IF ABS(RSTUDENT) =3 THEN RSTUDENT=.;\n    IF COOKSDLABEL EQ     THEN COOKSD=.;\n\n    IF FLAG=1;\n    DROP I FLAG;\nRUN;\n\nPROC PRINT DATA=influential;\n    ID observation;\n    VAR RSTUDENT COOKSD DFFITSOUT _DFBETASOUT: ;\nRUN;    PROC GLMSELECT automatically creates the  _GLSIND  macro variable which stores the list of effects that are in the model whose variable order you can check in the  Influence Diagnostics  panel  The  ODS  statement takes the data that creates each of the requested plots and saves it in the specified data set  The  LABEL  option includes a label for the extreme observations in the plot (labeled with the observation numbers if there is not ID specified)   Having  influential observations doesn't violate regression assumptions , but it's a major nuisance that you need to address:   Recheck  for data entry errors  If the data appears to be valid,  consider whether you have an adequate model  (a different model might fit the data better). Divide the number of influential observations you detect by the number of observations in you data set: if the result is  $ 5\\%$ you probably have the wrong model .  Determine whether the influential observation is  valid but just unusual  As a general rule you should  not exclude data  (some unusual observations contain important information)  If you choose to exclude some observations, include in your report a  description of the types of observations that you excluded and why  and discuss the limitation of the conclusions given the exclusions", 
            "title": "Identifying Influential Observations"
        }, 
        {
            "location": "/statistics/inference/#detecting-collinearity", 
            "text": "Collinearity (or multicollinearity) is a problem that you face in multiple regression. It occurs when two or more  predictor variables are highly correlated with each other  ( redundant information  among them, the predictor variables explain much of the same variation in the response). Collinearity doesn't violate the assumptions of multiple regression.   Collinearity can  hide significant effects  (if you include only one of the collinear variables in the model it is significant but when there are more than one included none of them are significant)  Collinearity  increases the variance  of the parameter estimates, making them  unstable  (the data points don't spread out enough in the space to provide stable support for the plane defined by the model) and, in turn, this  increases the prediction error  of the model   When an overall model is highly significant but the individual variables don't tell the same story, it's a  warning sign of collinearity . When the  standard error for an estimate is larger than the parameter estimate  itself, it's not going to be statistically significant. The SE tells us how variable the corresponding parameter estimate is: when the standard errors are high, the  model lacks stability .  1\n2\n3\n4 PROC REG DATA=SAS-data-set  options ;\n     label:  MODEL dependent = regressors / VIF  /options ;\nRUN;\nQUIT;    The  VIF  (variance inflation factor, $VIF_i=1/(1-R_i^2)$) option measures the magnitude of collinearity in a model (VIF$ 10$ for any predictor in the model, those predictors are probably involved in collinearity)  Other options are  COLLIN  (includes the intercept when analyzing collinearity and helps identify the predictors that are causing the problem) and  COLLINOINT  (requests the same analysis as COLLIN but excludes the intercept)    Effective modeling cycle   You want to get to know your data by  performing preliminary analysis :   Plot your data  Calculate descriptive statistics   Perform correlation analysis    Identify some  good candidate models  using PROC REG:   First check for collinearity   Use all-possible regression or stepwise selection methods and subject matter knowledge to select model candidates  Identify the good ones with the Mallows' (prediction) or Hocking's (explanatory) criterion for $C_p$   Check and validate your assumtions  by creating residual plots and conducting a few other statistical tests    Deal with any  problems in your data :    Determine whether any influential observations might be throwing off your model calculations  Determine whether any variables are collinear   Revise your model    Validate your model  with data not used to build the  model (prediction testing)", 
            "title": "Detecting Collinearity"
        }, 
        {
            "location": "/statistics/categorical-data/", 
            "text": "Chapter summary in SAS\n\n\nWhen you response variable is categorical, you need to use a different kind of regression analysis: \nlogistic regression\n.\n\n\nDescribing Categorical Data\n\n\nWhen you examine the distribution of a \ncategorical variable\n, you want to know the \nvalues\n of the variable and the \nfrequency or count\n of each value in the data (\none-way frequency able\n).\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable1 variable2 variable3 \n/options\n;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\nTo look for a possible \nassociation\n between two or more categorical variables, you can create a \ncrosstabulation\n/\ncontingency table\n (when it displays statistics for two variables is also called \ntwo-way frequency able\n).\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns \n/options\n;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\nTwo distribution plots are associated with a frequency or crosstabulation table: a \nfrequency plot\n, PLOTS=\n(FREQPLOT)\n, and a \ncumulative frequency plot\n.\n\n\nIn PROC FREQ output, the default order for character values is \nalphaumeric\n. To reorder the values of an ordinal variable in your FROC FREQ output you can:\n\n\n\n\nCreate a \nnew variable\n in which the values are stored in logical order\n\n\nApply a \ntemporary format\n to the original variable\n\n\n\n\nTests of Association\n\n\nTo perform a \nformal test of association\n between two categorical variables, you use the (Pearson) \nchi-square test\n which measures the difference between the observed cell frequencies and the cell frequencies that are expected if there is no association between variables (H0 is true): \n$Expected=Row \\ total\\cdot Column\\ total/Total \\ sample \\ size$\n\n\n\n\nIf the \nsample size decreases\n, the \nchi-square value decreases\n and the \np-value for the chi-square statistic increases\n\n\nHypothesis testing: \nH0\n: no association; \nHa\n: association\n\n\n\n\nCramer's V statistic\n is one measure of strength of an association between two categorical variables\n\n\n\n\nFor two-by-two tables, Cramer's V is in the range of -1 to 1\n\n\nFor larger tables, Cramer's V is int he range of 0 to 1 \n\n\nValues farther away from 0 indicate a relatively strong association between the variables\n\n\n\n\nTo measure the strength of the association between a binary predictor variable and a binary outcome variable, you can use an \nodds ratio\n: $Odds \\ Ratio=\\frac{Odds \\ of \\ Outcome \\ in \\ Group \\ B}{Odds \\ of \\ Outcome \\ in \\ Group \\ A}$; $Odds=p_{event}/(1-p_{event})$\n\n\n\n\nThe value of the odds ratio can range from 0 to $\\infty$; it cannot be negative\n\n\nWhen the odds ratio is $1$, there is no association between variables\n\n\nWhen the odds ratio $\n1$/$\n1$, the group in the numerator/denominator is more likely to have the outcome\n\n\nThe odds ratio is approximately the same \nregardless of the sample size\n\n\nTo estimate the true odds ratio while taking into account the variability of the sample statistic, you can calculate \nconfidence intervals\n\n\nYou can use an odds ratio to \ntest for significance\n between two categorical variables\n\n\nOdds ratio expressed as percent difference: $(odd \\ ratio -1) \\cdot 100$\n\n\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED \n/options\n;\n    \nadditional statements\n\nRUN;\n\n\n\n\n\n\n\n\nCHISQ\n produces the Pearson chi-square test of association, the likelihood-ratio chi-square and the Mantel-Haenszel: $\\sum \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$\n\n\nEXPECTED\n prints the expected cell frequencies\n\n\nCELLCHI2\n prints each cell's contribution to the total chi-square statistic: $ \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$\n\n\nNOCOL\n suppresses the printing of the column percentages\n\n\nNOPERCENT\n supresses the printing of the cell percentages\n\n\nRELRISK\n (relative risk) prints a table that contains risk ratios (probability ratios) and odds ratios; PROC FREQ uses the \nclassification in the first column\n of the crosstabulation table as the \noutcome of interest\n and the first/second row in the numerator/denominator\n\n\n\n\n\n\nFor \nordinal associations\n, the \nMantel-Haenszel\n chi-square test is a more powerful test.\n\n\n\n\nThe levels must be in a \nlogical order\n for the test results to be meaningful\n\n\nHypothesis testing: \nH0\n: no ordinal association; \nHa\n: ordinal association\n\n\nSimilarly to the Pearson case, the Mantel-Haenszel chi-square statistic/p-value indicate whether an association exists but not its magnitude and they depend on and reflect the sample size\n\n\n\n\nTo measure the \nstrength of the association\n between two ordinal variables you can use the \nSpearman correlation\n statistic.\n\n\n\n\nYou should only use it if both variables are ordinal and are in logical order\n\n\nIs considered to be a rank correlation because it provides a degree of association between the ranks of the ordinal variables\n\n\nThis statistic has a \nrange between -1 and +1\n: values close to -1/+1 indicate that there is a relatively high degree of negative/positive correlation and values close to 0 indicate a weak correlation\n\n\nIt is \nnot affected by the sample size\n\n\n\n\n1\n2\n3\n4\nPROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED \n/options\n;\n  \nadditional statements\n\nRUN;\n\n\n\n\n\n\n\n\nMEASURES\n produces the Spearman correlation statistic along with other measurement of association\n\n\nCL\n produces confidence bounds for the statistics that the MEASURES option requests\n\n\nThe confidence bounds are valid only if the sample size is large ($\n25$)\n\n\nThe asymptotic standard error (\nASE\n) is used for large samples and is used to calculate the confidence intervals for various measures of association (including the Spearman correlation coefficient)\n\n\n\n\nIntroduction to Logistic Regression\n\n\nLogistic Regression is a generalized linear model that you can use to predict a categorical response/outcome on the basis if one or more continuous or categorical predictor variables. There are three models:\n\n\n\n\nSome reasons why you \ncan't use linear regression\n with a \nbinary response variable\n are:\n\n\n\n\naaa\n\n\n\n\nMultiple Logistic Regression", 
            "title": "Categorical Data Analysis"
        }, 
        {
            "location": "/statistics/categorical-data/#describing-categorical-data", 
            "text": "When you examine the distribution of a  categorical variable , you want to know the  values  of the variable and the  frequency or count  of each value in the data ( one-way frequency able ).  1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable1 variable2 variable3  /options ;\n     additional statements \nRUN;   To look for a possible  association  between two or more categorical variables, you can create a  crosstabulation / contingency table  (when it displays statistics for two variables is also called  two-way frequency able ).  1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns  /options ;\n     additional statements \nRUN;   Two distribution plots are associated with a frequency or crosstabulation table: a  frequency plot , PLOTS= (FREQPLOT) , and a  cumulative frequency plot .  In PROC FREQ output, the default order for character values is  alphaumeric . To reorder the values of an ordinal variable in your FROC FREQ output you can:   Create a  new variable  in which the values are stored in logical order  Apply a  temporary format  to the original variable", 
            "title": "Describing Categorical Data"
        }, 
        {
            "location": "/statistics/categorical-data/#tests-of-association", 
            "text": "To perform a  formal test of association  between two categorical variables, you use the (Pearson)  chi-square test  which measures the difference between the observed cell frequencies and the cell frequencies that are expected if there is no association between variables (H0 is true): \n$Expected=Row \\ total\\cdot Column\\ total/Total \\ sample \\ size$   If the  sample size decreases , the  chi-square value decreases  and the  p-value for the chi-square statistic increases  Hypothesis testing:  H0 : no association;  Ha : association   Cramer's V statistic  is one measure of strength of an association between two categorical variables   For two-by-two tables, Cramer's V is in the range of -1 to 1  For larger tables, Cramer's V is int he range of 0 to 1   Values farther away from 0 indicate a relatively strong association between the variables   To measure the strength of the association between a binary predictor variable and a binary outcome variable, you can use an  odds ratio : $Odds \\ Ratio=\\frac{Odds \\ of \\ Outcome \\ in \\ Group \\ B}{Odds \\ of \\ Outcome \\ in \\ Group \\ A}$; $Odds=p_{event}/(1-p_{event})$   The value of the odds ratio can range from 0 to $\\infty$; it cannot be negative  When the odds ratio is $1$, there is no association between variables  When the odds ratio $ 1$/$ 1$, the group in the numerator/denominator is more likely to have the outcome  The odds ratio is approximately the same  regardless of the sample size  To estimate the true odds ratio while taking into account the variability of the sample statistic, you can calculate  confidence intervals  You can use an odds ratio to  test for significance  between two categorical variables  Odds ratio expressed as percent difference: $(odd \\ ratio -1) \\cdot 100$   1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED  /options ;\n     additional statements \nRUN;    CHISQ  produces the Pearson chi-square test of association, the likelihood-ratio chi-square and the Mantel-Haenszel: $\\sum \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$  EXPECTED  prints the expected cell frequencies  CELLCHI2  prints each cell's contribution to the total chi-square statistic: $ \\frac{(obs. \\ freq. - exp. \\ freq.)^2}{exp. \\ freq.}$  NOCOL  suppresses the printing of the column percentages  NOPERCENT  supresses the printing of the cell percentages  RELRISK  (relative risk) prints a table that contains risk ratios (probability ratios) and odds ratios; PROC FREQ uses the  classification in the first column  of the crosstabulation table as the  outcome of interest  and the first/second row in the numerator/denominator    For  ordinal associations , the  Mantel-Haenszel  chi-square test is a more powerful test.   The levels must be in a  logical order  for the test results to be meaningful  Hypothesis testing:  H0 : no ordinal association;  Ha : ordinal association  Similarly to the Pearson case, the Mantel-Haenszel chi-square statistic/p-value indicate whether an association exists but not its magnitude and they depend on and reflect the sample size   To measure the  strength of the association  between two ordinal variables you can use the  Spearman correlation  statistic.   You should only use it if both variables are ordinal and are in logical order  Is considered to be a rank correlation because it provides a degree of association between the ranks of the ordinal variables  This statistic has a  range between -1 and +1 : values close to -1/+1 indicate that there is a relatively high degree of negative/positive correlation and values close to 0 indicate a weak correlation  It is  not affected by the sample size   1\n2\n3\n4 PROC FREQ DATA=SAS-data-set;\n    TABLES variable-rows*variable-columns / CHISQ EXPECTED  /options ;\n   additional statements \nRUN;    MEASURES  produces the Spearman correlation statistic along with other measurement of association  CL  produces confidence bounds for the statistics that the MEASURES option requests  The confidence bounds are valid only if the sample size is large ($ 25$)  The asymptotic standard error ( ASE ) is used for large samples and is used to calculate the confidence intervals for various measures of association (including the Spearman correlation coefficient)", 
            "title": "Tests of Association"
        }, 
        {
            "location": "/statistics/categorical-data/#introduction-to-logistic-regression", 
            "text": "Logistic Regression is a generalized linear model that you can use to predict a categorical response/outcome on the basis if one or more continuous or categorical predictor variables. There are three models:   Some reasons why you  can't use linear regression  with a  binary response variable  are:   aaa", 
            "title": "Introduction to Logistic Regression"
        }, 
        {
            "location": "/statistics/categorical-data/#multiple-logistic-regression", 
            "text": "", 
            "title": "Multiple Logistic Regression"
        }, 
        {
            "location": "/statistics/prediction/", 
            "text": "Chapter summary in SAS\n\n\nIntroduction to Predictive Modeling\n\n\nScoring Predictive Models", 
            "title": "Model Building and Scoring for Prediction"
        }, 
        {
            "location": "/statistics/prediction/#introduction-to-predictive-modeling", 
            "text": "", 
            "title": "Introduction to Predictive Modeling"
        }, 
        {
            "location": "/statistics/prediction/#scoring-predictive-models", 
            "text": "", 
            "title": "Scoring Predictive Models"
        }, 
        {
            "location": "/missing_data/", 
            "text": "Depending on the \ntype of data and model\n you will be using, techniques such as \nmultiple imputation\n or \ndirect maximum likelihood\n may better serve your needs. The main goals of statistical analysis with missing data are:\n\n\n\n\nMinimize bias\n\n\nMaximize use of available information\n\n\nObtain appropriate estimates of uncertainty\n\n\n\n\nTo use the more appropriate imputation method you should consider the missing data mechanism of your data which describes the process that is believed to have generated the missing values:\n\n\n\n\nMissing completely at random (MCAR)\n:  neither the variables in the dataset nor the unobserved value of the variable itself predict whether a value will be missing\n\n\nMissing at random (MAR)\n: other variables (but not the variable itself) in the dataset can be used to predict missingness on a given variable\n\n\nMissing not at random (MNAR)\n: value of the unobserved variable itself predicts missingness\n\n\n\n\nImputed values are \nnot\n equivalent to observed values and serve only to help estimate the covariances between variables needed for inference.\n\n\nSome of the imputation techniques are:\n\n\n\n\nComplete case analysis (listwise deletion)\n:  deleting cases in a particular dataset that are missing data on any variable of interest (for MCAR cases the power is reduced but it does not add any bias) \n\n\nAvailable case analysis (pairwise deletion)\n:  deleting cases where a variable required for a particular analysis is missing, but including those cases in analyses for which all required variables are present\n\n\nMean Imputation\n:\n\n\nSingle Imputation\n:\n\n\nStochastic Imputation\n: \n\n\n\n\nDirect maximum likelihood\n\n\nMultiple imputation\n\n\nVisit \nthis website\n for more information.\n\n\nMultiple Imputation is always superior to any of the single imputation methods because:\n\n\n\n\nA single imputed value is never used\n\n\nThe variance estimates reflect the appropriate amount of uncertainty surrounding parameter estimates\n\n\n\n\nThere are several decisions to be made before performing a multiple imputation including \ndistribution\n, \nauxiliary variables\n and \nnumber of imputations\n that can affect the quality of the imputation.\n\n\n\n\nImputation phase (PROC MI)\n:  the user specifies the imputation model to be used and the number \n       of imputed datasets to be created\n\n\nAnalysis phase (PROG GLM/PROC GENMOD)\n: runs the analytic model of interest within each of the imputed datasets\n\n\nPooling phase (PROC MIANALYZE)\n: combines all the estimates across all the imputed datasets and outputs one set of parameter estimates for the model of interest\n\n\n\n\nMVN or FCS?\n\n\nAuxiliary variables\n\n\n\n\nThey can can help improve the likelihood of meeting the MAR assumption \n\n\nThey help yield more accurate and stable estimates and thus reduce the estimated standard errors in analytic models \n\n\nIncluding them can also help to increase power\n\n\n\n\nNumber of imputations (m)\n\n\n\n\nEstimates of coefficients stabilize at much lower values of \nm\n than estimates of variances and covariances of error terms \n\n\nA larger number of imputations may also allow hypothesis tests with less restrictive assumptions (i.e., that do not assume equal fractions of missing information for all coefficients)\n\n\nMultiple runs of m imputations are recommended to assess the stability of the parameter estimates\n\n\nRecommendations: \n\n\nFor low fractions of missing information (and relatively simple analysis techniques) 5-20 imputations and 50 or more when the proportion of missing data is relatively high\n\n\nThe number of imputations should equal the percentage of incomplete cases (\nm\n=max(FMI%)), this way the error associated with estimating the regression coefficients, standard errors and the resulting p-values is considerably reduced and results in an adequate level of reproducibility\n\n\n\n\nMore comments\n\n\n\n\nYou should include the dependent variable (DV) in the imputation model unless you would like to impute independent variables (IVs) assuming they are uncorrelated with your DV\n\n\nAlthough MI can perform well up to 50% missing observations,  the larger the amount the higher the chance of finding estimation problems during the imputation process and the lower the chance of meeting the MAR assumption", 
            "title": "Dealing with Missing Data"
        }, 
        {
            "location": "/missing_data/#direct-maximum-likelihood", 
            "text": "", 
            "title": "Direct maximum likelihood"
        }, 
        {
            "location": "/missing_data/#multiple-imputation", 
            "text": "Visit  this website  for more information.  Multiple Imputation is always superior to any of the single imputation methods because:   A single imputed value is never used  The variance estimates reflect the appropriate amount of uncertainty surrounding parameter estimates   There are several decisions to be made before performing a multiple imputation including  distribution ,  auxiliary variables  and  number of imputations  that can affect the quality of the imputation.   Imputation phase (PROC MI) :  the user specifies the imputation model to be used and the number \n       of imputed datasets to be created  Analysis phase (PROG GLM/PROC GENMOD) : runs the analytic model of interest within each of the imputed datasets  Pooling phase (PROC MIANALYZE) : combines all the estimates across all the imputed datasets and outputs one set of parameter estimates for the model of interest   MVN or FCS?  Auxiliary variables   They can can help improve the likelihood of meeting the MAR assumption   They help yield more accurate and stable estimates and thus reduce the estimated standard errors in analytic models   Including them can also help to increase power   Number of imputations (m)   Estimates of coefficients stabilize at much lower values of  m  than estimates of variances and covariances of error terms   A larger number of imputations may also allow hypothesis tests with less restrictive assumptions (i.e., that do not assume equal fractions of missing information for all coefficients)  Multiple runs of m imputations are recommended to assess the stability of the parameter estimates  Recommendations:   For low fractions of missing information (and relatively simple analysis techniques) 5-20 imputations and 50 or more when the proportion of missing data is relatively high  The number of imputations should equal the percentage of incomplete cases ( m =max(FMI%)), this way the error associated with estimating the regression coefficients, standard errors and the resulting p-values is considerably reduced and results in an adequate level of reproducibility   More comments   You should include the dependent variable (DV) in the imputation model unless you would like to impute independent variables (IVs) assuming they are uncorrelated with your DV  Although MI can perform well up to 50% missing observations,  the larger the amount the higher the chance of finding estimation problems during the imputation process and the lower the chance of meeting the MAR assumption", 
            "title": "Multiple imputation"
        }, 
        {
            "location": "/macros/", 
            "text": "\ufeffYou can learn about macros in the \nSAS Macro Language 1: Essentials course\n.\n\n\nMacro Program for Creating Box Plots for All of Predictor Variables\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n%let\n \ncategorical\n=\nHouse_Style2\n \nOverall_Qual2\n \nOverall_Cond2\n \nFireplaces\n \n         \nSeason_Sold\n \nGarage_Type_2\n \nFoundation_2\n \nHeating_QC\n \n         \nMasonry_Veneer\n \nLot_Shape_2\n \nCentral_Air\n;\n\n\n/* Macro Usage: %box(DSN = , Response = , CharVar = ) */\n\n\n%macro\n \nbox\n(\ndsn\n      \n=\n \n,\n\n           \nresponse\n \n=\n \n,\n\n           \nCharvar\n  \n=\n \n);\n\n\n%let\n \ni\n \n=\n \n1\n \n;\n\n\n%do\n \n%while\n(\n%scan\n(\ncharvar\n,\ni\n,\n%str\n(\n \n))\n \n^=\n \n%str\n())\n \n;\n\n    \n%let\n \nvar\n \n=\n \n%scan\n(\ncharvar\n,\ni\n,\n%str\n(\n \n));\n\n    \nproc\n \nsgplot\n \ndata\n=\ndsn\n;\n\n        \nvbox\n \nresponse\n \n/\n \ncategory\n=\nvar\n \n                         \ngrouporder\n=\nascending\n \n                         \nconnect\n=\nmean\n;\n\n        \ntitle\n \nresponse across Levels of \nvar\n;\n\n    \nrun\n;\n\n    \n%let\n \ni\n \n=\n \n%eval\n(\ni\n \n+\n \n1\n \n)\n \n;\n\n\n%end\n \n;\n\n\n%mend\n \nbox\n;\n\n\n%box\n(\ndsn\n      \n=\n \nstatdata\n.\nameshousing3\n,\n\n     \nresponse\n \n=\n \nSalePrice\n,\n\n     \ncharvar\n  \n=\n \ncategorical\n);\n\n\ntitle\n;\n\n\noptions\n \nlabel\n;", 
            "title": "Using Macros"
        }, 
        {
            "location": "/macros/#macro-program-for-creating-box-plots-for-all-of-predictor-variables", 
            "text": "1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24 %let   categorical = House_Style2   Overall_Qual2   Overall_Cond2   Fireplaces  \n          Season_Sold   Garage_Type_2   Foundation_2   Heating_QC  \n          Masonry_Veneer   Lot_Shape_2   Central_Air ;  /* Macro Usage: %box(DSN = , Response = , CharVar = ) */  %macro   box ( dsn        =   , \n            response   =   , \n            Charvar    =   );  %let   i   =   1   ;  %do   %while ( %scan ( charvar , i , %str (   ))   ^=   %str ())   ; \n     %let   var   =   %scan ( charvar , i , %str (   )); \n     proc   sgplot   data = dsn ; \n         vbox   response   /   category = var  \n                          grouporder = ascending  \n                          connect = mean ; \n         title   response across Levels of  var ; \n     run ; \n     %let   i   =   %eval ( i   +   1   )   ;  %end   ;  %mend   box ;  %box ( dsn        =   statdata . ameshousing3 , \n      response   =   SalePrice , \n      charvar    =   categorical );  title ;  options   label ;", 
            "title": "Macro Program for Creating Box Plots for All of Predictor Variables"
        }, 
        {
            "location": "/miscellanea/", 
            "text": "When you name a process flow Autoexec, SAS Enterprise Guide prompts you to run the process flow when you open the project. This makes it easy to recreate your data when you start practising in the course.\n\n\nFor the SAS Enterprise Guide you need to add this command to get the plots displayed in the output:\n\n\n1\n2\n3\nODS GRAPHICS ON;\n[your code here]\nODS GRAPHICS OFF;\n\n\n\n\n\n\nWhen you add the \nODS TRACE\n statement, SAS writes a trace record to the log that includes information about each output object (name, label, template, path):\n\n\n1\n2\n3\nODS TRACE ON;\n[your code here]\nODS TRACE OFF;\n\n\n\n\n\n\nYou produce a list of the possible output elements in the log that you may specify in the \nODS SELECT/EXCLUDE\n statement:\n\n\n1\n2\nODS SELECT lmeans diff meanplot diffplot controlplot;\n[your code here]\n\n\n\n\n\n\nThis way you can see the actual variable level values in the output rather than some indexes:\n\n\n1\nFORMAT variable DOSEF.;\n\n\n\n\n\n\n\n\nCreate different data sets from one:\n\n\n\n\n1\n2\n3\n4\n5\n6\nDATA data1 data2 data3;\n    SET original_data;\n    IF (condition1) THEN OUTPUT prueba1;\n    IF (condition2) THEN OUTPUT prueba2;\n    IF (condition3) THEN OUTPUT prueba3;\nRUN;\n\n\n\n\n\n\n\n\nCopy database \ntest\n into \nwork\n:\n\n\n\n\n1\n2\nproc copy in=test out=work;\nrun;\n\n\n\n\n\n\n\n\nRemove data sets:\n\n\n\n\n1\n2\nPROC DELETE DATA=data1 data2 data3;\nRUN;\n\n\n\n\n\n\n\n\nDeclaring \narrays\n:\n\n\n\n\nThe dimension has to be known in advance (???)\nThere's no way to write an implicit loop through all the elements of the array (???)\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\ndata _null_;\n\n    ARRAY arrayname[2,3] \n$\n v11-3 (0 0 0)\n                         \n$\n v21-3 (0 0 0);\n\n    DO i=1 TO DIM(arrayname);\n        arrayname[i] = arrayname[i] + 1;\n    END;\n\n    result=CATX(\n,\n,OF v11-3);\n    PUT result=;\n\nRUN;\n\n\n\n\n\n\n\n\nIF THEN analogue to \"CONTAINS\":\n\n\n\n\n1\nif find(variable_name,\npattern\n) then\n\n\n\n\n\n\n\n\nSite calculation from the two first numbers of the patient number:\n\n\n\n\n1\nsite = SUBSTR(PUT(patient,z4.),1,2);\n\n\n\n\n\n\n\n\nPUT\n: turns the numeric variable \npatient\n into a string (\nz4.\n adds leading zeroes if needed)\n\n\n\n\nSUBSTR\n: takes the first \n2\n characters starting from position \n1\n\n\n\n\n\n\nCount the distinct values of a variable\n\n\n\n\n\n\nRemove element/string from macro variable \n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n%put\n \n=\nlist\n;\n     \n/* Check list contents before */\n\n\n\n%let\n \nremovefromlist\n \n=\n \nstring_to_remove\n;\n\n\n%let\n \nlist\n \n=\n \n%sysfunc\n(\ntranwrd\n(\nlist\n.,\n \nremovefromlist\n.,\n \n%str\n()));;\n\n\n\n%put\n \n=\nlist\n;\n     \n/* Check list contents after */\n\n\n\n\n\n\n\n\n\nHow to uppercase the first letter of words:\n\n\n\n\n1\nvar_propercase = PROPCASE(var_uppercase);\n\n\n\n\n\n\n\n\nHow to replace the variable's name with the variable's label in PROC FREQ output\n\n\n\n\n1\n2\n3\n4\noptions validvarname=any;\nPROC FREQ DATA=SAS-data-set (RENAME=(variable1=\nLabel variable 1\nn variable1=\nLabel variable 1\nn));\n    TABLES \nLabel variable 1\nn;\nRUN;\n\n\n\n\n\n\n\n\nPROC TEMPLATE style tips\n\n\nWHERE variable IS MISSING", 
            "title": "Miscellanea"
        }
    ]
}